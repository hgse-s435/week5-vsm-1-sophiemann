{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 - Vector Space Model (VSM) and Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the next weeks, we are going to re-implement Sherin's algorithm and apply it to the text data we've been working on last week! Here's our roadmap:\n",
    "\n",
    "**Week 5 - data cleaning**\n",
    "1. import the data\n",
    "2. clean the data (e.g., remopve stop words, punctuation, etc.)\n",
    "3. build a vocabulary for the dataset\n",
    "4. create chunks of 100 words, with a 25-words overlap\n",
    "5. create a word count matrix, where each chunk of a row and each column represents a word\n",
    "\n",
    "**Week 6 - vectorization and linear algebra**\n",
    "6. Dampen: weight the frequency of words (1 + log[count])\n",
    "7. Scale: Normalize weighted frequency of words\n",
    "8. Direction: compute deviation vectors\n",
    "\n",
    "**Week 7 - Clustering**\n",
    "9. apply different unsupervised machine learning algorithms\n",
    "    * figure out how many clusters we want to keep\n",
    "    * inspect the results of the clustering algorithm\n",
    "\n",
    "**Week 8 - Visualizing the results**\n",
    "10. create visualizations to compare documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in python code, our goal is to recreate the steps above as functions\n",
    "# so that we can just one line to run topic modeling on a list of \n",
    "# documents: \n",
    "def ExtractTopicsVSM(documents, numTopics):\n",
    "    ''' this functions takes in a list of documents (strings), \n",
    "        runs topic modeling (as implemented by Sherin, 2013)\n",
    "        and returns the clustering results, the matrix used \n",
    "        for clustering a visualization '''\n",
    "    \n",
    "    # step 2: clean up the documents\n",
    "    documents = clean_list_of_documents(documents)\n",
    "    \n",
    "    # step 3: let's build the vocabulary of these docs\n",
    "    vocabulary = get_vocabulary(documents)\n",
    "    \n",
    "    # step 4: we build our list of 100-words overlapping fragments\n",
    "    documents = flatten_and_overlap(documents)\n",
    "    \n",
    "    # step 5: we convert the chunks into a matrix\n",
    "    matrix = docs_by_words_matrix(documents, vocabulary)\n",
    "    \n",
    "    # step 6: we weight the frequency of words (count = 1 + log(count))\n",
    "    matrix = one_plus_log_mat(matrix, documents, vocabulary)\n",
    "    \n",
    "    # step 7: we normalize the matrix\n",
    "    matrix = normalize(matrix)\n",
    "    \n",
    "    # step 8: we compute deviation vectors\n",
    "    matrix = transform_deviation_vectors(matrix, documents)\n",
    "    \n",
    "    # step 9: we apply a clustering algorithm to find topics\n",
    "    results_clustering = cluster_matrix(matrix)\n",
    "    \n",
    "    # step 10: we create a visualization of the topics\n",
    "    visualization = visualize_clusters(results_clustering, vocabulary)\n",
    "    \n",
    "    # finally, we return the clustering results, the matrix, and a visualization\n",
    "    return results_clustering, matrix, visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./Papers/paper12.txt', './Papers/paper5.txt', './Papers/paper4.txt', './Papers/paper13.txt', './Papers/paper11.txt', './Papers/paper6.txt', './Papers/paper7.txt', './Papers/paper10.txt', './Papers/paper14.txt', './Papers/paper3.txt', './Papers/paper2.txt', './Papers/paper15.txt', './Papers/paper0.txt', './Papers/paper1.txt', './Papers/paper16.txt', './Papers/paper9.txt', './Papers/paper8.txt']\n"
     ]
    }
   ],
   "source": [
    "# 1) using glob, find all the text files in the \"Papers\" folder\n",
    "# Hint: refer to last week's notebook\n",
    "import glob\n",
    "papers = glob.glob(\"./Papers/paper*.txt\")\n",
    "print(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\x0czone out no more: mitigating mind wandering during\\ncomputerized reading\\nsidney k. d’mello, caitlin mills, robert bixler, & nigel bosch\\nuniversity of notre dame\\n118 haggar hall\\nnotre dame, in 46556, usa\\nsdmello@nd.edu\\n\\nabstract\\nmind wandering, defined as shifts in attention from task-related\\nprocessing to task-unrelated thoughts, is a ubiquitous\\nphenomenon that has a negative influence on performance and\\nproductivity in many contexts, including learning. we propose\\nthat next-generation learning technologies should have some\\nmechanism to detect and respond to mind wandering in real-time.\\ntowards this end, we developed a technology that automatically\\ndetects mind wandering from eye-gaze during learning from\\ninstructional texts. when mind wandering is detected, the\\ntechnology intervenes by posing just-in-time questions and\\nencouraging re-reading as needed. after multiple rounds of\\niterative refinement, we summatively compared the technology to\\na yoked-control in an experiment with 104 participants. the key\\ndependent variable was performance on a post-reading\\ncomprehension assessment. our results suggest that the\\ntechnology was successful in correcting comprehension deficits\\nattributed to mind wandering (d = .47 sigma) under specific\\nconditions, thereby highlighting the potential to improve learning\\nby “attending to attention.”\\n\\nkeywords\\nmind wandering; gaze tracking; student modeling; attentionaware.\\n\\n1. introduction\\ndespite our best efforts to write a clear and engaging paper,\\nchances are high that within the next 10 pages you might fall prey\\nto what is referred to as zoning out, daydreaming, or mind\\nwandering [45]. despite your best intention to concentrate on our\\npaper, at some point your attention might drift away to unrelated\\nthoughts of lunch, childcare, or an upcoming trip. this prediction\\nis not based on some negative or cynical opinion of the\\nreader/reviewer (we read and review papers too), but on what is\\nknown about attentional control, vigilance, and concentration\\nwhile individuals are engaged in complex comprehension\\nactivities, such as reading for understanding.\\none recent study tracked mind wandering of 5,000 individuals\\nfrom 83 countries with a smartphone app that prompted people\\nwith thought-probes at random intervals throughout the day [24].\\npeople reported mind wandering for 46.9% of the prompts, which\\nconfirmed lab studies on the pervasiveness of mind wandering\\n(see [45] for a review). mind wandering is more than merely\\nincidental; a recent meta-analysis of 88 samples indicated a\\nnegative correlation between mind wandering and performance\\nacross a variety of tasks [34], a correlation which increases with\\ntask complexity. when compounded with its high frequency,\\nmind wandering can have serious consequences on the\\nperformance and productivity of society at large.\\n\\nof learning with technology. traditional learning technologies\\nrely on the assumption that students are attending to the learning\\nsession, although this is not always the case. for example, it has\\nbeen estimated that students mind wander approximately 40% of\\nthe time when engaging with online lectures [38], which are an\\nimportant component of moocs. some advanced technologies\\ndo aim to detect and respond to affective states like boredom, but\\nevidence for their effectiveness is still equivocal (see [9] for a\\nreview). further, boredom is related to but not the same as\\nattention [12]. there are technologies that aim to prevent mind\\nwandering by engendering a highly immersive learning\\nexperience and have achieved some success in this regard [40,\\n41]. but what is to be done when attentional focus inevitably\\nwanes as the session progresses and the novelty of the system and\\ncontent fades?\\nour central thesis is that next-generation learning technologies\\nshould include mechanisms to model and respond to learners’\\nattention in real-time [8]. such attention-aware technologies can\\nmodel various aspects of learner attention (e.g., divided attention,\\nalternating attention). here, we focus on detecting and mitigating\\nmind wandering, a quintessential signal of waning engagement.\\nwe situate our work in the context of reading because reading is\\na common activity shared across multiple learning technologies,\\nthereby increasing the generalizability of our results. further,\\nstudents mind wander approximately 30% of the time during\\ncomputerized reading [44]. and although mind wandering can\\nfacilitate certain cognitive processes like future planning and\\ndivergent thinking [2, 28], it negatively correlates with\\ncomprehension and learning (reviewed in [31, 45]), suggesting\\nthat it is important to address mind wandering during learning.\\ntowards this end, we developed and validated a closed-loop\\nattention-aware learning technology that combines a machinelearned mind wandering detector with a real-time interpolated\\ntesting and re-study intervention. our attention-aware technology\\nworks as follows. learners read a text on a computer screen using\\na self-paced screen-by-screen (also called page-by-page) reading\\nparadigm. we track eye-gaze during reading using a remote eye\\ntracker that does not restrict head movements. we focus on eyegaze for mind wandering detection due to decades of research\\nsuggesting a tight coupling between attentional focus and eye\\nmovements during reading [36]. when mind wandering is\\ndetected, the system intervenes in an attempt to redirect\\nattentional focus and correct any comprehension deficits that\\nmight arise due to mind wandering. the interventions consist of\\nasking comprehension question on pages where mind wandering\\nwas detected and providing opportunities to re-read based on\\nlearners’ responses. in this paper, we discuss the mind wandering\\n\\nmind wandering is also unfortunately an under-addressed\\nproblem in education and is yet to be deeply studied in the context8\\n\\n\\x0cdetector, intervention approach, and results of a summative\\nevaluation study 1.\\n\\n1.1 related work\\nthe idea of attention-aware user interfaces is not new, but was\\nproposed almost a decade ago by roda and thomas [39]. there\\nwas even an article on futuristic applications of attention-aware\\nsystems in educational contexts [35]. prior to this, gluck, et al.\\n[15] discussed the use of eye tracking to increase the bandwidth\\nof information available to an intelligent tutoring system (its).\\nsimilarly, anderson [1] followed up on some of these ideas by\\ndemonstrating how particular beneficial instructional strategies\\ncould only be launched via a real-time analysis of eye gaze.\\nmost of the recent work has been on leveraging eye gaze to\\nincrease the bandwidth of learner models [22, 23, 29]. conati, et\\nal. [5] provide an excellent review of much of the existing work\\nin this area. we can group the research into three categories: (1)\\noffline-analyses of eye gaze to study attentional processes, (2)\\ncomputational modeling of attentional states, and (3) closed-loop\\nsystems that respond to attention in real-time. offline-analysis of\\neye movements has received considerable attention in cognitive\\nand educational psychology for several decades [e.g., 16, 19], so\\nthis area of research is relatively healthy. online computational\\nmodels of learner attention are just beginning to emerge [e.g., 6,\\n11], while closed-loop attention-aware systems are few and far\\nbetween (see [7, 15, 42, 48] for a more or less exhaustive list).\\ntwo known examples, gazetutor and attentivereview, are\\ndiscussed below.\\ngazetutor [7] is a learning technology for biology. it has an\\nanimated conversational agent that provides spoken explanations\\non biology topics which are synchronized with images. the\\nsystem uses a tobii t60 eye tracker to detect inattention, which\\nis assumed to occur when learners’ gaze is not on the tutor agent\\nor image for at least five consecutive seconds. when this occurs,\\nthe system interrupts its speech mid utterance, directs learners to\\nreorient their attention (e.g., “i’m over here you know”), and\\nrepeats speaking from the start of the current utterance. in an\\nevaluation study, 48 learners (undergraduate students) completed\\na learning session on four biology topics with the attention-aware\\ncomponents enabled (experimental group) or disabled (control\\ngroup). the results indicated that gazetutor was successful in\\ndynamically reorienting learners’ attentional patterns towards the\\ninterface. importantly, learning gains for deep reasoning\\nquestions were significantly higher for the experimental vs.\\ncontrol group, but only for high aptitude learners. the results\\nsuggest that even the most basic attention-aware technology can\\nbe effective in improving learning, at least for a subset of learners.\\nhowever, a key limitation is that the researchers simply assumed\\nthat off-screen gaze corresponded to inattention, but did not test\\nthis assumption (e.g., students could have been concentrating\\nwith their eyes closed and this would have been perceived as\\nbeing inattentive).\\nattentivereview [32] is a closed-loop system for mooc learning\\non mobile phones. the system uses video-based\\nphotoplethysmography (ppg) to detect a learners’ heart rate from\\nthe back camera of a smartphone while they view mooc-like\\nlectures on the phone. attentivereview ranks the lectures based\\n1\\n\\non its estimates of learners’ “perceived difficulty,” selecting the\\nmost difficult lecture for subsequent review (called adaptive\\nreview). in a 32-participant between-subjects evaluation study,\\nthe authors found that learning gains obtained from the adaptive\\nreview condition were statistically on par with a full review\\ncondition, but were achieved in 66.7% less review time. although\\nthis result suggests that attentivereview increased learning\\nefficiency, there is the question as to whether the system should\\neven be considered to be an “attention-aware” technology. this is\\nbecause it is arguable if the system has anything to do with\\nattention (except for “attention” appearing in its name) as it\\nselects items for review based on a model of “perceived\\ndifficulty” and not on learners’ “attentional state.” the two might\\nbe related, but are clearly not the same.\\n\\n1.2 novelty\\nour paper focuses on closing the loop between research on\\neducational data and learning outcomes by developing and\\nvalidating the first (in our view) real-time learning technology\\nthat detects and mitigates mind wandering during computerized\\nreading. although automated detection of complex mental states\\nwith the goal of developing intelligent learning technologies that\\nrespond to the sensed states is an active research area (see reviews\\nby [9, 18]), mind wandering has rarely been explored as an aspect\\nof a learner’s mental state that warrants detection and corrective\\naction. and while there has been some work on modeling the\\nlocus of learner attention (see review by [5]), mind wandering is\\ninherently different than more commonly studied forms of\\nattention (e.g., selective attention, distraction), because it involves\\nmore covert forms of involuntary attentional lapses spawned by\\nself-generated internal thought [45]. simply put, mind wandering\\nis a form of “looking without seeing” because the eyes might be\\nfixated on the appropriate external stimulus, but very little is\\nbeing processed as the mind is consumed by stimulusindependent internal thoughts. offline automated approaches to\\ndetect mind wandering have been developed (e.g., [3, 11, 27, 33]),\\nbut these detectors have not yet been used to trigger online\\ninterventions. here, we adapt an offline gaze-based automated\\nmind wandering detector [13] to trigger real-time interventions to\\naddress mind wandering during reading. we conduct a\\nrandomized control trial to evaluate the efficacy of our attentionaware learning technology in improving learning.\\n\\n2. mind wandering detection\\nwe adopted a supervised learning approach for mind wandering\\ndetection. below we provide a high-level overview of the\\napproach; readers are directed to [3, 13] for a detailed discussion\\nof the general approach used to build gaze-based detectors of\\nmind wandering.\\n\\n2.1 training data\\nwe obtained training data from a previous study [26] that\\ninvolved 98 undergraduate students reading a 57-page text on the\\nsurface tension of liquids [4] on a computer screen for an average\\nof 28 minutes. the text contained around 6500 words, with an\\naverage of 115 words per page, and was displayed on a computer\\nscreen with courier new typeface. we recorded eye-gaze with a\\ntobii tx300 eye tracker set to a sampling frequency of 120 hz.\\n\\nthis paper reports updated results of an earlier version [10] presented\\nas a “late-breaking work” (lbw) poster at the 2016 acm chi\\nconference. lbw “extended abstracts” are not included in the main\\nconference proceedings and copyright is retained by the authors.9\\n\\n\\x0cparticipants could read normally and were free to move or gesture\\nas they pleased.\\nparticipants were instructed to report mind wandering (during\\nreading) by pressing a predetermined key when they found\\nthemselves “thinking about the task itself but not the actual\\ncontent of the text” or when they were “thinking about anything\\nelse besides the task.” this is consistent with contemporary\\napproaches (see [45]) that rely on self-reporting because mind\\nwandering is an internal conscious phenomena. further, selfreports of mind wandering have been linked to predictable\\npatterns in physiology [43], pupillometry [14], eye-gaze [37], and\\ntask performance [34], providing validity for this approach.\\n\\non the page, we classified the page as a positive instance of mind\\nwandering. this was done because analyses indicated that\\nparticipants were more likely to be mind wandering in those cases\\n(but see [13] for alternate strategies to handle missing instances).\\n\\non average, we received mind wandering reports for 32% of the\\npages (sd = 20%), although there was considerable variability\\namong participants (ranging from 0% to 82%). self-reported\\nmind wandering negatively correlated (r = -.23, p < .05) with\\nscores on a subsequent comprehension assessment [26], which\\nprovides evidence for the predictive validity of the self-reports.\\n\\n2.2 model building\\nthe stream of eye-gaze data was filtered to produce a series of\\nfixations, saccades, and blinks, from which global eye gaze\\nfeatures were extracted (see figure 1). global features are\\nindependent of the words being read and are therefore more\\ngeneralizable than so-called local features. a full list of 62 global\\nfeatures along with detailed descriptions is provided in [13], but\\nbriefly the features can be grouped into the following four\\ncategories: (1) eye movement descriptive features (n = 48) were\\nstatistical functionals (e.g., min, median) for fixation duration,\\nsaccade duration, saccade amplitude, saccade velocity, and\\nrelative and absolute saccade angle distributions; (2) pupil\\ndiameter descriptive features were statistical functionals (n = 8)\\ncomputed from participant-level z-score standardized estimates\\nof pupil diameter; (3) blink features (n = 2) consisted of the\\nnumber of blinks and the mean blink duration; (4) miscellaneous\\ngaze features (n = 4) consisted of the number of saccades,\\nhorizontal saccade proportion, fixation dispersion, and the\\nfixation duration/saccade duration ratio. we proceeded with a\\nsubset of 32 features after eliminating features exhibiting\\nmulticollinearity.\\nfeatures were calculated from only a certain amount of gaze data\\nfrom each page, called the window. the end of the window was\\npositioned 3 seconds before a self-report so as to not overlap with\\nthe key-press. the average amount of time between self-reports\\nand the beginning of the page was 16 seconds. we used this time\\npoint as the end of the window for pages with no self-report.\\npages that were shorter than the target window size were\\ndiscarded, as were pages with windows that contained fewer than\\nfive gaze fixations as there was insufficient data to compute some\\nof the features. there were a total of 4,225 windows with\\nsufficient data for supervised classification.\\nwe experimented with a number of supervised classifiers on\\nwindow sizes of 4, 8, and 12 seconds to discriminate positive\\n(pages with a self-report = 32%) from negative (pages without a\\nself-report) instances of mind wandering. the training data were\\ndownsampled to achieve a 50% base rate; testing data were\\nunaltered. a leave-one-participant-out validation approach was\\nadopted where models were built on data from n-1 participants\\nand evaluated on the held-out participant. the process was\\nrepeated for all participants. model validation was conducted in a\\nway to simulate a real-time system by analyzing data from every\\npage. when classification was not possible due to a lack of valid\\ngaze data and/or because participants did not spend enough time\\n\\nfigure 1: gaze fixations during mind wandering (top)\\nand normal reading (bottom)\\n\\n2.3 detector accuracy\\nthe best model was a support vector machine that used global\\nfeatures and operated on a window size of 8-seconds. the area\\nunder the roc curve (auc or auroc or a’) was .66, which\\nexceeds the 0.5 chance threshold [17].\\nwe assigned each instance as mind wandering or not mind\\nwandering based on whether the detector’s predicted likelihood\\nof mind wandering (ranges from 0 to 1) was below or above 0.5\\nwe adopted the default 0.5 threshold as it led to a higher rate of\\ntrue positives while maintaining a moderate rate of true negatives.\\nthis resulted in the following confusion matrix shown in table 1.\\nthe model had a weighted precision of 72.2% and a weighted\\nrecall of 67.4%, which we deemed to be sufficiently accurate for\\nintervention.\\ntable 1: proportionalized confusion matrix for mind\\nwandering detection\\npredicted mind wandering (mw)\\nactual mw\\n\\nyes\\n\\nno\\n\\nyes\\n\\n0.715 (hit)\\n\\n0.285 (miss)\\n\\nno\\n\\n0.346 (false positive)\\n\\n0.654 (correct rejection)\\n\\n3. intervention to address mind wandering\\nour intervention approach is grounded in the basic idea that\\nlearning of conceptual information involves creating and\\nmaintaining an internal model (mental model) by integrating\\ninformation from the text with prior knowledge from memory\\n[25]. this integration process relies on attentional focus and\\nbreaks down during mind wandering because information from\\nthe external environment is no longer being integrated into the\\ninternal mental model. this results in an impaired model which\\nleads to less effective suppression of off-task thoughts. this\\nincrease in mind wandering further impairs the mental model,10\\n\\n\\x0cresulting in a vicious cycle. our intervention targets this vicious\\ncycle by redirecting attention to the primary task and attempting\\nto correct for comprehension deficits attributed to mind\\nwandering. based on research demonstrating the effectiveness of\\ninterpolated testing [47], we propose that asking questions on\\npages where mind wandering is detected and encouraging rereading in response to incorrect responses will aid in re-directing\\nattention to the text and correct knowledge deficits.\\n\\npage regardless of whether the second question was answered\\ncorrectly, so as not to be overly burdensome.\\n\\n3.1 intervention implementation\\nour initial intervention was implemented for the same text used\\nto create the mind wandering detector (although it could be\\napplied to any text). the text was integrated into the computer\\nreading interface. mind wandering detection occurred when the\\nlearner navigated to the next page using the right arrow key. in\\norder to address ambiguity in mind wandering detection, we used\\nthe detector’s mind wandering likelihood to probabilistically\\ndetermine when to intervene. for example, if the mind wandering\\nlikelihood was 70%, then there was a 70% chance of intervention\\non any given page (all else being equal). we did not intervene for\\nthe first three pages in order to allow the learner to become\\nfamiliar with the text and interface. to reduce disruption, there\\nwas a 50% reduced probability of intervening on adjacent pages,\\nand the maximum number of interventions was capped at 1/3 ×\\nthe number of pages (19 for the present 57-page text). table 2\\npresents pseudo code for when to launch an intervention.\\ntable 2: pseudo code for intervention strategy\\nlaunch_intervention:\\nif current_page >= waitpages\\nand\\ntotal_interventions < maxintrv)\\nand\\ngaze_likelihood > random(0,1)\\nand\\n(!has_intervened(previous_page)\\nor 0.5 < random (0,1)):\\ndo_intervention()\\nelse:\\nshow_next_page()\\ndo_intervention:\\nanswer1 = show_question1()\\nif answer1 is correct:\\nshow_positive_feedback()\\nshow_next_page()\\nelse:\\nshow_neg_feedback()\\nsuggest_rereading()\\nif page advance detected:\\nanswer2 = show_question2();\\nshow_next_page()\\n\\nfigure 2 presents an outline of the intervention strategy. the\\nintervention itself relied on two multiple choice questions for\\neach page (screen) of the text. when the system decided to\\nintervene, one of the questions (randomly selected) was presented\\nto the learner. if the learner answered this online question\\ncorrectly, positive feedback was provided, and the learner could\\nadvance to the next page. if the learner answered incorrectly,\\nnegative feedback was provided, and the system encouraged the\\nlearner to re-read the page. the learner was then provided with a\\nsecond (randomly selected) online question, which could either\\nbe the same or the alternate question for that page. feedback was\\nnot provided and the learner was allowed to advance to the next\\n\\nfigure 2: outline of intervention strategy\\n\\n3.2 iterative refinement\\nthe technology was refined through multiple rounds of formative\\ntesting with 67 participants, recruited from the same institution\\nused to build the detector. participants were observed while\\ninteracting with the technology, their responses were analyzed,\\nand they were interviewed about their experience. we used the\\nfeedback gleaned from these tests to refine the intervention\\nparameters (i.e., when to launch, how many interventions to\\nlaunch, whether to launch interventions on subsequent pages),\\nintervention questions themselves, and instructions on how to\\nattend to the intervention. for example, earlier versions of the\\nintervention used a fixed threshold (instead of the aforementioned\\nprobabilistic approach) to trigger an intervention. despite many\\nattempts to set this threshold, the end result was that some\\nparticipants received many interventions while others received\\nalmost no interventions. this issue was corrected by\\nprobabilistically rather than deterministically launching the\\nintervention. additional testing/refinement of the comprehension\\nquestions used in the intervention was done using crowdsourcing\\nplatforms, specifically amazon’s mechanical turk (mturk).\\n\\n4. evaluation study\\nwe conducted a randomized controlled trial to evaluate the\\ntechnology. the experiment had two conditions: an intervention\\ncondition and a yoked control condition (as described below). the\\nyoked control was needed to verify that any learning benefits are\\nattributed to the technology being sensitive to mind wandering\\nand not merely to the added opportunities to answer online\\nquestions and re-read. this is because we know that interpolated\\ntesting itself has beneficial comprehension effects [47].\\n\\n4.1 method\\nparticipants (n = 104) were a new set of undergraduate students\\nwho participated to fulfill research credit requirements. they\\nwere recruited from the same university used to build the mw\\ndetector and for the iterative testing and refinement cycles.\\nwe did not use a pretest because we expected participants to be\\nunfamiliar with the topic. participants were not informed that the\\ninterface would be tracking their mind wandering (until the11\\n\\n\\x0cdebriefing at the end), instead, they were instructed as follows:\\n“while reading the text, you will occasionally be asked some\\nquestions about the page you just read. depending on your\\nanswer, you will re-read the same page and you will be asked\\nanother question that may or may not be the same question.”\\nparticipants in the intervention condition received the\\nintervention as described above (i.e., based on detected mind\\nwandering likelihoods). each participant in the yoked control\\ncondition was paired with a participant in the intervention\\ncondition. he or she received an intervention question on the\\nsame pages as their paired intervention participant regardless of\\nmind wandering likelihood. for example, if participant a (i.e.,\\nintervention condition) received questions on pages 5, 7, 10, and\\n25, participant b (i.e., yoked control condition) would receive\\nintervention questions on the same pages. however, if the yoked\\nparticipant answered incorrectly, then (s)he had the opportunity\\nto re-read and answer another question regardless of the outcome\\nof their intervention-condition partner.\\nafter reading, participants completed a 38-item multiple choice\\ncomprehension assessment to measure learning. the questions\\nwere randomly selected from the 57 pages (one per page) with the\\nexception that a higher selection priority was given to pages that\\nwere re-read on account of the intervention. participants in the\\nyoked control condition received the same posttest questions as\\ntheir intervention condition counterparts.\\n\\n4.2 results\\nparticipants received an average of 16 (min of 7 and max of 19)\\ninterventions. they spent an average of 27.5 seconds on each\\nscreen prior to receiving an intervention. there was no significant\\ndifference across conditions (p = .998), suggesting that reading\\ntime was not a confound. in what follows, we compared each\\nintervention participant to his/her yoked control with a two-tailed\\npaired-samples t-test and a 0.05 criteria for statistical\\nsignificance.\\nmind wandering detection. the detector’s likelihood of mind\\nwandering was slightly higher for participants in the yokedcontrol condition (m = .431; sd = .170) compared to the\\nintervention condition (m = .404; sd = .112), but the difference\\nwas not statistically significant (p = .348). this was unsurprising\\nas participants in both groups received the same interventions,\\nwhich itself was expected to reduce mind wandering. importantly,\\nmind wandering likelihoods were negatively correlated with\\nperformance on the online questions (r = -.296, p = .033) as well\\nas on posttest questions (r = -.319, p = .021). this provides\\nevidence for the validity of the mind wandering detector when\\napplied to a new set of learners and under different conditions\\n(i.e., reading interspersed with online questions compared to\\nuninterrupted reading).\\ncomprehension assessment. there was some overlap between\\nthe online questions and the posttest questions. to obtain an\\nunbiased estimate of learning, we only analyzed performance on\\npreviously unseen posttest questions. that is, questions that were\\nused as part of the intervention were first removed before\\ncomputing posttest scores.\\nthere were no significant condition differences on overall\\nposttest scores (p = .846). the intervention condition answered\\n57.6% (sd = .157) of the questions correctly while the yoked\\ncontrol condition answered 58.1% (sd = .129) correctly. this\\nfinding was not surprising as both conditions received the exact\\nsame treatment except that the interventions were triggered based\\n\\non detected mind wandering in the intervention condition but not\\nthe control condition.\\nnext, we examined posttest performance as a function of mind\\nwandering during reading. each page was designated as a low or\\nhigh mind wandering page based on a median split of mind\\nwandering likelihoods (medians = .35 and .36 on a 0 to 1 scale for\\nintervention and control conditions, respectively). we then\\nanalyzed performance on posttest questions corresponding to\\npages with low vs. high likelihoods of mind wandering (during\\nreading). the results are shown in table 3.\\nwe found no significant posttest differences on pages where both\\nthe intervention and control participants had low (p = .759) or\\nhigh (p = .922) mind wandering likelihoods (first and last rows in\\ntable 3, respectively). there was also no significant posttest\\ndifference (p = .630) for pages where the intervention condition\\nhad high mind wandering likelihoods but the control condition\\nhad low mind wandering likelihoods (row 3). however, the\\nintervention condition significantly (p = .003, d = .47 sigma)\\noutperformed the control condition for pages where the\\nintervention participants had low likelihoods of mind wandering\\nbut control participants had high mind wandering likelihoods\\n(row 2). these last two finding suggests that the intervention had\\nthe intended effect of reducing comprehension deficits\\nattributable to mind wandering because it led to equitable\\nperformance when mind wandering was high and improved\\nperformance when it was low.\\ntable 3: posttest performance (proportion of correct\\nresponses) as a function of mind wandering during reading.\\nstandard deviations in parenthesis.\\nmind\\nwandering\\n\\nposttest\\n\\nn\\n\\nint.\\n\\ncntrl.\\n\\nint.\\n\\ncntrl.\\n\\n43\\n\\nlow\\n\\nlow\\n\\n.604 (.288)\\n\\n.623 (.287)\\n\\n40\\n\\nlow\\n\\nhigh\\n\\n.643 (.263)\\n\\n.489 (.298)\\n\\n43\\n\\nhigh\\n\\nlow\\n\\n.535 (.295)\\n\\n.566 (.305)\\n\\n45\\n\\nhigh\\n\\nhigh\\n\\n.522 (.312)\\n\\n.515 (.291)\\n\\nscores\\n\\nnote. int. = intervention. cntrl. = control. bolded cells represent a\\nstatistically significant difference. n = number of pairs (out of 52) in each\\nanalysis. it differs slightly across analyses as not all participants were\\nassigned to each mind wandering group.\\n\\nafter-task interview. we interviewed a subset of the participants\\nin order to gauge their subjective experience with the\\nintervention. a few key themes emerged. participants reported\\npaying closer attention to the text after realizing they would be\\nperiodically answering multiple-choice questions. this was good.\\nhowever, participants also reported that they adapted their\\nreading strategies in one of two ways in response to the questions.\\nsince the questions targeted factual information (sometimes\\nverbatim) from the text, some participants paid more attention to\\ndetails and precise wordings instead of the broader concepts being\\ndiscussed in the text. more discouragingly, some participants\\nreported adopting a preemptive skimming strategy in that they\\nwould only look for keywords that they expected to appear in a\\nsubsequent question.\\nparticipants were encouraged to re-read text when they answered\\nincorrectly before receiving another question (or the same\\nquestion in some cases). many participants reported simply\\nscanning the text (when re-reading) to locate keywords from the\\nquestion before moving on. since the scanning strategy was often12\\n\\n\\x0csuccessful to answer the subsequent question, participants\\nreported that the questions were too easy and it took relatively\\nlittle effort to locate the correct answer compared to re-reading.\\nthey suggested that it may have been better if the questions had\\ntargeted key concepts rather than facts.\\nfinally, participants reported difficulties with re-engaging with\\nthe text after answering an online question because the text was\\ncleared when an intervention question was displayed; an item that\\ncan be easily corrected in subsequent versions.\\n\\n5. discussion\\nwe developed the first educational technology capable of realtime mind wandering detection and dynamic intervention during\\ncomputerized reading. in the remainder of this section, we discuss\\nthe significance of our main findings, limitations, and avenues for\\nfuture work.\\n\\n5.1 significance of main findings\\nwe have three main findings. first, we demonstrated that a\\nmachine-learned mind wandering detector built in one context\\ncan be applied to a different (albeit related) interaction context.\\nspecifically, the detector was trained on a data set involving\\nparticipants silently reading and self-reporting mind wandering,\\nbut was applied to an interactive context involving interpolated\\nassessments, which engendered different reading strategies.\\nfurther, self-reports of mind wandering were not collected in this\\ninteractive context, which might have influenced mind wandering\\nrates in and of itself. despite these differences, we were able to\\ndemonstrate the predictive validity of the detector by showing\\nthat it negatively correlated with both online and offline\\ncomprehension scores when evaluated on new participants.\\nsecond, we showed promising effects for our intervention\\napproach despite a very conservative experimental design, which\\nensured that the intervention and control groups were equated\\nalong all respects, except that the intervention was triggered based\\non the mind wandering detector (key manipulation). further, we\\nused a probabilistic approach to trigger an intervention, because\\nthe detector is inherently imperfect. as a result, participants could\\nhave received an intervention when they were not mind\\nwandering and/or could have failed to receive one when they were\\nmind wandering. therefore, it was essential to compare the two\\ngroups under conditions when the mind wandering levels\\ndiffered. this more nuanced analysis revealed that although the\\nintervention itself did not lead to a boost in overall comprehension\\n(because it is remedial), it equated comprehension scores when\\nmind wandering was high (i.e., scores for the intervention group\\nwere comparable when the control group was low on mind\\nwandering). it also demonstrated the cost of not intervening\\nduring mind wandering (i.e., scores for the intervention group\\nwere greater when the control group was high on mind\\nwandering). in other words, the intervention was successful in\\nmitigating the negative effects of mind wandering.\\nthird, despite the advantages articulated above, the intervention\\nitself was reactive and engendered several unintended (and\\npresumably suboptimal) behaviors. in particular, students altered\\ntheir reading strategies in response to the interpolated questions,\\nwhich were a critical part of the intervention. in a sense, they\\nattempted to “game the intervention” by attempting to proactively\\npredict the types of questions they might receive and then\\nadopting a complementary reading strategy consisting of\\nskimming and/or focusing on factual information. this reliance\\non surface- rather than deeper-levels of processing was\\nincongruent with our goal of promoting deep comprehension.\\n\\n5.2 limitations\\nthere are a number of methodological limitations with this work\\nthat go beyond limitations with the intervention (as discussed\\nabove). first, we focused on a single text that is perceived as\\nbeing quite dull and consequently triggers rather high levels of\\nmind wandering [26]. this raises the question of whether the\\ndetector will generalize to different texts. we expect some level\\nof generalizability in terms of features used because the detector\\nonly used content- and position- (on the screen) free global gaze\\nfeatures. however, given that several supervised classifiers are\\nvery sensitive to differences in base rates, the detector might overor under- predict mind wandering when applied to texts that\\nengender different rates of mind wandering. therefore, retraining\\nthe detector with a more diverse set of texts is warranted.\\nanother limitation is the scalability of our learning technology.\\nthe eye tracker we used was a cost-prohibitive tobii tx300 that\\nwill not scale beyond the laboratory. fortunately, commercialoff-the-shelf (cots) eye trackers, such as eye tribe and tobii\\neyex, can be used to surpass this limitation. it is an open question\\nas to whether the mind wandering detector can operate with\\nsimilar fidelity with these cots eye trackers. our use of global\\ngaze features which do not require high-precision eye tracking\\nholds considerable promise in this regard. nevertheless,\\nreplication with scalable eye trackers and/or scalable alternatives\\nto eye tracking (e.g., facial-feature tracking [46] or monitoring\\nreading patterns [27]) is an important next step (see section 5.3).\\nour use of surface-level questions for both the intervention and\\nthe subsequent comprehension assessment is also a limitation as\\nis the lack of a delayed comprehension assessment. it might be\\nthe case that the intervention effects manifest as richer encodings\\nin long-term memory, a possibility that cannot be addressed in the\\ncurrent experiment that only assessed immediate learning.\\nother limitations include a limited student sample (i.e.\\nundergraduates from a private midwestern college) and a\\nlaboratory setup. it is possible that the results would not\\ngeneralize to a more diverse student population or in more\\necological environments (but see below for evidence of\\ngeneralizability of the detector in classroom environments).\\nreplication with data from more diverse populations and\\nenvironments would be a necessary next step to increase the\\necological validity of this work.\\n\\n5.3 future work\\nour future work is progressing along two main fronts. one is to\\naddress limitations in the intervention and design of the\\nexperimental evaluation as discussed above. accordingly, we are\\nexploring alternative intervention strategies, such as: (a) tagging\\nitems for future re-study rather than interrupting participants\\nduring reading; (b) highlighting specific portions of the text as an\\novert cue to facilitate comprehension of critical information; (c)\\nasking fewer intervention questions, but selecting inference\\nquestions that target deeper levels of comprehension and that span\\nmultiple pages of the text; and (d) asking learners to engage in\\nreflection by providing written self-explanations of the textual\\ncontent. we are currently evaluating one such redesigned\\nintervention – open-ended questions targeting deeper levels of\\ncomprehension (item c). our revised experimental design taps\\nboth surface- and inference-level comprehension and assesses\\ncomprehension immediately after reading (to measure learning)\\nand after a one-week delay (to measure retention).\\nwe are also developing attention-aware versions of more\\ninteractive interfaces, such as learning with an intelligent tutoring13\\n\\n\\x0csystem called gurututor [30]. this project also addresses some\\nof the scalability concerns by replacing expensive research-grade\\neye tracking with cost-effective cots eye tracking (e.g., the eye\\ntribe or tobii eyex) and provides evidence for real-world\\ngeneralizability by collecting data in classrooms rather than the\\nlab. we recently tested our implementation on 135 students (total)\\nin a noisy computer-enabled high-school classroom where eyegaze of entire classes of students was collected during their\\nnormal class periods [20]. using a similar approach to the present\\nwork, we used the data to build and validate a studentindependent gaze-based mind wandering detector. the resultant\\nmind wandering detection accuracy (f1 of 0.59) was substantially\\ngreater than chance (f1 of 0.24) and outperformed earlier work on\\nthe same domain [21]. the next step is to develop interventions\\nthat redirect attention and correct learning deficiencies\\nattributable to mind wandering and to test the interventions in\\nreal-world environments. by doing so, we hope to advance our\\nfoundational vision of developing next-generation technologies\\nthat enhance the process and products of learning by “attending\\nto attention.”\\n\\n[6]\\n\\n[7]\\n\\n[8]\\n\\n[9]\\n\\n[10]\\n\\n[11]\\n\\n[12]\\nfigure 3: guru tutor interface overlaid with eye-gaze\\nobtained via the eyetribe\\n\\n[13]\\n\\n6. acknowledgements\\nthis research was supported by the national science foundation\\n(nsf) (drl 1235958 and iis 1523091). the authors are grateful\\nto kris kopp and jenny wu for their contributions to the study.\\nany opinions, findings and conclusions, or recommendations\\nexpressed in this paper are those of the authors and do not\\nnecessarily reflect the views of nsf.\\n\\n[14]\\n\\n[15]\\n\\n7. references\\n[1] anderson, j.r. 2002. spanning seven orders of magnitude:\\na challenge for cognitive modeling. cognitive science, 26\\n(1), 85-112.\\n[2] baird, b., smallwood, j., mrazek, m.d., kam, j.w.,\\nfranklin, m.s. and schooler, j.w. 2012. inspired by\\ndistraction mind wandering facilitates creative incubation.\\npsychological science, 23 (10), 1117-1122.\\n[3] bixler, r. and d'mello, s.k. 2016. automatic gaze-based\\nuser-independent detection of mind wandering during\\ncomputerized reading. user modeling & user-adapted\\ninteraction, 26, 33-68.\\n[4] boys, c.v. 1895. soap bubbles, their colours and the forces\\nwhich mold them. society for promoting christian\\nknowledge.\\n[5] conati, c., aleven, v. and mitrovic, a. 2013. eye-tracking\\nfor student modelling in intelligent tutoring systems. in\\nsottilare, r., graesser, a., hu, x. and holden, h. eds.\\ndesign recommendations for intelligent tutoring systems -\\n\\n[16]\\n\\n[17]\\n\\n[18]\\n\\n[19]\\n\\nvolume 1: learner modeling, army research laboratory,\\norlando, fl.\\nconati, c. and merten, c. 2007. eye-tracking for user\\nmodeling in exploratory learning environments: an\\nempirical evaluation. knowledge-based systems, 20 (6),\\n557-574.\\nd'mello, s., olney, a., williams, c. and hays, p. 2012.\\ngaze tutor: a gaze-reactive intelligent tutoring system.\\ninternational journal of human-computer studies, 70 (5),\\n377-398.\\nd'mello, s.k. 2016. giving eyesight to the blind: towards\\nattention-aware aied. international journal of artificial\\nintelligence in education, 26 (2), 645-659.\\nd'mello, s.k., blanchard, n., baker, r., ocumpaugh, j. and\\nbrawner, k. 2014. i feel your pain: a selective review of\\naffect-sensitive instructional strategies. in sottilare, r.,\\ngraesser, a., hu, x. and goldberg, b. eds. design\\nrecommendations for adaptive intelligent tutoring\\nsystems: adaptive instructional strategies (volume 2), us\\narmy research laboratory, orlando, fl.\\nd'mello, s.k., kopp, k., bixler, r. and bosch, n. 2016.\\nattending to attention: detecting and combating mind\\nwandering during computerized reading in extended\\nabstracts of the acm sigchi conference on human\\nfactors in computing systems (chi 2016), acm, new\\nyork.\\ndrummond, j. and litman, d. 2010. in the zone: towards\\ndetecting student zoning out using supervised machine\\nlearning. in aleven, v., kay, j. and mostow, j. eds.\\nintelligent tutoring systems., springer-verlag, berlin /\\nheidelberg.\\neastwood, j.d., frischen, a., fenske, m.j. and smilek, d.\\n2012. the unengaged mind: defining boredom in terms of\\nattention. perspectives on psychological science, 7 (5), 482495.\\nfaber, m., bixler, r. and d'mello, s.k. in press. an\\nautomated behavioral measure of mind wandering during\\ncomputerized reading. behavior research methods.\\nfranklin, m.s., broadway, j.m., mrazek, m.d., smallwood,\\nj. and schooler, j.w. 2013. window to the wandering mind:\\npupillometry of spontaneous thought while reading. the\\nquarterly journal of experimental psychology, 66 (12),\\n2289-2294.\\ngluck, k.a., anderson, j.r. and douglass, s.a. 2000.\\nbroader bandwidth in student modeling: what if its were\\n“eye” ts? in gauthier, c., frasson, c. and vanlehn, k. eds.\\nproceedings of the 5th international conference on\\nintelligent tutoring systems, springer, berlin.\\ngraesser, a., lu, s., olde, b., cooper-pye, e. and whitten,\\ns. 2005. question asking and eye tracking during cognitive\\ndisequilibrium: comprehending illustrated texts on devices\\nwhen the devices break down. memory and cognition, 33,\\n1235-1247.\\nhanley, j.a. and mcneil, b.j. 1982. the meaning and use\\nof the area under a receiver operating characteristic (roc)\\ncurve. radiology, 143 (1), 29-36.\\nharley, j.m., lajoie, s.p., frasson, c. and hall, n.c. in\\npress. developing emotion-aware, advanced learning\\ntechnologies: a taxonomy of approaches and features.\\ninternational journal of artificial intelligence in education.\\nhegarty, m. and just, m. 1993. constructing mental models\\nof machines from text and diagrams. journal of memory and\\nlanguage, 32 (6), 717-742.14\\n\\n\\x0c[20] hutt, s., mills, c., bosch, n., krasich, k., brockmole, j.r.\\nand d'mello, s.k. in review. out of the fr-eye- ing pan:\\ntowards gaze-based models of attention during learning\\nwith technology in the classroom.\\n[21] hutt, s., mills, c., white, s., donnelly, p.j. and d’mello,\\ns.k. 2016. the eyes have it: gaze-based detection of mind\\nwandering during learning with an intelligent tutoring\\nsystem. in proceedings of the 9th international conference\\non educational data mining (edm 2016), international\\neducational data mining society.\\n[22] jaques, n., conati, c., harley, j.m. and azevedo, r. year.\\npredicting affect from gaze data during interaction with an\\nintelligent tutoring system. in intelligent tutoring systems,\\n(2014), springer, 29-38.\\n[23] kardan, s. and conati, c. 2012. exploring gaze data for\\ndetermining user learning with an interactive simulation. in\\ncarberry, s., weibelzahl, s., micarelli, a. and semeraro, g.\\neds. proceedings of the 20th international conference on\\nuser modeling, adaptation, and personalization (umap\\n2012), springer, berlin.\\n[24] killingsworth, m.a. and gilbert, d.t. 2010. a wandering\\nmind is an unhappy mind. science, 330 (6006), 932-932.\\n[25] kintsch, w. 1998. comprehension: a paradigm for\\ncognition. cambridge university press, new york.\\n[26] kopp, k., d’mello, s. and mills, c. 2015. influencing the\\noccurrence of mind wandering while reading. consciousness\\nand cognition, 34 (1), 52-62.\\n[27] mills, c. and d’mello, s.k. 2015. toward a real-time (day)\\ndreamcatcher: detecting mind wandering episodes during\\nonline reading. in romero, c., pechenizkiy, m., boticario,\\nj. and santos, o. eds. proceedings of the 8th international\\nconference on educational data mining (edm 2015),\\ninternational educational data mining society.\\n[28] mooneyham, b.w. and schooler, j.w. 2013. the costs and\\nbenefits of mind-wandering: a review. canadian journal of\\nexperimental psychology/revue canadienne de psychologie\\nexpérimentale, 67 (1), 11.\\n[29] muir, m. and conati, c. 2012. an analysis of attention to\\nstudent–adaptive hints in an educational game. in cerri,\\ns.a., clancey, w.j., papadourakis, g. and panourgia, k.\\neds. proceedings of the international conference on\\nintelligent tutoring systems, springer, berlin.\\n[30] olney, a., d'mello, a., person, n., cade, w., hays, p.,\\nwilliams, c., lehman, b. and graesser, a. 2012. guru: a\\ncomputer tutor that models expert human tutors. in cerri, s.,\\nclancey, w., papadourakis, g. and panourgia, k. eds.\\nproceedings of the 11th international conference on\\nintelligent\\ntutoring\\nsystems,\\nspringer-verlag,\\nberlin/heidelberg.\\n[31] olney, a., risko, e.f., d'mello, s.k. and graesser, a.c.\\n2015. attention in educational contexts: the role of the\\nlearning task in guiding attention. in fawcett, j., risko, e.f.\\nand kingstone, a. eds. the handbook of attention, mit\\npress, cambridge, ma.\\n[32] pham, p. and wang, j. 2016. adaptive review for mobile\\nmooc learning via implicit physiological signal sensing.\\nin proceedings of the 18th acm international conference\\non multimodal interaction (icmi 2016), acm, new york,\\nny.\\n[33] pham, p. and wang, j. 2015. attentivelearner: improving\\nmobile mooc learning via implicit heart rate tracking. in\\ninternational conference on artificial intelligence in\\neducation, springer, berlin heidelberg.\\n\\n[34] randall, j.g., oswald, f.l. and beier, m.e. 2014. mindwandering, cognition, and performance: a theory-driven\\nmeta-analysis of attention regulation. psychological\\nbulletin, 140 (6), 1411-1431.\\n[35] rapp, d.n. 2006. the value of attention aware systems in\\neducational settings. computers in human behavior, 22 (4),\\n603-614.\\n[36] rayner, k. 1998. eye movements in reading and information\\nprocessing: 20 years of research. psychological bulletin, 124\\n(3), 372-422.\\n[37] reichle, e.d., reineberg, a.e. and schooler, j.w. 2010. eye\\nmovements during mindless reading. psychological science,\\n21 (9), 1300.\\n[38] risko, e.f., buchanan, d., medimorec, s. and kingstone, a.\\n2013. everyday attention: mind wandering and computer\\nuse during lectures. computers & education, 68 (1), 275283.\\n[39] roda, c. and thomas, j. 2006. attention aware systems:\\ntheories, applications, and research agenda. computers in\\nhuman behavior, 22 (4), 557-587.\\n[40] rowe, j., mott, b., mcquiggan, s., robison, j., lee, s. and\\nlester, j. year. crystal island: a narrative-centered learning\\nenvironment for eighth grade microbiology. in workshop on\\nintelligent educational games at the 14th international\\nconference on artificial intelligence in education,\\nbrighton, uk, (2009), 11-20.\\n[41] shute, v.j., ventura, m., bauer, m. and zapata-rivera, d.\\n2009. melding the power of serious games and embedded\\nassessment to monitor and foster learning: flow and grow.\\nin ritterfeld, u., cody, m. and vorderer, p. eds. serious\\ngames: mechanisms and effects, routledge, taylor and\\nfrancis, mahwah, nj.\\n[42] sibert, j.l., gokturk, m. and lavine, r.a. 2000. the reading\\nassistant: eye gaze triggered auditory prompting for reading\\nremediation. in proceedings of the 13th annual acm\\nsymposium on user interface software and technology,\\nacm, new york, ny.\\n[43] smallwood, j., davies, j.b., heim, d., finnigan, f.,\\nsudberry, m., o'connor, r. and obonsawin, m. 2004.\\nsubjective experience and the attentional lapse: task\\nengagement and disengagement during sustained attention.\\nconsciousness and cognition, 13 (4), 657-690.\\n[44] smallwood, j., fishman, d.j. and schooler, j.w. 2007.\\ncounting the cost of an absent mind: mind wandering as an\\nunderrecognized influence on educational performance.\\npsychonomic bulletin & review, 14 (2), 230-236.\\n[45] smallwood, j. and schooler, j.w. 2015. the science of mind\\nwandering: empirically navigating the stream of\\nconsciousness. annu. rev. psychol, 66, 487-518.\\n[46] stewart, a., bosch, p., chen, h., donnelly, p.j. and\\nd’mello, s.k. 2016. where's your mind at? video-based\\nmind wandering detection during film viewing. in aroyo,\\nl., d'mello, s., vassileva, j. and blustein, j. eds.\\nproceedings of the 2016 acm on international conference\\non user modeling, adaptation, & personalization (acm\\numap 2016), acm, new york.\\n[47] szpunar, k.k., khan, n.y. and schacter, d.l. 2013.\\ninterpolated memory tests reduce mind wandering and\\nimprove learning of online lectures. proceedings of the\\nnational academy of sciences, 110 (16), 6313-6317.\\n[48] wang, h., chignell, m. and ishizuka, m. 2006. empathic\\ntutoring software agents using real-time eye tracking. in\\nproceedings of the 2006 symposium on eye tracking\\nresearch &applications, acm, new york.\""
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) get all the data from the text files into the \"documents\" list\n",
    "# P.S. make sure you use the 'utf-8' encoding\n",
    "documents = []\n",
    "for paper in papers:\n",
    "    file=open(paper,'r')\n",
    "    documents.append(file.read())\n",
    "documents[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\f",
      "zone out no more: mitigating mind wandering during\n",
      "computerized reading\n",
      "sidney k. d’mello, caitlin mills, robert bixler, & nigel bosch\n",
      "university of notre dame\n",
      "118 haggar hall\n",
      "notre dame, in 46556, usa\n",
      "sdmello@nd.edu\n",
      "\n",
      "abstract\n",
      "mind wandering, defined as shifts in attention from task-related\n",
      "processing to task-unrelated thoughts, is a ubiquitous\n",
      "phenomenon that has a negative influence on performance and\n",
      "productivity in many contexts, including learning. we propose\n",
      "that next-generation learning technologies should have some\n",
      "mechanism to detect and respond to mind wandering in real-time.\n",
      "towards this end, we developed a technology that automatically\n",
      "detects mind wandering from eye-gaze during learning from\n",
      "instructional texts. when mind wandering is detected, the\n",
      "technology intervenes by posing just-in-time questions and\n",
      "encouraging re-reading as needed. after multiple rounds of\n",
      "iterative refinement, we summatively compared the technology to\n",
      "a yoked-control in an experiment with 104 par\n"
     ]
    }
   ],
   "source": [
    "# 3) print the first 1000 characters of the first document to see what it \n",
    "# looks like (we'll use this as a sanity check below)\n",
    "print(documents[12][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40387 40313\n",
      "37214 35568\n",
      "44037 33209\n",
      "45258 40413\n",
      "32277 42524\n",
      "47851 28901\n",
      "42617 41827\n",
      "49177 35430\n",
      "40655 43028\n",
      "47377 33398\n",
      "46761 43296\n",
      "31574 42473\n",
      "50043 28481\n",
      "41110 39537\n",
      "42046 35694\n",
      "47845 37927\n",
      "45724 44405\n"
     ]
    }
   ],
   "source": [
    "# 4) only select the text that's between the first occurence of the \n",
    "# the word \"abstract\" and the last occurence of the word \"reference\"\n",
    "# Optional: print the length of the string before and after, as a \n",
    "# sanity check\n",
    "# HINT: https://stackoverflow.com/questions/14496006/finding-last-occurrence-of-substring-in-string-replacing-that\n",
    "# read more about rfind: https://www.tutorialspoint.com/python/string_rfind.htm\n",
    "abstract=[]\n",
    "for doc in documents: \n",
    "    print(len(doc),end)\n",
    "    start = doc.rfind('\\nabstract\\n')+len('abstract')\n",
    "    end = doc.rfind('reference')\n",
    "    doc = abstract.append(doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"103\\n\\n\\x0cepistemic network analysis and topic modeling for chat\\ndata from collaborative learning environment\\nzhiqiang cai\\n\\nbrendan eagan\\n\\nnia m. dowell\\n\\nthe university of memphis\\n365 innovation drive, suite 410\\nmemphis, tn, usa\\n\\nuniversity of wisconsin-madison\\n1025 west johnson street\\nmadison, wi, usa\\n\\nthe university of memphis\\n365 innovation drive, suite 410\\nmemphis, tn, usa\\n\\nzcai@memphis.edu\\n\\neaganb@gmail.com\\n\\nniadowell@gmail.com\\n\\njames w. pennebaker\\n\\ndavid w. shaffer\\n\\narthur c. graesser\\n\\nuniversity of texas-austin\\n116 inner campus dr stop g6000\\naustin, tx, usa\\n\\nuniversity of wisconsin-madison\\n1025 west johnson street\\nmadison, wi, usa\\n\\nthe university of memphis\\n365 innovation drive, suite 403\\nmemphis, tn, usa\\n\\npennebaker@utexas.edu\\n\\ndws@education.wisc.edu\\n\\nart.graesser@gmail.com\\n\\nabstract\\nthis study investigates a possible way to analyze chat data from\\ncollaborative learning environments using epistemic network\\nanalysis and topic modeling. a 300-topic general topic model\\nbuilt from tasa (touchstone applied science associates) corpus was used in this study. 300 topic scores for each of the 15,670\\nutterances in our chat data were computed. seven relevant topics\\nwere selected based on the total document scores. while the aggregated topic scores had some power in predicting students’\\nlearning, using epistemic network analysis enables assessing the\\ndata from a different angle. the results showed that the topic\\nscore based epistemic networks between low gain students and\\nhigh gain students were significantly different (𝑡 = 2.00). overall,\\nthe results suggest these two analytical approaches provide complementary information and afford new insights into the processes\\nrelated to successful collaborative interactions.\\n\\nkeywords\\nchat; collaborative learning; topic modeling; epistemic network\\nanalysis\\n\\n1. introduction\\ncollaborative learning is a special form of learning and interaction\\nthat affords opportunities for groups of students to combine cognitive resources and synchronously or asynchronously participate in\\ntasks to accomplish shared learning goals [15; 20]. collaborative\\nlearning groups can range from a pair of learners (called a dyad),\\nto small groups (3-5 learners), to classroom learning (25-35 learners), and more recently large-scale online learning environments\\nwith hundreds or even thousands of students [5; 22]. the collaborative process provides learners with a more efficient learning\\nexperience and improves learners’ collaborative learning skills,\\nwhich are critical competencies for students [14]. members in a\\nteam are different in many ways. they have their own experience,\\nknowledge, skills, and approaches to learning. a student in a col-\\n\\nlaborative learning environment can take other students’ views\\nand ideas about the information provided in the learning material.\\nthe ideas coming out of the team can then be integrated as a\\ndeeper understanding of the material, or a better solution to a\\nproblem.\\ntraditional collaborative learning occurred in the form of face to\\nface group discussion or problem solving. as the internet and\\nlearning technologies develop, online collaborative learning environments come out and are playing more and more important\\nroles. for example, moocs (massive open online courses) have\\ndrawn massive number of learners. learners in moocs are connected by the internet and can easily interact with each other using\\nvarious types of tools, such as forums, blogs and social networks\\n[23]. these digitized environments make it possible to track the\\nlearning processes in collaborative learning environments in\\ngreater detail.\\ncommunication is one of the main factors that differentiates collaborative learning from individual learning [4; 6; 9]. as such,\\nchats from collaborative learning environments provide rich data\\nthat contains information about the dynamics in a learning process. understanding massive chat data from collaborative learning\\nenvironments is interesting and challenging. many tools have\\nbeen invented and used in chat data analysis, such as liwc (linguistic inquiry and word count) [12], coh-metrix [10], and topic\\nmodeling, just to name a few. epistemic network analysis (ena)\\nhas been playing a unique role in analyzing chat data from epistemic games [18]. ena is rooted in a specific theory of learning:\\nthe epistemic frame theory, in which the collection of skill,\\nknowledge, identity, value and epistemology (skive) forms an\\nepistemic frame. a critical theoretical assumption of ena is that\\nthe connections between the elements of epistemic frames are\\ncritical for learning, not their presence in isolation. the online\\nena toolkit allows users to analyze chat data by comparing the\\nconnections within the epistemic networks derived from chats.\\nena visualization displays the clustering of learners and groups\\nand the network connections of individual learners and groups.\\nena requires coded data which has traditionally relied on hand\\ncoded data sets or classifiers that rely on regular expression mapping. combining topic modeling with ena will provide a new\\nmode of preparing data sets for analysis using ena.\\nin this study, we used a combination of topic modeling and ena\\nto analyze chat data to see if we could detect differences between\\nthe connections made by students with high learning gains versus\\nstudents with low learning gains. incorporating topic modeling104\\n\\n\\x0cwith ena will make the analytic tool more fully automated and of\\ngreater use to the research community.\\n\\n2. related work\\nchats have two obvious features. first, they appear in the form of\\ntext. therefore, any text analysis tool may have a role in chat\\nanalysis. second, chats come from individuals’ interaction, which\\nreflects social dynamics between participants. therefore, a combination of text analysis and social network analysis should be\\nhelpful in understanding underlying chat dynamics. for instance,\\ntuulos et al. [21] combined topic modeling with social network\\nanalysis in chat data analysis. they found that topic modeling can\\nhelp identify the receiver of chats (the person who a chat is given\\nto).\\nin a similar effort, scholand et al. [16] combined liwc and social\\nnetwork analysis to form a method called “social language network analysis” (slna). the social networks were formed by\\ncounting the number of times chat occurred between any two\\nparticipants. based on the counts, participants were clustered into\\na tree structure, representing the level of subgroups the participants belong to. liwc was then used to get the text features of\\nchats. it was found that, some liwc features were significantly\\ndifferent between in group conversations and out of group conversations.\\nresearchers have also recently explored the advantages of combining sna (social network analysis) with deeper level computational linguistic tools, like coh-metrix. coh-metrix computes\\nover 100 text features. the five most important coh-metrix features are: narrativity, syntax simplicity, word concreteness, referential cohesion and deep cohesion. dowell and colleagues [8]\\nexplored the extent to which characteristics of discourse diagnostically reveals learners’ performance and social position in\\nmoocs. they found that learners who performed significantly\\nbetter engaged in more expository style discourse, with surface\\nand deep level cohesive integration, abstract language, and simple\\nsyntactic structures. however, linguistic profiles of the centrally\\npositioned learners differed from the high performers. learners\\nwith a more significant and central position in their social network\\nengaged using a more narrative style discourse with less overlap\\nbetween words and ideas, simpler syntactic structures and abstract\\nwords. an increasing methodological contribution of this work\\nhighlights how automated linguistic analysis of student interactions can complement social network analysis (sna) techniques\\nby adding rich contextual information to the structural patterns of\\nlearner interactions.\\n\\nfinal sample. within the population, 50.5% of the sample identified as caucasian, 22.2% as hispanic/latino, 15.4% as asian\\namerican, 4.4% as african american, and less than 1% identified\\nas either native american or pacific islander.\\ncourse details and procedure. students were told that they\\nwould be participating in an assignment that involved a collaborative discussion on personality disorders and taking quizzes. students were told that their assignment was to log into an online\\neducational platform specific to the university at a specified time,\\nwhere they would take quizzes and interact via web chat with one\\nto four random group members. students were also instructed\\nthat, prior to logging onto the educational platform, they would\\nhave to read material on personality disorders. after logging into\\nthe system, students took a 10 item, multiple choice pretest quiz.\\nthis quiz asked students to apply their knowledge of personality\\ndisorders to various scenarios and to draw conclusions based on\\nthe nature of the disorders. the following is an example of the\\ntypes of quiz questions students were exposed to:\\n\\uf0b7\\n\\uf0b7\\n\\n\\uf0b7\\n\\njacob was diagnosed with narcissistic personality disorder. why might dr. simon think this was the wrong\\ndiagnosis?\\ndr. level has measured and described his 10 mice of\\nvarying ages in terms of their length (cm) and weight\\n(g). how might he describe them on these characteristics using a dimensional approach?\\ndanielle checks her facebook page every hour. does\\ndanielle have narcissistic personality disorder?\\n\\nafter completing the quiz, they were randomly assigned to other\\nstudents who were waiting to engage in the chatroom portion of\\nthe task. when there were at least 2 students and no more than 5\\nstudents (m = 4.59), individuals were directed to an instant messaging platform that was built into the educational platform. the\\ngroup chat began as soon as someone typed the first message and\\nlasted for 20 minutes. the chat window closed automatically after\\n20 minutes, at which time students took a second 10 multiplechoice question quiz. each student contributed 154.0 words on\\naverage (sd = 104.9) in 19.5 sentences (sd = 12.5). as a group,\\ndiscussions were about 714.8 words long (sd = 235.7) and 90.6\\nsentences long (sd = 33.5).\\nan excerpt of a collaborative interaction chat in a chat room is\\nshown below in table 1. (student names have been changed):\\ntable 1. an excerpt of a collaborative interaction chat\\nstudent\\n\\nchat text\\n\\nin another study, dowell et al. [7] showed that students’ linguistic\\ncharacteristics, namely higher degrees of narrativity and deep\\ncohesion, are predictive of their learning. that is, students engaged in deep cohesive interactions performed better.\\n\\nart\\n\\nok cool, everyone's here. sooo first question\\n\\nart\\n\\nok so the certain characteristics to be considered to\\nhave a personality disorder?\\n\\nin the present research, we explore collaborative interaction chat\\ndata using the combination of topic modeling and epistemic network analysis. while previous studies focused on the relationship\\nbetween language features and social network connections, our\\nstudy focuses on prediction learning performance by semantic\\nnetwork connections students make in chats.\\n\\nshaffer\\n\\nalright sooo first question: based on these criteria describe several reasons why a psychologist might not\\nlabel someone with grandiose thoughts as having narcissistic personality disorder?\\n\\nshaffer\\n\\nhahaha never mind\\n\\nshaffer\\n\\nthat was the second question.\\n\\n3. methods\\n\\nart\\n\\nlol its all good\\n\\nshaffer\\n\\nokay so certain characteristics: doesn't it have to be like\\na stable thing?\\n\\ncarl\\n\\ni think the main thing about having a disorder is that its\\ndisruptive socially and/or makes the person a danger to\\nhimself or others\\n\\nparticipants. participants were enrolled in an introductory-level\\npsychology course taught in the fall semester of 2011 at a large\\nuniversity in the usa. while 854 students participated in this\\ncourse, some minor data loss occurred after removing outliers and\\nthose who failed to complete the outcome measures. the final\\nsample consisted of 844 students. females made up 64.3% of this105\\n\\n\\x0cvasile\\n\\nyes, stable over time\\n\\nshaffer\\n\\nyeah, and it also mentioned it can't be because of drugs\\n\\nart\\n\\nalso they have to have like unrealistic fantasies\\n\\nnia\\n\\nyeah and not normal in their culture\\n\\ncarl\\n\\nno drugs or physical injury\\n\\nvasile\\n\\nbegins in early adulthood or adolescence\\n\\nshaffer\\n\\ni think that covers them? haha\\n\\nart\\n\\nok, so arrogance doesn't just define it, they have to have\\nmost of these characteristics\\n\\nart\\n\\nyeah i think we got them\\n\\nshaffer\\n\\nis it most or is it like 6?\\n\\nfrom the above excerpt, we can see several obvious things. first,\\nthe lengths of the utterances varied from one single word to multiple sentences. this needs to be considered in text analysis because some methods work only for longer texts. for example,\\ncoh-metrix usually works well for texts with more than 200\\nwords. topic modeling also needs enough length to reliably infer\\ntopic scores. second, the number of utterances each participant\\ngave were different. from how much and what a member said, we\\ncan see each member played a different role in that chat. third,\\nthe ordered sequence of the utterances forms a time series. understanding and visualizing the underlying discourse dynamics are\\nimportant for meaning making with this type of data.\\nthe data set contained 15,670 utterances, pretest scores (the first\\nquiz) and post test scores (the second quiz) for 844 students,\\ngrouped in 182 chat rooms. each chat room had 2 to 5 students,\\n4.73 by average. the average speech turns each student gave was\\n18.2 and the average speech turns in each room was 86.1.\\nthe average pretest score was 36.01% correct and the average\\npost-test scores 45.73% correct. paired sample test shows that the\\npost-test is significantly higher (𝑡 = 14.13, 𝑁 = 844). we computed the learning gain of each student, using the formula\\n𝑔𝑎𝑖𝑛 =\\n\\n𝑝𝑜𝑠𝑡𝑡𝑒𝑠𝑡 𝑠𝑐𝑜𝑟𝑒 − 𝑝𝑟𝑒𝑡𝑒𝑠𝑡 𝑠𝑐𝑜𝑟𝑒\\n1−𝑝𝑟𝑒𝑡𝑒𝑠𝑡 𝑠𝑐𝑜𝑟𝑒\\n\\n.\\n\\nfor all students (𝑁 = 844), the average learning gain is 0.11,\\n59.5% had positive learning gains above 0.1. 16.5% had the same\\nscores and 23% had negative learning gains. not surprisingly,\\nstudents who had lower pretest scores had higher learning gains\\nbecause they had greater potential to learn. figure 1 shows the\\naverage learning gain as function of pretest score.\\n\\n0.6\\n0.4\\n\\nthis data set has been analyzed in multiple studies. cade et al. [3]\\nanalyzed the cohesion of the chats and found that deep cohesion\\nof the chats predicts the students feeling of power and connectedness to the group. dowell et al. [7] found that some coh-metrix\\nmeasures predicts learning. coh-metrix measures describe common textual features that are not content specific. for example,\\ncohesion is about how text segments are semantically linked to\\neach other, which has nothing to do with what the text content is\\nabout. in this study, we use topic modeling to provide content\\ndependent features and use epistemic network analysis to explore\\nhow the topics were associated in the chats.\\n\\n4. topic modeling\\ntopic modeling has been widely used in text analysis to find what\\ntopics are in a text and what proportion/amount of each topic is\\ncontained. latent dirichlet allocation (lda) [2; 24] is one of the\\nmost popular methods for topic modeling. lda uses a generative\\nprocess to find topic representations. lda starts from a large\\ndocument set 𝐷 = {𝑑1 , 𝑑2 , ⋯ , 𝑑𝑚 }. a word list 𝑊 =\\n{𝑤1 , 𝑤2 , ⋯ , 𝑤𝑛 } is then extracted from the document set. lda\\nassumes that the document set contains a certain number of topics,\\nsay, k topics. each document has a probability distribution over\\nthe k topics and each topic has a probability distribution over the\\ngiven list of words. when a document was composed, each word\\nthat occurred in a document was assumed to be drawn based on\\nthe document-topic probability and the topic-word probability.\\nfor a given corpus (document set) and a given number of topics\\nk, lda can compute the topic assignment of each word in each\\ndocument.\\nfor a given topic, the word probability distribution can be easily\\ncomputed from the number of times each word was assigned to\\nthe given topic. the beauty of topic modeling is that the “top\\nwords” (words with highest probabilities in a topic) usually give a\\nmeaningful interpretation of a topic. the distributions are the\\nunderlying representation of the topics. the top words are usually\\nused to show what topics are contained in the corpus.\\nby counting the number of words assigned to each topic, a topic\\nproportion score can be computed for each document on each\\ntopic. the topic proportion scores then become a document feature that can be used in further analysis. however, the proportion\\nscores are based on the statistical topic assignment of words.\\nwhen documents are very short, such as most utterances in our\\nchat data, the topic proportion scores won’t be reliable. cai et al.\\n[4] argued that alternative ways to compute document topic scores\\nare possible.\\n\\n4.1 tasa topic model\\n\\n0.2\\n0.0\\n-0.2\\n\\nfor students with pretest scores less than 50% correct (n=624),\\nthe average learning gain is 0.88, 69.7% had positive learning\\ngains, 15.7% had the same scores and 14.6% had negative learning gains.\\n\\n.00 .10 .20 .30 .40 .50 .60 .70 .80\\n\\n-0.4\\n-0.6\\nfigure 1. average learning gain as a function of pretest score.\\n\\nalthough our chat data set contained 15,670 utterances, the utterances were short and the corpus is not large enough to build a\\nreliable topic model. to get a reliable model, we used a well\\nknown corpus provided by tasa (touchstone applied science\\nassociates). this corpus contained documents on seven known\\ncategories, including business, health, home economics, industrial\\narts, language arts, science and social studies. our content topic,\\npersonality disorders, is obviously in the health category. of\\ncourse, not all topics in tasa are relevant to our study. therefore, after building up the model, we need to select relevant topics. we will cover that in the next sub-section.106\\n\\n\\x0cthere are a total of 37,651 documents in tasa corpus, each of\\nwhich is about 250 words long. before we ran lda, we filtered\\nout very high frequency words and very low frequency words.\\nhigh frequency words, such as “the”, “of”, “in”, etc., won’t contain much topic information. rare words won’t contribute to\\nmeaningful statistics. 28,483 words (it might be better to say\\n“terms”) were left after filtering. a model with 300 topics was\\nconstructed by lda.\\n\\n4.2 topic score computation and topic selection\\nfrom the tasa topic model, we computed the word-topic probabilities based on the number of times a word was assigned to each\\nof the 300 topics. thus, each word is represented by a 300 dimensional probability distribution vector. for each chat in our chat\\ncorpus, we simply summed up the word probability vectors for the\\nwords appeared in each chat. that gave us 300 topic scores for\\neach chat. recall that, the chats were associated with a reading\\nmaterial and two quizzes. while the students were free to talk\\nabout anything, the content of the reading material and the quizzes\\nset up the main chat topics, that is, personality disorders.\\n\\ntopic score\\n1400\\n1200\\n1000\\n800\\n600\\n\\n200\\n0\\n0\\n\\n20\\n\\n40\\n\\n60\\n\\nfigure 2. sorted topic scores for topic selection.\\nthe first thing we needed to do then was to investigate whether or\\nnot the “hot” topics from the computation made sense. to find\\nthat out, we computed the sum of all topic scores over all chats.\\nthe topics were sorted according the total topic score. the hottest\\ntopic had a total score higher than 1300, much higher than the\\nsecond highest (less than 900). by examining the top words, this\\ntopic is about “illness”, which is highly relevant to personality\\ndisorders. six hot topics scored in the range from 600 to 900.\\nthey are about “outdoors”, “biology”, “people/social”, “education” and “healthcare”. the top words are listed below.\\n\\n\\uf0b7\\n\\uf0b7\\n\\uf0b7\\n\\n\\uf0b7\\n\\n\\uf0b7\\n\\n“illness”, “biology”, “psychology” and “healthcare” are the topics\\nthe learning materials involved. “education” topic is about the\\neducation environment where the chat happened. “outdoor” and\\n“people/social” are off-task topics.\\nto get an idea about whether or not the topic scores were related\\nto the learning gain, we aggregated the scores by person and computed the correlation between the total topic score and the learning\\ngain for each topic. we were only interested in looking at the\\nstudents with larger potential to learn, so we removed the data\\nwith pretest score greater than or equal to 0.5, leaving 624 students out of 844. the results (table 1) showed that all topics were\\nsignificantly correlated to learning gain. it doesn’t seem to be\\ngreat, because that seems to suggest that, whatever topic a student\\ntalked about, more a student talked, larger gain the student obtained. the real reason is that in the aggregation, all topic scores\\nwere summed up. therefore, all topic scores were influenced by\\nthe chat length. so the correlation in table 2 basically showed the\\nchat length effect.\\ntable 2. correlation between total topic scores and learning\\ngain (n=624, pretest<0.5)\\n\\n400\\n\\n\\uf0b7\\n\\n\\uf0b7\\n\\nperson, animal, mental, response, positive, stress, personality, subject, reaction\\npeople/social: joe, pete, mr, charlie, dad, frank, billy,\\ntony, jerry, 'll, mom, 'd, going, 're, got, boys, looked,\\nasked, paper, go\\neducation: students, teacher, teachers, child, children,\\nstudent, school, education, schools, learning, parents,\\ntests, test, program, teaching, behavior, skills, reading,\\nteam, information\\nhealthcare: patient, doctor, health, hospital, medical,\\ndr, patients, nurse, disease, doctors, team, care, office,\\nnursing, drugs, medicine, services, dental, diseases, help\\n\\nillness: health, disease, patient, body, diseases, medical,\\nstress, mental, physical, heart, doctor, problems, cause,\\nperson, patients, exercise, illness, problem, nurse,\\nhealthy\\noutdoors: dog, energy, plants, earth, car, light, food,\\nheat, words, animals, music, rock, language, children,\\nair, uncle, city, sun, women, plant\\nbiology: cells, cell, genes, chromosomes, traits, color,\\norganisms, sex, egg, species, gene, body, male, female,\\nparents, nucleus, eggs, sperm, organism, sexual\\npsychology: behavior, learning, theory, environment,\\nfeelings, sexual, physical, social, sex, human, research,\\n\\ntopic\\n\\npost-test\\n\\npretest\\n\\ngain\\n\\nillness\\n\\n.183**\\n\\n.116**\\n\\n.132**\\n\\noutdoors\\n\\n.216**\\n\\n.133**\\n\\n.154**\\n\\nbiology\\n\\n.159**\\n\\n.125**\\n\\n.105**\\n\\npsychology\\n\\n.182**\\n\\n.096*\\n\\n.140**\\n\\npeople/social\\n\\n.115**\\n\\n.022\\n\\n.107**\\n\\neducation\\n\\n.175**\\n\\n.118**\\n\\n.121**\\n\\nhealthcare\\n\\n.157**\\n\\n.130**\\n\\n.097*\\n\\nto remove the chat length effect, the simplest way is to divide all\\nscores by the number of words (terms) in each chat. however, in\\nthis study, to be consistent with subsequent analysis, we normalized the topic scores to topic proportion scores by dividing each\\ntopic score for each utterance by the sum of all seven topic scores\\nof the same utterance.\\nthe results (table 3) showed that the topic “people/social” had a\\nsignificant negative correlation to learning gain. others were not\\nsignificant but were in the direction we would expect. “illness”,\\n“biology”, “psychology” and “healthcare” were positively correlated with gain scores, while “outdoors” and “people/social” topics were negatively correlated with gains scores. we observed\\nalmost no correlation for the “education” topic. this seems to\\nindicate that the aggregated topic scores have limited power in\\npredicting learning. therefore, we used ena to examine the connections or association of these topics in the students discourse to107\\n\\n\\x0cdevelop a predictive model of learning gains based on the use of\\nthese topics.\\ntable 3. correlation between normalized topic proportion\\nscores and learning gain (n=624, pretest<0.5)\\ntopic\\n\\npost-test\\n\\npretest\\n\\ngain\\n\\nillness\\n\\n.099*\\n\\n0.077\\n\\n0.067\\n\\noutdoors\\n\\n-0.063\\n\\n-0.043\\n\\n-0.044\\n\\nbiology\\n\\n.085*\\n\\n0.054\\n\\n0.063\\n\\npsychology\\n\\n0.067\\n\\n0.019\\n\\n0.058\\n\\npeople/social\\n\\n-.127**\\n\\n-0.076\\n\\n-.083*\\n\\neducation\\n\\n0.027\\n\\n0.056\\n\\n-0.002\\n\\nhealthcare\\n\\n0.073\\n\\n.096*\\n\\n0.027\\n\\n5. epistemic network analysis\\nena measures the connections between elements in data and\\nrepresents them in dynamic network models. ena creates these\\nnetwork models in a metric space that enables the comparison of\\nnetworks in terms of (a) difference graph that highlights how the\\nweighted connections of one network differ from another; and (b)\\nstatistics that summarize the weighted structure of network connections, enabling comparisons of many networks at once.\\nena was originally developed to model cognitive networks involved in complex thinking. these cognitive networks represent\\nassociations between knowledge, skills, habits of mind of individual learners or groups of learners. in this study, we used ena to\\nconstruct network models. for each individual student, we constructed an ena network using the selected seven topic scores for\\neach utterance the student contributed to the group.\\n\\n5.1 process\\nwhile the process of creating ena models is described in more\\ndetail elsewhere (e.g. [11; 17-19]), we will briefly describe how\\nena models are created based on topic modeling. here we defined network nodes as the seven topics identified from the topic\\nmodel. we defined the connections between nodes, or edges, as\\nthe strength of the co-occurrence of topics within a moving stanza\\nwindow (msw) of size 5 [19]. to model connections between\\ntopics we used the products of the topic scores summed across all\\nchats in the msw. that is, for each topic, the topic scores are\\nsummed across all 5 chats in the msw. then ena computed the\\nproduct of the summed topic loadings for each pair topics to\\nmeasure the strength of their co-occurrence. for example, if the\\nsum of the topics scores across five chats was 0.5 for “illness”, 0.3\\nfor “psychology”, and 0.2 for “healthcare”, these scores would\\nresult in three co-occurrences, “illness-psychology”, “illnesshealthcare”, and “psychology-healthcare”, with scores of 0.15,\\n0.1, and 0.06, respectively.\\nnext ena created adjacency matrices for each student that quantified the co-occurrences of topics within the students’ discourse\\nin the context of their chat group. subsequently, the adjacency\\nmatrices were then treated as vectors in a high dimensional space,\\nwhere each dimension corresponds to co-occurrence of a pair of\\ntopics. the vectors were then normalized to unit vectors. notice\\nthat the normalization removed the effect of chat length embedded\\nin the topic scores. a singular value decomposition (svd) was\\nthen performed for dimensional reduction. ena then projected a\\nvector for each student into a low dimensional space that maximizes the variance explained in the data. finally, the nodes of the\\n\\nnetworks, which in this case correspond to the seven selected\\ntopics generated from tasa corpus, were placed in the low dimensional space. the topic nodes were placed using an optimization algorithm such that the overall distances between centroids\\n(centers of the mass of the networks) and the corresponding projected student locations was minimized. a critical feature of ena\\nis that these node placements are fixed, that is, the nodes of each\\nnetwork are in the same place for all units in the analysis. this\\nfixing of the location of the nodes allows for meaningful comparisons between networks in terms of their connection patterns\\nwhich allow us to interpret the metric space. as a result, ena\\nproduced two coordinated representations: (1) the location of each\\nstudent in a projected metric space, in which all units of analysis\\nincluded in the model were located, and (2) weighted network\\ngraphs for each student, which explained why the student was\\npositioned where it was in the space.\\nena also allows us to compare the mean network graphs and\\nmean position in ena space between different groups of students. in this study, we only considered the students with high\\npotential to learn, i.e., the 624 students with pretest score < 0.5\\n(50% correct). among these students, we compared the networks\\nof low learning gain students (gain<-0.1, 𝑁=194) with the networks of high learning gain students (gain>0.43, 𝑁=105). we\\ncompared these groups using difference network graph, which\\nwas formed by subtracting the edge weights of the mean discourse\\nnetwork for the low gain group students from the mean discourse\\nnetwork from the high gain group. this difference network graph\\nshows us which topic connections are stronger for each group. in\\naddition, we conducted a t-test to test the difference between\\ngroup means.\\n\\n5.2 results\\nfigure 3 shows mean discourse networks for students with low\\ngain scores (left, red), students with high gain scores (right, blue),\\nand a difference network graph (center) that shows how the discourse patterns of each group differs. students with low gains had\\nstronger connections between the “people/social” topic and all\\nother topics except for “illness”. more importantly, the connection that was the strongest for low gain students compared to high\\ngain students was between “people/social” and “outdoors”. students with high gain scores made stronger connections between\\nthe topics of “illness”, “psychology”, “healthcare”, “biology”, and\\n“education”.\\ntable 4. comparison of centroids between low gain and high\\ngain students, 𝒑 = 𝟎. 𝟎𝟒𝟕, 𝒕 = 𝟐. 𝟎𝟎\\nn\\n\\nmean\\n\\nsd\\n\\nhigh gain\\n\\n105\\n\\n0.033\\n\\n0.220\\n\\nlow gain\\n\\n194\\n\\n-0.048\\n\\n0.322\\n\\nfigure 4 shows centroids, or the centers of mass, of individual\\nstudents’ discourse networks and their means with low gain score\\nstudents in red and high gain score students in blue. the differences between these two groups were significant on the x dimensions (see table 4). this means that the differences we saw in\\nfigure 2 and described above are statistically significant. in other\\nwords, the high learning gain students’ discourse was more towards the right side of the ena space and the low learning gain\\nstudents’ discourse was more towards the left side. that indicates\\nthat the discourse of students with high learning gains made more\\nconnections between on-task topics (“illness”, “psychology”,\\n“healthcare”, “biology”, and “education”), while the discourse of108\\n\\n\\x0clow gain students made more connections between off-task topics\\n(“people/social” and “outdoors”).\\n\\n6. discussion\\nena makes it possible to visualize the chat dynamics to help\\nresearchers gain deeper understanding of what is going on in a\\ncollaborative learning environment. differences in what topics\\nstudents connect in discourse can predict learning outcomes. previous use of ena has relied on human coded data or use of regular expressions to classify data. utilizing topic modeling can lead\\nto fully automated ena, making it more accessible to a wider\\ngroup of researchers and allows ena to be used with more and\\nlarger data sets.\\nthe fact that the epistemic network predicts learning validates\\nfurther application of ena. for example, the turn by turn chat\\ndynamics can be plotted as trajectories in the 2-d space, where the\\n\\ntopics are placed. investigating the trajectory patterns and their\\nrelationship to learning or socio-affective components are interesting future research directions.\\nwe used a general topic model in this study. many studies in the\\nliterature used lda for topic modeling on relatively small corpora. this causes two problems. 1) lda topic models built upon\\nsmall corpora are not reliable, because lda requires large number documents with relatively large size for each document. inadequate corpus can result in misleading results. 2) using a topic\\nmodel that is not common would result in arbitrary interpretation.\\nfor example, the representation of “illness” from different corpus\\ncould be very different. therefore, it is hard to compare the claims\\nmade to “illness” across different studies. using a reliable, common topic models will set up a common language for different\\nstudies.\\n\\nfigure 3: mean discourse networks for students with low gain scores (left, red), students with high gain scores (right, blue), and a\\ndifference network graph (center).\\nchat utterances are too short. the statistical inference algorithm\\ncontains a high degree of randomness for short documents. as an\\nextreme example, an utterance with a single word, would result in\\ninferred topic proportion scores with “1” on one topic and “0” on\\nothers. the problem is that, this “1” was assigned to a topic with\\ncertain degree of uncertainty. that is, the topic this “1” was assigned to could be any topic. while aggregated analysis may not\\nbe sensitive to such uncertainty, detailed utterance by utterance\\nanalysis would suffer from it.\\nour method of computing topic scores is based on the topic probability distribution over each word. we treat the topic distribution\\nof each word as a vector. when computing the topic score, the\\nsimple sum of all word vectors gives scores to all topics. as we\\nhave pointed out, the summation algorithm will have a length\\neffect. therefore, when such topic scores are used, removing\\nlength effects through normalization is necessary. in this article,\\nwe did not use weighted sum as suggested in cai et al. [4]. comparing the effect of different weighting is beyond the scope of this\\npaper.\\nfigure 4: discourse network centroids low gain score students\\nred, high gain score students blue.\\ntopic scores for documents are usually inferred from topic models. while for longer documents, the topic scores can be used in\\nmany applications (e.g., text clustering [1]), the inferred topic\\nproportion scores won’t be useful for analyzing chats if we need\\nto treat each utterance as a unit of analysis. it is not useful because\\n\\nwhen a general topic model is used, selecting topics relevant to\\nthe specific analysis becomes important. our approach was to\\nlook at the total scores of utterances and find the “hot” topics by\\nsorting the total topic scores. in our study, we had a quickly decreasing curve that helped us to select topics. we believe this\\nwould be the case for most studies using a model containing far\\nmore topics than the topics contained in the target data.109\\n\\n\\x0calthough our study started with topic modeling to capture the\\n“what” in the chats, the association networks constructed in the\\nepistemic network analysis actually turned the “what” into a\\n“how”: how the topics in the chats associated with each other.\\nthis is conceptually similar to the cohesion features dowell [7]\\nand cade [3] used.\\ntopic modeling emphasizes content words. when a topic model is\\nbuilt, stop words are usually removed. an interesting question is,\\nwhat if we do the opposite: keep stop words and remove content\\nwords? pennebaker (e.g., [13]) laid foundational work in this direction. the liwc tool pennebaker and his colleagues created\\nprovides over a hundred text measures by counting non-content\\nwords. liwc measures could provide different features to epistemic network analysis and reveal different aspects of the chat\\ndynamics.\\n\\n7. acknowledgments\\nthe research on was supported by the national science foundation (drk-12-0918409, drk-12 1418288), the institute of education sciences (r305c120001), army research lab (w911inf12-2-0030), and the office of naval research (n00014-12-c0643; n00014-16-c-3027). any opinions, findings, and conclusions or recommendations expressed in this material are those of\\nthe authors and do not necessarily reflect the views of nsf, ies,\\nor dod. the tutoring research group (trg) is an interdisciplinary research team comprised of researchers from psychology,\\ncomputer science, and other departments at university of memphis (visit http://www.autotutor.org).\\n\\n8. references\\n[1]\\n\\nalghamdi, r. and alfalqi, k. 2015. a survey of topic\\nmodeling in text mining. ijacsa) international journal\\nof advanced computer science and applications. 6, 1\\n(2015), 147–153.\\n\\n[2]\\n\\nblei, d.m., edu, b.b., ng, a.y., edu, a.s., jordan, m.i.\\nand edu, j.b. 2003. latent dirichlet allocation. journal\\nof machine learning research. 3, (2003), 993–1022.\\n\\n[3]\\n\\ncade, w.l., dowell, n.m.m. and pennebaker, j. 2014.\\nmodeling student socioaffective responses to group\\ninteractions in a collaborative online chat environment.\\nproceedings of the 7th international conference on\\neducational data mining (edm). 2, 21 (2014), 399–400.\\n\\n[4]\\n\\n[5]\\n\\ncai, z., li, h., graesser, a.c. and hu, x. 2016. can\\nword probabilities from lda be simply added up to\\nrepresent documents\\u202f? proceedings of the 9th\\ninternational conference on educational data mining.\\n(2016), 577–578.\\nvon davier, a.a. and halpin, p.f. 2013. collaborative\\nproblem-solving and the assessment of cognitive skills:\\npsychometric considerations. ets research report\\nseries. december (2013), 36 p.\\n\\n[6]\\n\\ndillenbourg, p. and traum, d. 2006. sharing solutions:\\npersistence and grounding in multimodal collaborative\\nproblem solving. the journal of the learning sciences.\\n15, 1 (2006), 121–151.\\n\\n[7]\\n\\ndowell, n., cade, w.\\u202f, tausczik, y., pennebaker, j., and\\ngraesser, a. 2014. what works: creating adaptive and\\nintelligent systems for collaborative learning support.\\nspringer international publishing switzerland. (2014),\\n124–133.\\n\\n[8]\\n\\ndowell, n.m.m., skrypnyk, s., joksimović, s., graesser,\\n\\na., dawson, s., gašević, d., hennis, t. a., vries, p. de\\nand kovanović, v. 2015. modeling learners ’ social\\ncentrality and performance through language and\\ndiscourse. educational data mining - edm’15 (2015),\\n250–257.\\n[9]\\n\\nfiore, s.m., rosen, m. a., smith-jentsch, k. a., salas, e.,\\nletsky, m. and warner, n. 2010. toward an\\nunderstanding of macrocognition in teams: predicting\\nprocesses in complex collaborative contexts. human\\nfactors. 52, 2 (2010), 203–224.\\n\\n[10]\\n\\ngraesser, a.c., mcnamara, d.s., louwerse, m.m. and\\ncai, z. 2004. coh-metrix: analysis of text on cohesion\\nand language. behavior research methods, instruments,\\n& computers. 36, 2 (2004), 193–202.\\n\\n[11]\\n\\nli, h., samei, b., olney, a., graesser, a. and shaffer, d.\\n2014. question classification in an epistemic game.\\ninternational conference on intelligent tutoring\\nsystems. (2014).\\n\\n[12]\\n\\npennebaker, j.w., boyd, r.l., jordan, k. and blackburn,\\nk. 2015. the development and psychometric properties\\nof liwc2015. austin, tx: university of texas at austin.\\n(2015).\\n\\n[13]\\n\\npennebaker, j.w., chung, c.k., frazee, j. and lavergne,\\ng.m. 2014. when small words foretell academic\\nsuccess\\u202f: the case of college admissions essays.\\n(2014), 1–10.\\n\\n[14]\\n\\nrosen, y. 2014. assessing collaborative problem\\nsolving through computer agent technologies.\\nencyclopedia of information science and technology. 9,\\nnovember (2014), 94–102.\\n\\n[15]\\n\\nsawyer, r.k. 2014. the new science of learning. the\\ncambridge handbook of the learning sciences. 1–18.\\n\\n[16]\\n\\nscholand, a.j., tausczik, y.r. and pennebaker, j.w.\\n2010. assessing group interaction with social language\\nnetwork analysis. lecture notes in computer science\\n(including subseries lecture notes in artificial\\nintelligence and lecture notes in bioinformatics). 6007\\nlncs, (2010), 248–255.\\n\\n[17]\\n\\nshaffer, d.w. 2006. epistemic frames for epistemic\\ngames. computers and education. 46, 3 (2006), 223–\\n234.\\n\\n[18]\\n\\nshaffer, d.w., hatfield, d., svarovsky, g.n., nash, p.,\\nnulty, a., bagley, e., frank, k., rupp, a.a. and\\nmislevy, r.j. 2009. epistemic network analysis: a\\nprototype for 21st-century assessment of learning.\\ninternational journal of learning and media. 1, 2\\n(2009), 33–53.\\n\\n[19]\\n\\nsiebert-evenstone, a.l., arastoopour, g., collier, w.,\\nswiecki, z., ruis, a.r. and shaffer, d.w. 2016. in\\nsearch of conversational grain size: modeling semantic\\nstructure using moving stanza windows. international\\nconference of the learning sciences. (2016).\\n\\n[20]\\n\\nslavin, r.e. 1995. cooperative learning: theory,\\nresearch and practice (2nd ed.). the nature of learning.\\n(1995), 208.\\n\\n[21]\\n\\ntuulos, v.h. and tirri, h. 2004. combining topic\\nmodels and social networks for chat data mining.\\nproceedings of the 2004 ieee/wic/acm international\\nconference on web intelligence. october (2004), 206–110\\n\\n\\x0c213.\\n[22]\\n[23]\\n\\nwhitepaper, a.r. 2014. what happens when we learn\\ntogether. (2014).\\nyousef, a.m.f., chatti, m.a., schroeder, u., wosnitza,\\nm. and jakobs, h. 2014. a review of the state-of-theart. proceedings of the 6th international conference on\\n\\ncomputer supported education - csedu2014. (2014),\\n9–20.\\n[24]\\n\\nwang z., qiu b., bai, w., chuan, s. and le, y. 2014.\\ncollapsed gibbs sampling for latent dirichlet\\nallocation on spark. jmlr: workshop and conference\\nproceedings. 2004 (2014), 17–28.\",\n",
       " '47\\n\\n\\x0cgrade prediction with temporal course-wise influence\\nzhiyun ren\\n\\ncomputer science\\ngeorge mason university\\n4400 university drive,\\nfairfax, va 22030\\n\\nzren4@gmu.edu\\n\\nxia ning\\n\\ncomputer & information\\nscience\\nindiana university - purdue\\nuniversity indianapolis\\n420 university blvd,\\nindianapolis, in 46202\\n\\nhuzefa rangwala\\n\\ncomputer science\\ngeorge mason university\\n4400 university drive,\\nfairfax, va 22030\\n\\nrangwala@cs.gmu.edu\\n\\nxning@cs.iupui.edu\\n\\nabstract\\nthere is a critical need to develop new educational technology applications that analyze the data collected by universities to ensure that students graduate in a timely fashion\\n(4 to 6 years); and they are well prepared for jobs in their\\nrespective fields of study. in this paper, we present a novel\\napproach for analyzing historical educational records from\\na large, public university to perform next-term grade prediction; i.e., to estimate the grades that a student will get\\nin a course that he/she will enroll in the next term. accurate next-term grade prediction holds the promise for better student degree planning, personalized advising and automated interventions to ensure that students stay on track\\nin their chosen degree program and graduate on time. we\\npresent a factorization-based approach called matrix factorization with temporal course-wise influence that incorporates course-wise influence effects and temporal effects for\\ngrade prediction. in this model, students and courses are\\nrepresented in a latent “knowledge” space. the grade of a\\nstudent on a course is modeled as the similarity of their latent representation in the “knowledge” space. course-wise\\ninfluence is considered as an additional factor in the grade\\nprediction. our experimental results show that the proposed\\nmethod outperforms several baseline approaches and infer\\nmeaningful patterns between pairs of courses within academic programs.\\n\\nkeywords\\nnext-term grade prediction, course-wise influence, temporal\\neffect, latent factor\\n\\n1. introduction\\ndata analytics is at the forefront of innovation in several\\nof today’s popular educational technologies (edtech) [17].\\ncurrently, one of the grand challenges facing higher education is the problem of student retention and graduation [19].\\nthere is a critical need to develop new edtech applications\\n\\nthat analyze the data collected by universities to ensure that\\nstudents graduate in a timely fashion (4 to 6 years), and they\\nare well prepared for jobs in their respective fields of study.\\nto this end, several universities deploy a suite of software\\nand tools. for example, degree planners 1 assist students\\nin deciding their majors or fields of study, choosing the sequence of courses within their chosen major and providing\\nadvice for achieving career and learning objectives. early\\nwarning systems [27] inform advisors/students of progress,\\nand additionally provide cues for intervention when students\\nare at the risk of failing one or more courses and dropping\\nout of their program of study. in this work, we focus on the\\nproblem of next-term grade prediction where the goal is to\\npredict the grade that a student is expected to obtain in a\\ncourse that he/she may enroll in the next term (future).\\nin the past few years, several algorithms have been developed to analyze educational data, including matrix factorization (mf) algorithms inspired from recommender system\\nresearch. mf methods decompose the student-course (or\\nstudent-task) grade matrix into two low-rank matrices, and\\nthen the prediction of the grade for a student on an untaken\\ncourse is calculated as the product of the corresponding vectors in the two decomposed matrices [22, 11]. traditional\\nmf algorithms have shown a strong ability to deal with\\nsparse datasets [14] and their extensions have incorporated\\ntemporal and dynamic information [12]. in our setting, we\\nconsider that a student’s knowledge is continuously being\\nenriched while taking a sequence of courses; and it is important to incorporate this dynamic influence of sequential\\ncourses within our models. therefore, we present a novel\\napproach referred as matrix factorization with temporal\\ncourse-wise influence (mftci) model to predict next term\\nstudent grades. mftci considers that a student’s grade on\\na certain course is determined by two components: (i) the\\nstudent’s competence with respect to each course’s topics,\\ncontent and requirement, etc., and (ii) student’s previous\\nperformance over other courses. we performed a comprehensive set of experiments on various datasets. the experimental results show that the proposed method outperforms\\nseveral state-of-the-art methods. the main contributions of\\nour work in this paper are as follows:\\n1. we model and incorporate temporal course-wise influence in addition to matrix factorization for grade\\n1\\nhttp://www.blackboard.com/mobilelearning/planner.aspx48\\n\\n\\x0cprediction. our experimental results demonstrate significant improvement from course-wise influence.\\n2. our model successfully captures meaningful coursewise influences which correlate to the course content.\\n3. the learned influences between pairs of courses help\\nin understanding pre-requisite structures within programs and tuning academic program chains.\\n\\n2. related work\\nover the past few years, several methods have been developed to model student behavior and academic performance [2, 9], and they gain improvement of learning outcomes [21]. methods influenced by recommender system\\n(rs) research [1], including collaborative filtering (cf) [18]\\nand matrix factorization [13], have attracted increasing attention in educational mining applications which relate to\\nstudent grade prediction [32] and in-class assessment prediction [8]. sweeney et. al. [31, 30] performed an extensive study of several recommender system approaches including svd, svd-knn and factorization machine (fm) to\\npredict next-term grade performance. inspired by contentbased recommendation [20] approaches, polyzou et. al. [23]\\naddressed the future course grade prediction problem with\\nthree approaches: course-specific regression, student-specific\\nregression and course-specific matrix factorization. moreover, neighborhood-based cf approaches [25, 4, 6] predict\\ngrades based on the student similarities, i.e., they first identify similar students and use their grades to estimate the\\ngrades of the students with similar profiles.\\nin order to capture the changing of user dynamics over time\\nin rs, various dynamic models have been developed. many\\nof such models are based on matrix factorization and state\\nspace models. sun et. al. [28, 29] model user preference\\nchange using a state space model on latent user factors, and\\nestimate user factors over time using noncausal kalman filters. similarly, chua et.al. [5] apply linear dynamical systems (lds) on non-negative matrix factorization (nmf)\\nto model user dynamics. ju et. al. [12] encapsulate the\\ntemporal relationships within a non-negative matrix formulation. zhang et. al. [34] learn an explicit transition\\nmatrix over the latent factors for each user, and estimate\\nthe user and item latent factors and the transition matrices within a bayesian framework. other popular methods\\nfor dynamic modeling include time-weighting similarity decaying [7], tensor factorization [33] and point processes [16].\\nthe method proposed in this paper tackle the challenges of\\nnext-term grade prediction which relates to the evolvement\\nof student knowledge over taking a sequence of courses. our\\nkey contribution involves how we incorporate the temporal\\ncourse-wise relationships within a mf approach. additionally, the proposed approach learns pairwise relationships between courses that can help in understanding pre-requisite\\nstructures within programs and tuning academic program\\nchains.\\n\\n3. preliminaries\\n3.1 problem statement and notations\\nformally, student-course grades will be represented by a series of matrices {g1 , g2 , ..., gt } for t terms. each row\\nof gt represents a student, each column of gt represents a\\n\\nt\\ncourse, and each value in gt , denoted as gs,c\\n, represents a\\nt\\ngrade that student s got on course c in term t (gs,c\\n∈ (0, 4],\\nt\\ngs,c = 0 indicates that student s did not take the course c in\\nterm t. we add a small value to failing grade to distinguish\\n0 score from such situation.). student-course\\np grades up to\\nthe tth term will be represented by gt = ti=1 gi with size\\nof n × m, where n is the number of students and m is the\\nnumber of courses. given the database of (student, course,\\ngrade) up to term (t − 1) (i.e., gt −1 ), the next-term grade\\nprediction problem is to predict grades for each student on\\ncourses they might enroll in the next term t . to simplify\\nthe notations, if not specifically stated in this paper, we will\\nt\\nuse gs,c to denote gs,c\\n. our testing set is then (student,\\ncourse, grade) triples in the tth term, represented by matrix\\ngt . rows from the grade matrices representing a student s\\nwill simply be represented as g(s, :) and the specific courses\\nthat student has a grade for in this row can be given by\\nc0 ∈ g(s, :).\\n\\nin this paper, all vectors (e.g., uts and vc ) are represented\\nby bold lower-case letters and all matrices (e.g., a) are represented by upper-case letters. column vectors are represented by having the transpose supscriptt , otherwise by default they are row vectors. a predicted/approximated value\\nis denoted by having a ˜ head.\\n\\n4. methods\\n4.1 mf with temporal course-wise influence\\nwe consider the student s’ grade on a certain course c, denoted as gs,c , as determined by two factors. the first factor\\nis the student s’ competence with respect to the course c’s\\ntopics, content and requirement. this is modeled through\\na latent factor model, in which s’ competence is captured\\nusing a size-k latent factor us , c’s topics and contents are\\ncaptured using a size-k latent factor vc in the same latent\\nspace as us . then the competence of s over c is modeled\\nby the “similarity” between us and vc via their dot product\\n(i.e., uts vc ).\\nthe second factor is the previous performance of student s\\nover other courses. we hypothesize that if course c0 has a\\npositive influence on course c, and student s achieved a high\\ngrade on c0 , then s tends to have a high grade on c. under\\nthis hypothesis, we model this second factor as a product\\nbetween the performance of student on a previous “related”\\ncourse where the pairwise course relationships are learned\\nin our formulation. note that we consider this pairwise\\ncourse influence as time independent, i.e., the influence of\\none course over another does not change over time. however, the impact from previous performance/grades can be\\nmodeled using a decay function over time. taking these two\\nfactors, the estimated grade is given as follows:\\ng̃s,c = uts vc\\np\\n+ e−α\\n|\\n\\n+ e−2α\\n|c0 ∈gt −1 (s,:)\\n\\np\\n\\na(c0 , c)gs,c0\\n\\n|gt −1 (s, :)|\\n{z\\n∆(t −1)\\n\\nc00 ∈gt −2 (s,:)\\n\\n00\\n\\n(1)\\n\\na(c , c)gs,c00\\n\\n|gt −2 (s, :)|\\n{z\\n∆(t −2)\\n\\n}\\n\\n,\\n\\n}\\n\\n49\\n\\n\\x0cin which a(c0 , c) is the influence of c0 on c, gt −1 (s, :)/gt −2 (s, :\\n) is the subset of courses out of all courses that s has taken in\\nthe first/second previous terms, |gt −1 (s, :)|/|gt −2 (s, :)| is\\nthe number of such taken courses. e−α /e−2α denote the\\ntime-decay factors. in equation 1, we consider previous\\ntwo terms. more previous terms can be included with even\\nstronger time-decay factors. given the grade estimation as\\nin equation 1, we formulate the grade prediction problem\\nfor term t as the following optimization problem,\\nmin\\n\\nu,v,a\\n\\nγ\\n1x\\n(gs,c − g̃s,c )2 + (ku k2f + kv k2f )\\n2 s,c\\n2\\n+ τ kak∗ + λkak`1\\n\\n×\\n\\ne−α\\ng\\n|gt −1 (s,:)| s,ci\\ne−2α\\ng\\n|g\\n(s,:)| s,ci\\nt −2\\n\\n(if ci is taken in term t − 1)\\n\\n(if ci is taken in term t − 2)]\\n\\nmin τ kz1 k∗ +\\nz1\\n\\nρ\\nka − z1 k2f + ρ(tr(u1t (a − z1 )))\\n2\\n\\nwe apply the admm [3] technique for equation 2 by reformulating the optimization problem as follows,\\nγ\\n1x\\n(gs,c − g̃s,c )2 + (ku k2f + kv k2f )\\n2 s,c\\n2\\n\\n+τ kz1 k∗ + λkz2 k`1\\nρ\\n+ (ka − z1 k2f + ka − z2 k2f )\\n2\\n+ρ(tr(u1t (a − z1 )))\\n\\n(4)\\n\\nthe closed-form solution of this problem is\\nz1 = s τρ (a + u1 )\\n\\n(5)\\n\\nwhere sα (x) is a soft-thresholding function that shrinks the\\nsingular values of x with a threshold α, that is,\\nsα (x) = u diag((σ − α)+ )v t\\n\\n4.1.1 optimization algorithm of mftci\\n\\ns.t.,\\n\\ns,cj\\n\\n(\\n\\nstep 3: update z1 and z2 . for z1 , the problem becomes\\n\\nwhere u and v are the latent non-negative student factors\\nand course factors, respectively; kak∗ is the nuclear norm\\nof a, which will induce an a of low rank; and kak`1 is the\\n`1 norm of a, which will introduce sparsity in a. in addition, the non-negativity constraint on a is to enforce only\\npositive influence across courses.\\n\\nu,v,a,u1 ,u2 ,z1 ,z2\\n\\na(ci , cj ) = a(ci , cj ) − lr × [ρ(a(ci , cj ) − z1 (ci , cj ))\\n+ ρ(a(ci , cj ) − z2 (ci , cj )) + ρu1 (ci , cj ) + ρu2 (ci , cj )\\nx\\n−\\n(gs,cj − g̃s,cj )\\n\\n(3)\\nwith projection into [0, +∞), where lr is a learning rate.\\n\\ns.t., a ≥ 0\\n\\nmin\\n\\nusing the gradient descent, the elements in a can be updated as follows.\\n\\nwhere x = u σv\\nand\\n\\nt\\n\\n(6)\\n\\nis the singular value decomposition of x,\\n(x)+ = max(x, 0).\\n\\nfor z2 , the problem becomes\\nρ\\nmin λkz2 k`1 + ka − z2 k2f + ρ(tr(u2t )(a − z2 ))\\nz2\\n2\\n\\n(7)\\n\\n(8)\\n\\nthe closed-form solution is\\nz2 = e λ (a + u2 )\\nρ\\n\\n+ρ(tr(u2t (a − z2 )))\\n\\n(9)\\n\\nwhere eα (x) is a soft-thresholding function that shrinks the\\nvalues in x with a threshold α, that is,\\n\\na≥0\\n\\neα (x) = (x − α, 0)+\\n\\n(10)\\n\\nwhere z1 and z2 are two auxiliary variables, and u1 and u2\\nare two dual variables. all the variables are solved via an\\nalternating approach as follows.\\n\\nwhere ()+ is defined as in equation 7.\\n\\nstep 1: update u and v . fixing all the other variables and\\n\\nstep 4: update u1 and u2 . u1 and u2 are updated based\\non standard admm updates:\\n\\nsolving for u and v , the problem becomes a classical matrix\\nfactorization problem:\\nmin\\nu,v\\n\\nx\\n1x\\nγ x\\n(fs,c − uts vc )2 + (\\nkus k22 +\\nkvc k22 )\\n2 s,c\\n2 s\\nc\\n\\n(2)\\n\\nwhere fs,c = gs,c − ∆(t − 1) − ∆(t − 2) (see eq 1). the\\nmatrix factorization problem can be solved using alternating\\nminimization.\\n\\nstep 2: update a. fixing all the other variables and solving for a, the problem becomes\\nmin\\na\\n\\ns.t.,\\n\\nρ\\n1x\\n(gs,c − g̃s,c )2 + (ka − z1 k2f + ka − z2 k2f )\\n2 s,c\\n2\\n\\n+ρ(tr(u1t (a − z1 ))) + ρ(tr(u2t (a − z2 )))\\na≥0\\n\\nu1 = u1 + (a − z1 );\\n\\nu2 = u2 + (a − z2 )\\n\\n(11)\\n\\nin addition, we conduct computational complexity analysis\\nof mftci and put it in appendix.\\n\\n5. experiments\\n5.1 dataset description\\nwe evaluated our method on student grade records obtained\\nfrom george mason university (gmu) from fall 2009 to\\nspring 2016. this period included data for 23,013 transfer\\nstudents and 20,086 first-time freshmen (non-transfer i.e.,\\nstudents who begin their study at gmu) across 151 majors\\nenrolled in 4,654 courses.\\nspecifically, we extracted data for six large and diverse majors for both non-transfer and transfer students. these majors include: (i) applied information technology (ait), (ii)50\\n\\n\\x0ctable 1: dataset descriptions\\nmajor\\nait\\nbiol\\nceie\\ncpe\\ncs\\npsyc\\ntotal\\n\\nnon-transfer students\\n#s\\n#c\\n#(s,c)\\n239\\n453\\n5,739\\n1,448\\n990\\n33,527\\n393\\n642\\n9,812\\n340\\n649\\n7,710\\n908\\n818\\n18,376\\n911\\n874\\n22,598\\n4,239 1,115 97,762\\n\\ntransfer students\\n#s\\n#c\\n#(s,c)\\n982\\n465\\n14,396\\n1,330\\n833\\n22,691\\n227\\n305\\n4,538\\n91\\n219\\n1,614\\n480\\n464\\n7,967\\n1504\\n788\\n24,661\\n4,614 1,019 75,867\\n\\n#s, #c and #s-c are number of students, courses and student-course\\npairs in educational records across the 6 majors from fall 2009 to\\nspring 2016, respectively.\\n\\nfall 2009 to fall 2015\\n\\nfall 2009 to spring 2015\\n\\nfall 2009 to fall 2014\\n\\nspring 2015\\n\\nspring 2016\\n\\nfall 2015\\ntraining set:\\ntest set:\\n\\nfigure 1: different experimental protocols\\n\\nbiology (biol), (iii) civil, environmental and infrastructure engineering (ceie), (iv) computer engineering (cpe)\\n(v) computer science (cs) and (vi) psychology (psyc).\\ntable 1 provides more information about these datasets.\\n\\n5.2\\n\\nexperimental protocol\\n\\nto assess the performance of our next-term grade prediction\\nmodels, we trained our models on data up to term t − 1\\nand make predictions for term t . we evaluate our method\\nfor three test terms, i.e., spring 2016, fall 2015 and spring\\n2015. as an example, for evaluating predictions for term\\nfall 2015, data from fall 2009 to spring 2015 is considered\\nas training data and data from fall 2015 is testing data.\\ndatasets. figure 1 shows the three different train-test splits.\\n\\ndefine a tick to denote the difference between two consecutive letter grades (e.g., c+ vs c or c vs c-). to assess the\\nperformance of our grade prediction method, we convert the\\npredicted grades into their closest letter grades and compute the percentage of predicted grades with no error (or\\n0-ticks), within 1-tick and within 2-ticks denoted by pct0 ,\\npct1 and pct2 , respectively. for the problem of course selection and degree planning, courses predicted within 2 ticks\\ncan be considered sufficiently correct. we name these metrics as percentage of tick accuracy (pta).\\n\\n5.4\\n\\nbaseline methods\\n\\nwe compare the performance of our proposed method to the\\nfollowing baseline approaches.\\n\\n5.4.1\\n\\nmatrix factorization\\n\\nmatrix factorization is known to be successful in predicting ratings accurately in recommender systems [26]. this\\napproach can be applied directly on next-term grade prediction problem by considering student-course grade matrix as\\na user-item rating matrix in recommender systems. based\\non the assumption that each course and student can be represented in the same low-dimensional space, corresponding\\nto the knowledge space, two low-rank matrices containing\\nlatent factors are learned to represent courses and students\\n[30]. specifically, the grade a student s will achieve on a\\ncourse c is predicted as follows:\\ng̃s,c = µ + ps + qc + uts vc\\n\\n(12)\\nn\\n\\nwhere µ is a global bias term, ps (p ∈ r ) and qc (q ∈\\nrm ) are the student and course bias terms (in this case, for\\nstudent s and course c), respectively, and us (u ∈ rk×n )\\nand vc (v ∈ rk×m ) are the latent factors for student s and\\ncourse c, respectively.\\n\\n5.4.2\\n\\nmatrix factorization without bias (mf0 )\\n\\nwe only considered the student and course latent factors to\\npredict the next-term grades. therefore, the grade a student\\ns will achieve on a course c is calculated as follows:\\ng̃s,c = uts vc\\n\\n5.3\\n\\nevaluation metrics\\n\\nwe use root mean squared error (rmse) and mean\\nabsolute error (mae) as metrics for evaluation, and are\\ndefined as follows:\\nsp\\n2\\ns,c∈gt (gs,c − g̃s,c )\\n,\\nrm se =\\n|gt |\\np\\ns,c∈gt |gs,c − g̃s,c |\\nm ae =\\n|gt |\\n\\nwhere gs,c and g̃s,c are the ground truth and predicted grade\\nfor student s on course c, and gt is the testing set of (student, course, grade) triples in the tth term. normally, in\\nnext-term grade prediction problem, mae is more intuitive\\nthan rmse since mae is a straightforward method which\\ncalculates the deviation of errors directly while rmse has\\nimplications such as penalizing large errors more.\\nfor our dataset, a student’s grade can be a letter grade (i.e.\\na, a-, . . . , f). as done previously by polyzou et. al. [24] we\\n\\n5.4.3\\n\\n(13)\\n\\nnon-negative matrix factorization (nmf) [15]\\n\\nwe add non-negative constraints on matrix u and matrix v\\nin equation 13. the non-negativity constraints allows mf\\napproaches to have better interpretability and accuracy for\\nnon-negative data [10].\\n\\n6. results and discussion\\n6.1 overall performance\\ntable 2 presents the comparison of pct0 , pct1 and pct2 for\\nnon-transfer students for the three terms considered as test:\\nspring 2016, fall 2015 and spring 2015. we observe that the\\nmftci model outperforms the baselines across the different\\ntest sets. on average, mftci outperforms the mf, mf0\\nand nmf methods by 34.18%, 11.59% and 4.08% in terms of\\npct0 , 16.64%, 7.96% and 4.03% in terms of pct1 , and 2.10%,\\n3.00% and 1.98% in terms of pct2 , respectively. we observe\\nsimilar results for transfer students as well (not included\\nhere for brevity).51\\n\\n\\x0ctable 2: comparison performance with pta (%)\\nmethods\\nmf\\nmf0\\nnmf\\nmftci\\n\\nspring 2016\\nfall 2015\\nspring 2015\\npct0 (↑) pct1 (↑) pct2 (↑) pct0 pct1 pct2 pct0 pct1 pct2\\n13.25\\n27.71\\n58.02 12.05 26.63 58.89 13.03 26.09 54.83\\n16.52\\n31.65\\n57.46 15.51 30.03 55.64 15.53 29.53 54.94\\n13.21\\n27.04\\n57.18 15.33 30.12 56.15 15.56 29.23 54.93\\n19.78 35.52 61.44 19.71 35.16 60.12 18.56 32.78 58.80\\n\\ni) “↑” indicates the higher the better. ii) reported values of pct0 , pct1 and pct2 are percentages. iii) best performing methods are highlighted with bold.\\n\\n0.70\\n\\ntable 3 presents the performance of the baselines and mftci\\nmodel for the three different terms of both non-transfer and\\ntransfer students using rmse and mae as evaluation metrics. the mftci model consistently outperforms the baselines across the different datasets in terms of mae. in addition, the results shows that mf0 , nmf and mftci tend\\nto have better performance for spring 2016 term than fall\\n2015 term. similar trend is observed between fall 2015 term\\nand spring 2015 term. this suggests that mftci is likely\\nto have better performance with more information in the\\ntraining set.\\n\\n6.2\\n\\nanalysis on individual majors\\n\\nwe divide non-transfer students based on their majors and\\ntest the baselines and mftci model on each major, separately. table 4 shows the comparison of pct0 , pct1 and\\npct2 on different majors. the results show that mftci has\\nthe best performance for almost all the majors. among all\\nthe results, mftci has the highest accuracy when predicting grades for psyc and biol students for which we have\\nmore student-course pairs in the training set.\\n\\n6.3\\n\\neffects from previous terms on mftci\\n\\nin order to see the influence of number of previous terms\\nconsidered in mftci, we run our model with only ∆(t − 1)\\nin equation 1. this method is represented as mftcip1 .\\nfigure 2 shows the comparison results of mae for six subsets of data which are reported in table 3, where “ntr”\\nstands for non-transfer students and “tr” stands for transfer students. the results show that mftci consistently\\noutperforms mftcip1 on all datasets. this suggests that\\nconsidering two previous terms is necessary for achieving\\ngood prediciton results. moreover, since we consider that\\nthe student’s knowledge is modeled using an exponential\\ndecaying function over time, we do not include the influence\\nfrom the third previous term in our model as its influence\\nfor the grade prediction is negligible in comparison to the\\nprevious two terms.\\n\\n6.4\\n\\nvisualization of course influence\\n\\nto interpret what is captured in the course influence matrix\\na (see eq 1), we extract the top 20 values with the corresponding course names (and topics) for analysis. figure 3\\nand 4 show the captured pairwise course influences for cs\\nand ait majors, respectively. each node corresponds to\\none course which is represented by the shortened course’s\\nname. we can notice from the figures that most influences\\nreflect content dependency between courses. for example,\\nin the cs major, “object oriented programming” course\\nhas significant influence on performance of “low-level pro-\\n\\nmftcip1\\nmftci\\n\\nmae\\n\\n0.68\\n0.66\\n0.64\\n0.62\\n0.60\\n\\nfigure 2:\\nmftci\\n\\nntr spring ntr fall ntr spring tr spring tr fall tr spring\\n2016\\n2015\\n2015\\n2016\\n2015\\n2015\\n\\ncomparison performance for mftcip1 and\\n\\ngramming” course (the former one is also the latter one’s\\nprerequisite course); “linear algebra” and “discrete mathematics” have influence on each other; “formal methods &\\nmodels” course has influence on “analysis of algorithms”\\ncourse. in case of the ait major, both “introductory it”\\ncourse and “introductory computing” course have influence\\non “it problem & programming” course; “multimedia &\\nweb design” course has influence on both “applied it programming” course and “it in the global economy” course.\\ngmu has a sample schedule of eight-term courses for each\\nmajor in order to guide undergraduate students to finish\\ntheir study step by step based on the level, content and\\ndifficulty of courses 2 . among the identified relationships\\nshown in figures 3 and 4 we found 17 and 13 of the cs and\\nait courses influences in the guide map, respectively. the\\nrest of the identified influences are among other general electives but required courses (e.g., “public speaking” course),\\nor specific electives pertaining to the major (e.g., “research\\nmethods” course). this shows that our model learns meaningful course-wise influences and successfully uses it to improve mf model.\\nfigure 5 shows the identified course influences for the biol,\\nceie, cpe and psyc majors. these identified course-wise\\ninfluences seem to capture similarity of course content.\\n\\n7.\\n\\nconclusion and future work\\n\\nwe presented a matrix factorization with temporal coursewise influence (mftci) model that integrates factorization\\nmodels and the influence of courses taken in the preceding\\nterms to predict student grades for the next term.\\nwe evaluate our model on the student educational records\\nfrom fall 2009 to spring 2016 collected from george ma2\\n\\nhttp://catalog.gmu.edu52\\n\\n\\x0ctable 3: comparison performance with rmse and mae.\\nmethods\\nmf\\nmf0\\nnmf\\nmftci\\n\\nspring 2016\\nrmse mae\\n0.999 0.754\\n0.929 0.714\\n1.020 0.769\\n0.928 0.685\\n\\nnon-transfer students\\nfall 2015\\nspring\\nrmse mae\\nrmse\\n1.037 0.786\\n1.023\\n0.977 0.752\\n1.014\\n0.967 0.746\\n1.000\\n0.982 0.717\\n1.012\\n\\n2015\\nmae\\n0.784\\n0.778\\n0.771\\n0.750\\n\\nspring\\nrmse\\n0.925\\n0.893\\n0.906\\n0.887\\n\\n2016\\nmae\\n0.688\\n0.668\\n0.683\\n0.636\\n\\ntransfer students\\nfall 2015\\nrmse mae\\n0.921 0.686\\n0.944 0.705\\n0.932 0.701\\n0.927 0.662\\n\\nresearch methods\\n\\nwestern history\\n\\n0.563\\nobject oriented programming\\n\\ncomputer ethics\\n\\n0.691\\n0.3661\\n\\n0.4953\\n\\ndata structures\\n0.4313\\ndigital electronics\\n\\n0.4314\\n\\nintroductory programming\\n0.4264\\n\\nformal methods & models\\n\\nanalytic geometry & calculus\\n\\n0.3526\\n0.3646\\n\\nreading & writng\\n\\n0.4199\\npublic speaking\\n\\n0.3797\\n\\n0.3691\\n\\n0.536\\n\\n0.3852\\n\\nlow-level programming\\n\\n0.4392\\n\\nspring 2015\\nrmse mae\\n0.985 0.732\\n1.011 0.765\\n0.979 0.746\\n1.000 0.721\\n\\n0.3512\\n\\nlinear algebra\\n\\n0.6033\\nadvanced composition\\n\\n0.4122 0.4929\\n\\ndiscrete mathematics\\n\\n0.3512\\nanalysis of algorithms\\n\\nfigure 3: identified course influences for cs major\\n\\ntable 4: comparison performance for different majors\\nmethods\\nmf\\nmf0\\npct0\\nnmf\\nmftci\\nmf\\nmf0\\npct1\\nnmf\\nmftci\\nmf\\nmf0\\npct2\\nnmf\\nmftci\\n\\nait\\n18.71\\n19.45\\n19.77\\n22.30\\n37.95\\n37.21\\n36.79\\n39.64\\n67.02\\n66.17\\n66.70\\n66.70\\n\\nbiol ceie cpe\\ncs psyc\\n18.00 15.99 12.99 15.98 20.18\\n22.10 16.70 14.21 16.47 22.12\\n22.16 17.01 14.32 16.61 22.17\\n24.24 16.80 14.32 17.32 25.83\\n35.43 31.47 27.86 31.53 39.41\\n39.68 31.87 27.97 30.51 39.63\\n39.74 31.67 27.19 30.43 39.36\\n40.87 32.38 27.53 31.78 42.29\\n67.78 58.66 52.28 56.91 71.01\\n67.54 58.35 50.72 56.24 67.74\\n67.54 58.55 51.17 56.17 67.79\\n68.25 58.76 52.94 58.18 68.29\\n\\nson university. the dataset in this study contains both\\nnon-transfer and transfer students from six different majors. our experimental evaluation shows that mftci consistently outperforms the different state-of-the-art methods.\\nmoreover, we analyze the effects from previous terms on\\nmftci, and we make the conclusion that it is necessary\\nto consider two previous terms. in addition, we visualize\\nthe patterns learned between pairs of courses. the results\\nstrongly demonstrate that the learned course influences correlate with the course content within academic programs.\\nin the future, we will explore incorporation of additional constraints over the the pairwise course influence matrix, such\\nas prerequisite information, compulsory and elective provision of a course. we will explore using the course influence\\n\\ninformation to build a degree planner for future students.\\n\\n8.\\n\\nacknowledgments\\n\\nfunding was provided by nsf grant, 1447489.\\n\\nappendix\\na. computational complexity analysis\\nthe computational complexity of mftci is determined by\\nthe four steps in the alternating approach as described above.\\nto update u and v as in equation 2 using gradient descent method via alternating minimization, the computational complexity is o(niteruv (k × ns,c + k × m + k × n)) =\\no(niteruv (k×ns,c )) (typically ns,c ≥ max(m, n)), where ns,c\\nis the total number of student-course dyads, n is the number of students, m is the number of courses, k is the latent\\ndimensions of u and v , and niteruv is the number of iterations. to update a as in equation 3 using gradient descent\\nmethod, the computational complexity is upper-bounded by\\nns,c\\n)), where ncc is the number of course pairs\\no(nitera (ncc × m\\nns,c\\nthat have been taken by at least one student, m\\nis the average number of students for a course, which upper bounds\\nthe average number of students who co-take two courses,\\nand nitera is the number of iteractions. essentially, to update a, we only need to update a(ci , cj ) where ci and cj\\nhave been co-taken by some students. for a(ci , cj ) where\\nci and cj have never been taken together, they will remain\\n0. to update z1 as in equation 4, a singular value decomposition is involved and thus its computational complexity\\nis upper bounded by o(m3 ). to update z2 as in equation 8, the computational complexity is o(m2 ). to update53\\n\\n\\x0ccalculus & applications\\n\\ncomposition\\n\\n0.2262\\n\\n0.2753\\n\\nintroductory computing\\n\\n0.2523\\n\\ncomputer hardware\\n\\npublic speaking\\n\\n0.31\\n\\n0.2317 0.2624\\n\\nmultimedia & web design\\n0.2602\\n\\n0.3392\\n\\nwestern history\\n\\n0.2248\\n\\ndiscrete mathematics for it\\n\\n0.2453\\napplied it\\n\\n0.2461\\n\\n0.276\\n\\nit in the global economy\\n\\n0.2675 0.3012\\n\\ninformation security\\n\\n0.3033\\n\\nit problem & programming\\n\\nintroductory statistics\\n\\n0.226\\n\\ndatabase fundamentals\\n\\n0.2456\\n\\napplied it programming\\n\\n0.2393\\n\\nintroductory it\\n\\nit problem & object oriented techniques\\n\\n0.2217\\nadvanced composition\\n\\nfigure 4: identified course influences for ait major\\n\\nintroductory engineering\\ngeneral chemistry i\\n\\ncell structure & function\\n\\nbiostatistics\\n\\n0.6901\\n0.6581\\n\\n1.5541\\n\\n0.7832\\n\\n1.1224\\n\\n1.1074\\n0.5888\\n\\norganic chemistry i\\n\\ngeneral chemistry ii\\n\\ncomputer graphics\\n\\n0.6835\\n\\n0.6277 0.7009\\n\\nchemistry for engineers\\n\\n0.8673\\n\\n0.6068\\n\\norganic chemistry lab i\\n\\norganic chemistry lab ii\\n\\nphysics lab i\\n\\nphysics i\\n\\n0.9025\\n\\n0.6707\\n\\n0.5658\\n\\n0.6046\\n\\nphysics ii\\n\\nmicroeconomic\\n\\ncalculus ii\\n\\n0.8467\\n\\n0.7125\\n\\n0.5907\\n\\ncalculus i\\n\\n0.5345\\nphysics i\\n\\nbiology of microorganisms\\n\\n(a) identified course influences for biol major\\n\\n(b) identified course influences for ceie major\\n\\nstatistics in psychology\\n\\ncomposition i\\n\\nsocial psychology\\n\\n0.3382 0.4064\\nintroductory engineering\\n0.0556\\ncalculus iii\\n\\n0.0478\\n0.0682\\n\\nuniversity physics i\\n0.0436\\n\\nphysics lab i\\n\\nlinear algebra\\n\\ncalculus i\\n0.0509\\n\\n0.0675\\n\\ncalculus ii\\n0.0548\\n\\n0.0495\\n\\nuniversity physics ii\\n\\n0.0611\\nintroductory programming\\n\\n0.6269\\n\\ncognitive psychology\\n\\n0.4069\\n\\n0.4037\\nresearch in psychology\\n\\n0.4545\\nabnormal psychology\\n\\n0.389\\ncomposition ii\\n\\n0.385\\n0.4304\\n\\n0.4898\\n\\n0.0397\\nphysics lab ii\\n\\n(c) identified course influences for cpe major\\n\\nphysiological psychology\\n\\n(d) identified course influences for psyc major\\n\\nfigure 5: identified course influences for different majors\\n\\nu1 and u2 as in equation 11, the computational complexity\\nis o(m2 ). thus, the computational complexity for mtfci\\nns,c\\n) + m3 + m2 ))\\nis o(niter(niteruv (k × ns,c ) + nitera (ncc × m\\nns,c\\n= o(niter(niteruv (k×ns,c )+nitera (ncc × m )+m3 )), where\\nniter is the number of iterations for the four steps. although the complexity is dominated by m3 due to the svd\\non a + u1 , since n (i.e., the number of courses) is typically\\nnot large, the run time will be more dominated by ns,c (i.e.,\\nthe number of student-course dyads).\\n\\nb.\\n\\nreferences\\n\\n[1] charu c. aggarwal. recommender systems: the\\ntextbook. springer publishing company, incorporated,\\n\\n1st edition, 2016.\\n[2] rsjd baker et al. data mining for education.\\ninternational encyclopedia of education, 7:112–118,\\n2010.\\n[3] stephen boyd, neal parikh, eric chu, borja peleato,\\nand jonathan eckstein. distributed optimization and\\nstatistical learning via the alternating direction\\nr in\\nmethod of multipliers. foundations and trends\\nmachine learning, 3(1):1–122, 2011.\\n[4] hana bydžovská. are collaborative filtering methods\\nsuitable for student performance prediction? in\\nportuguese conference on artificial intelligence, pages\\n425–430. springer, 2015.54\\n\\n\\x0c[5] freddy chong tat chua, richard j oentaryo, and\\nee-peng lim. modeling temporal adoptions using\\ndynamic matrix factorization. in 2013 ieee 13th\\ninternational conference on data mining, pages\\n91–100. ieee, 2013.\\n[6] tristan denley. course recommendation system and\\nmethod, january 10 2013. us patent app. 13/441,063.\\n[7] yi ding and xue li. time weight collaborative\\nfiltering. in proceedings of the 14th acm international\\nconference on information and knowledge\\nmanagement, cikm ’05, pages 485–492, new york,\\nny, usa, 2005. acm.\\n[8] asmaa elbadrawy, scott studham, and george\\nkarypis. personalized multi-regression models for\\npredicting students performance in course activities.\\numn cs, pages 14–011, 2014.\\n[9] wu he. examining studentsâăź online interaction in\\na live video streaming environment using data mining\\nand text mining. computers in human behavior,\\n29(1):90–102, 2013.\\n[10] ngoc-diep ho. nonnegative matrix factorization\\nalgorithms and applications. phd thesis, école\\npolytechnique, 2008.\\n[11] chein-shung hwang and yi-ching su. unified\\nclustering locality preserving matrix factorization for\\nstudent performance prediction. iaeng int. j.\\ncomput. sci, 42(3):245–253, 2015.\\n[12] bin ju, yuntao qian, minchao ye, rong ni, and\\nchenxi zhu. using dynamic multi-task non-negative\\nmatrix factorization to detect the evolution of user\\npreferences in collaborative filtering. plos one,\\n10(8):e0135090, 2015.\\n[13] yehuda koren, robert bell, and chris volinsky.\\nmatrix factorization techniques for recommender\\nsystems. computer, 42(8):30–37, august 2009.\\n[14] yehuda koren, robert bell, chris volinsky, et al.\\nmatrix factorization techniques for recommender\\nsystems. computer, 42(8):30–37, 2009.\\n[15] daniel d lee and h sebastian seung. algorithms for\\nnon-negative matrix factorization. in advances in\\nneural information processing systems, pages 556–562,\\n2001.\\n[16] dixin luo, hongteng xu, yi zhen, xia ning,\\nhongyuan zha, xiaokang yang, and wenjun zhang.\\nmulti-task multi-dimensional hawkes processes for\\nmodeling event sequences. in proceedings of the 24th\\ninternational conference on artificial intelligence,\\nijcai’15, pages 3685–3691. aaai press, 2015.\\n[17] rabab naqvi. data mining in educational settings.\\npakistan journal of engineering, technology &\\nscience, 4(2), 2015.\\n[18] xia ning, christian desrosiers, and george karypis.\\na comprehensive survey of neighborhood-based\\nrecommendation methods. in francesco ricci, lior\\nrokach, and bracha shapira, editors, recommender\\nsystems handbook, pages 37–76. springer, 2015.\\n[19] michelle parker. advising for retention and\\ngraduation. 2015.\\n[20] michael j. pazzani and daniel billsus. the adaptive\\nweb. chapter content-based recommendation\\nsystems, pages 325–341. springer-verlag, berlin,\\n\\nheidelberg, 2007.\\n[21] alejandro peña-ayala. educational data mining: a\\nsurvey and a data mining-based analysis of recent\\nworks. expert systems with applications,\\n41(4):1432–1462, 2014.\\n[22] štefan pero and tomáš horváth. comparison of\\ncollaborative-filtering techniques for small-scale\\nstudent performance prediction task. in innovations\\nand advances in computing, informatics, systems\\nsciences, networking and engineering, pages 111–116.\\nspringer, 2015.\\n[23] agoritsa polyzou and george karypis. grade\\nprediction with models specific to students and\\ncourses. international journal of data science and\\nanalytics, pages 1–13, 2016.\\n[24] agoritsa polyzou and george karypis. grade\\nprediction with models specific to students and\\ncourses. international journal of data science and\\nanalytics, pages 1–13, 2016.\\n[25] sanjog ray and anuj sharma. a collaborative\\nfiltering based approach for recommending elective\\ncourses. in international conference on information\\nintelligence, systems, technology and management,\\npages 330–339. springer, 2011.\\n[26] francesco ricci, lior rokach, bracha shapira, and\\npaul b kantor. recommender systems handbook.,\\n2011.\\n[27] jill m simons. a national study of student early\\nalert models at four-year institutions of higher\\neducation. eric, 2011.\\n[28] john z sun, dhruv parthasarathy, and kush r\\nvarshney. collaborative kalman filtering for dynamic\\nmatrix factorization. ieee transactions on signal\\nprocessing, 62(14):3499–3509, 2014.\\n[29] john z sun, kush r varshney, and karthik subbian.\\ndynamic matrix factorization: a state space\\napproach. in 2012 ieee international conference on\\nacoustics, speech and signal processing (icassp),\\npages 1897–1900. ieee, 2012.\\n[30] mack sweeney, jaime lester, and huzefa rangwala.\\nnext-term student grade prediction. in big data (big\\ndata), 2015 ieee international conference on, pages\\n970–975. ieee, 2015.\\n[31] mack sweeney, huzefa rangwala, jaime lester, and\\naditya johri. next-term student performance\\nprediction: a recommender systems approach. arxiv\\npreprint arxiv:1604.01840, 2016.\\n[32] nguyen thai-nghe, lucas drumond, artus\\nkrohn-grimberghe, and lars schmidt-thieme.\\nrecommender system for predicting student\\nperformance. procedia computer science,\\n1(2):2811–2819, 2010.\\n[33] liang xiong, xi chen, tzu-kuo huang, jeff\\nschneider, and jaime g. carbonell. temporal\\ncollaborative filtering with bayesian probabilistic\\ntensor factorization, pages 211–222. 2010.\\n[34] chenyi zhang, ke wang, hongkun yu, jianling sun,\\nand ee-peng lim. latent factor transition for\\ndynamic collaborative filtering. in sdm, pages\\n452–460. siam, 2014.',\n",
       " '39\\n\\n\\x0cthe antecedents of and associations with elective replay\\nin an educational game: is replay worth it?\\nzhongxiu liu\\n\\nnorth carolina state\\nuniversity\\n\\nchrista cody\\n\\nnorth carolina state\\nuniversity\\n\\ntiffany barnes\\n\\nnorth carolina state\\nuniversity\\n\\nzliu24@ncsu.edu\\ncncody@ncsu.edu\\ntmbarnes@ncsu.edu\\ncollin lynch\\nteomara rutherford\\nnorth carolina state\\nuniversity\\n\\ncflynch@ncsu.edu\\nabstract\\nreplayability has long been touted as a benefit of educational games. however, little research has measured its impact on learning, or investigated when students choose to replay prior content. in this study, we analyzed data on a sample of 4,827 3rd-5th graders from st math, a game-based educational platform integrated into classroom instruction in\\nover 3,000 classrooms across the u.s. we identified features\\nthat describe elective replays relative to prior gameplay performance, and associated elective replays with in-game accuracy, confidence, and general math ability assessments outside of the games. we found some elective replay patterns\\nwere associated with learning, whereas others indicated that\\nstudents were struggling in the current educational content.\\nwe suggest, therefore, that educational games should use\\nelective replay behaviors to target interventions according\\nto when and whether replay is helpful for learning.\\n\\nkeywords\\neducational games, serious game analytics, replayability\\n\\n1. introduction\\n“replayability is an important component of successful games.”\\n[15] in most games, there are two types of plays: play and\\nreplay to pass a level (pass attempts) and replay after passing a level (elective replay). in this paper, we investigate\\nthe latter. elective replay (er) is particularly interesting\\nbecause the motivations behind a student’s decision to replay and the impact of those replays are relatively unknown.\\nthis paper explores potential associations between elective\\nreplay and student characteristics and performance in the\\ndomain of educational games.\\nreplayability has been touted as a benefit of educational\\ngames [9]. replayability encourages players to engage in\\n\\nnorth carolina state\\nuniversity\\n\\ntaruther@ncsu.edu\\nrepeated judgement-behavior-feedback loops, where users\\nmake decisions based on the situation and/or feedback, act\\non those decisions, and receive feedback based on their actions [18]. in the retain model designed by gunter et al.\\n[10] to evaluate educational games, replayability is a criteria for naturalization – an important component in helping\\nstudents make their knowledge automatic, reducing the cognitive load of low-level details to allow for higher order thinking. in the retain model, “replay is encouraged to assist\\nin retention and to remediate shortcomings.” [10] meaningful elective replay is often encouraged by game features\\nsuch as score leaderboards, which inspire students to replay for higher scores [4]. because higher scores typically\\nrequire a deeper understanding of the educational content\\nin a well-designed game, encouraging elective replay may\\npromote mastery. games with replay also allow the student to be exposed to more material and give them more\\nfreedom to control their learning. studies have shown that\\ngiving students control over their learning process can increase motivation, engagement, and performance [6, 8].\\nhowever, few studies have investigated when students choose\\nto replay, why they do so, or have measured the outcomes associated with elective replay. one reason is that educational\\ngame studies are often comparatively brief, so replayability\\nis often minimally assessed with post-game questionnaires\\nasking about students’ intention for future play [14, 5]. consequently, there is a need to investigate elective replay with\\nactual logged actions in a game setting where students have\\nsufficient time and freedom to replay.\\nthis work analyzed gameplay logs from a series of math\\ngames within the year-long supplemental digital mathematics curriculum spatial temporal (st) math. we analyzed\\ngameplay data from 4,827 3rd-5th graders throughout the\\n2012-2013 school year. our data contained 37,452 logged\\nelective replays, accounting for 1.48% of the logged play.\\nwe analyzed gameplay and elective replay features in association with students’ demographic information, in-game\\nmath objective tests, and the state standardized math test.\\nwe sought to answer three research questions: q1: what are\\nthe characteristics of students who engage in elective replay,\\nq2: what gets replayed, and under what circumstances?\\nand q3: is elective replay associated with improvements\\nin students’ accuracy on math objectives, confidence, and40\\n\\n\\x0cgeneral math ability?\\n\\n2. related work\\n2.1 factors influencing elective replay\\nfew empirical studies have investigated the motivations behind elective replay in educational games. burger et al. [5]\\nstudied the effect of verbal feedback from a virtual agent\\non replay in the context of a brain-training game. they\\nfound that elaborated feedback increases, whereas comparative feedback decreases, the students’ interest in future replay. they also found that negative feedback generated an\\nimmediate interest in replay, whereas positive feedback created long term interest in the educational content. in another study, plass et al. [14] compared three conditions in a\\nmath game: working individually, competing with another\\nplayer, or collaborating with a peer. the study showed that\\nboth competition and collaboration modes heightened students’ intention to replay when compared with the individual mode, with the latter result being statistically significant. however, both studies measured replay via questionnaires asking the students’ desire to play the entire game\\nagain instead of observed replay behavior. moreover, these\\nstudies sought to understand replay only from the angle of\\ngame design, and did not address the connections, if any,\\nbetween student characteristics and interest in replay.\\nother studies suggest elective replay is a habitual behavior\\nthat arises from individual need, although these studies did\\nnot directly investigate replay. bartle [3] found one type\\nof player who is primarily motivated by concrete measurements of success. in st math, these achiever-type players\\nmay largely use replay to get better ’scores’ (losing fewer\\nlives when passing a level). mostow et al. [12] observed\\na student in a reading tutor who used the learner-control\\nfeatures to spend the majority of time replaying stories or\\nwriting ”junk” stories instead of progressing to new material. thus, some students may also use replay as a form\\nof work avoidance – playing already passed levels instead\\nof solving the current problem or moving on. sabourin et\\nal. [17] found that students in an educational game used\\noff-task behaviors to cope with frustration, implying that\\noff-task behavior can be a productive self-regulation of negative emotions. in st math, when students get frustrated\\nwith the current educational content but still have to play\\nthe game in the classroom, they may replay already learned\\ncontent as a mental break from the current task. these\\nstudies showed that the circumstances of replay and students’ characteristics influence their decisions to replay and\\nits outcomes.\\n\\n2.2\\n\\nthe outcomes of replay\\n\\ndespite the believed benefits of replayability [9, 18, 10, 4],\\nfew studies have investigated the educational impact of elective replay. boyce et al. [4] evaluated the effects of game\\nelements that were designed to motivate gameplay and elective replay. these included a leaderboard that shows each\\nstudent’s rank based upon their score, a tool for creating\\ncustom puzzles, and a social system for messaging among\\nplayers. the experimental design required students to play\\nthe game in one session, and to replay the game as more\\nfeatures were added in the subsequent sessions. the study\\nfound a sharp increase in test scores as these features were\\n\\nadded to the game. the authors concluded that features designed to increase replayability can increase learning gains.\\nhowever, this result may be due to increased time on task as\\nthe same group replaying the base game with new features.\\nin another study, clark et al. [7] analyzed logged studentinitiated elective replay in a digital game. they found that\\nfrequency of elective replay did not correlate with learning\\ngains, prior gaming habits/experience, or how much students liked the game. they also found that, while there\\nwas no statistically significant difference between the male\\nand female students, males replayed more than the females.\\nthis may have been responsible for their slightly higher, although not statistically significant, “best level scores” – the\\nhighest score received on each level. these studies showed\\nthat elective replay may lead to increased learning or higher\\nin-game performance. however, more research is needed to\\nunderstand the potential educational impact of replay in educational games, particularly elective replays initiated solely\\nby the players.\\n\\n3. game, data and features\\n3.1 st math game\\n\\nfigure 1: st math content and examples\\nst math is designed to act as a supplemental program to\\na school’s existing mathematics curriculum. st math is\\nmostly played during classroom sessions, but students have\\nthe option to play it at home. in st math [16], mathematics\\nconcepts are taught through spatial puzzles within various\\ngame-like arenas. st math games are structured at the top\\nlevel by objectives, which are broad learning topics. within\\neach objective, individual games teach more targeted concepts through presentation of puzzles, which are grouped\\ninto levels for students to play. students start by completing a series of training games on the use of the st math\\nplatform and features. they are then guided to complete\\nthe first available objective in their grade-level curriculum,\\nsuch as “multiplication concepts.” students can only see\\nthis objective and must complete a pre-test before beginning\\nthe content. games represent scenarios for problem-solving\\nusing a particular mathematical concept, such as “finding\\nthe right number of boots for x animals of y legs.” each\\ngame contains between one and ten levels, which follow the\\nsame general structure of the game, but increase in difficulty.\\nfigure 1 illustrates the hierarchy of st math content and\\nexamples.\\nas with many games, the student is given a set number of\\n‘lives’ at the start of each level. every time they fail to41\\n\\n\\x0ccomplete a puzzle correctly they lose one life. if all of their\\nlives for a given level are exhausted, they will fail the level\\nand be required to restart the level with a new set of lives.\\nonce a student has passed a level, they can elect to replay\\nit at any time. after a student has passed every level in an\\nobjective, they can take the objective post-test. students\\ncannot progress to the next objective until they have completed the last objective post-test. both the objective preand post-tests consist of 5-10 multiple choice questions related to the objective. the post-tests parallel the pre-tests\\nin both the question format and difficulty of the content.\\nwhile answering each question in both tests, students indicate their relative confidence in their answer (low/high).\\n\\n3.2\\n\\ndata\\n\\nmind research institute (mind), the developers of stmath, collected and provided to the researchers gameplay\\ndata from 4,827 3rd-5th graders during the school year 20122013. these students came from 17 schools and 221 classrooms. table 1 summarizes students’ demographic information. these demographic data, together with students’ state\\nstandardized test scores in 2012 and 2013, were matched to\\ngameplay data through anonymized ids.\\ntable 1: populations’ demographics information\\ngrade3 grade4 grade5\\n#students\\nmale\\neligible for reduced\\nlunch\\nhispanic or latino\\nenglish language\\nlearner\\nwith listed disability\\n\\n1567\\n50.6%\\nna:2.9%\\n80.7%\\nna:2.9%\\n84.7%\\nna:2.8%\\n66.2%\\nna:2.9%\\n10.9%\\nna:2.1%\\n\\n1528\\n50.1%\\nna:2.0%\\n77.8%\\nna:2.1%\\n82.3%\\nna:1.9%\\n56.1%\\nna:2.1%\\n11.5%\\nna:1.7%\\n\\n1732\\n52.2%\\nna:3.5%\\n81.4%\\nna:3.2%\\n83.5%\\nna:3.1%\\n53.0%\\nna:3.2%\\n11.9%\\nna:2.8%\\n\\nthis gameplay data includes pre- and post-tests for each\\nobjective and the number of level attempts. for each preand post-test, st math logged students’ accuracy and selfreported confidence level (1 for ’high’ and 0 for ’low) for\\neach question. for each play at a level, st math logged the\\nstudent’s id, timestamp, and the number of puzzles completed. from these data, we identified er as plays made\\nafter a student initially passed the level. we found ers in\\n89.6% of all objectives in st math, accounting for 1.48%\\nof all level attempts. among 4,827 students, 59.85% ered\\nat least one level, with an average of 7.84 levels (sd=12.99,\\n95% ci [7.37, 8.32]) across 3.06 average objectives replayed\\nper student. in the next section, we describe the features\\nwe created to analyze er.\\n\\n3.3\\n\\nfeatures\\n\\nwe created features at three different levels of granularity\\n(from finest to largest): level, objective, and student. for\\nthe level granularity, we treated each unique student-level\\ncombination as an observation. we calculated the features\\nby averaging all gameplay for a specific student at a specific level. for objective granularity, each unique studentobjective combination was treated as a single observation.\\n\\nfeatures were created by averaging across all levels played by\\na specific student within a single objective. the objective\\ngranularity also included the objective pre- and post-test\\naccuracy and confidence. for the student granularity, we\\ntreated each student as a single observation. we calculated\\nthe features by averaging across all objectives played by a\\nstudent over the entire year. the student granularity also\\nincluded student demographic data and state standardized\\nmath test scores. these granularities ensured that our analysis did not favor units with the majority of data logs. each\\nstudent was considered equally in our analysis, regardless\\nof how many objectives they played. our data contained\\n4,827 students and 2,524,681 plays, which yielded 1,462,660\\nstudent-level observations, and 74,985 student-objective observations.\\ntable 2 shows five example plays of “division-level3,” including four pass attempts and one er of this level, interspersed with ers from other levels. we consider consecutive\\ners as an er session, as these ers are circumstanced on\\nthe same pass attempts.\\ntable 2: example of er and pass attempts\\nplay objective-level\\npassed? play type\\n1\\ndivision- level3\\nno\\npass attempt\\n2\\ndivision- level3\\nno\\npass attempt\\n3\\ndivision-level1\\nyes\\ner (er session1)\\n4\\ndivision- level3\\nno\\npass attempt\\n5\\ndivision-level1\\nyes\\ner (er session2)\\n6\\ndivision- level3\\nyes\\npass attempt\\n7\\ndivision- level3\\nyes\\ner (er session3)\\n8\\nsubtraction-level1 no\\ner (er session3)\\n\\n3.3.1\\n\\npass attempt features\\n\\nwe defined performance to be the percentage of puzzles a\\nstudent completed before losing all lives on the level. pass\\nattempts are plays prior to er, where we assumed students play with the intention of passing the level. pass attempt features included: performance when a student first\\nattempted a level (1st pass attempt performance), number of\\nattempts taken to pass a level (# pass attempts), and average performance of all pass attempts (average pass attempt\\nperformance). at the student granularity, students took an\\naverage of 1.91 (sd=0.89) attempts to pass each level, with\\naverage performance of 0.80 (sd=0.10) on the first pass attempt, and 0.87 (sd=0.07) on all pass attempts (indicating\\noverall improved performance on later attempts).\\n\\n3.3.2\\n\\nelective replay features\\n\\ntable 3 shows er features that describe er from three angles: (i) the frequencies of er, (ii) the performance of er,\\nand (iii) the circumstances of er in terms of the er’s prior\\nplays. to summarize, the majority of ers had higher performance than their levels’ first attempt, and resulted in\\nanother pass of their levels. levels that were ered had similar performance compared to levels that weren’t ered, but\\nlevels that were followed(54.65%) or interrupted (54.35%)\\nby er had much lower performance than those that weren’t\\nfollowed or interrupted by er. most ers’ immediately prior\\npass attempts were from different levels or objectives. there\\nwere few instances (9.80%) where students passed a level and\\nimmediately ered it following the pass.42\\n\\n\\x0ctable 3: elective replay (er) features and their descriptive statistics among students who electively replayed, collapsed to the student granularity.\\ner features\\n\\ndescriptive stats\\n\\ni. frequencies of er\\n% er out of all plays\\nm=2.40%, sd=4.26%\\n% objectives that have been electively replayed\\nm=22.94%, sd=20.89%\\n% objectives whose pass attempts were interrupted/followed by er\\nm=19.48%, sd=17.57%\\nii. performance of er\\nperformance of er\\nm=0.71, sd=0.28\\nm=71.96%, sd=31.44%\\n% ers performed better than the level’s first attempt\\n% ers that result in another pass of the level\\nm=60.36%, sd=35.51%\\niii. circumstances of er\\nthe replayed level e.g. “division-lvl1,”“division-lvl3,” and “subtraction-lvl1” in table 2\\npass attempts features\\nm=0.79, 1.98, 0.87 for 1st performance, #pass attempts, and avg performance\\nthe immediately-prior play of the er e.g. play 2 is the immediately-prior play of play 3 in table2\\nperformance on the immediately-prior play\\nm=0.63, sd=0.29\\n% ers whose immediately-prior plays is also an er\\nm=0.31, sd=0.28\\n% er whose immediately prior pass attempt is on the same level\\nm=9.80%, sd=23.84%\\nm=40.75%, sd=39.09%\\n% ...... on a different level in the same objective\\n% ...... on a different objective\\nm=49.44%, sd=40.76%\\nthe immediate prior pass attempts followed or interrupted by er and er session e.g. “division-lvl3” for\\nall er sessions in table 2\\npass attempts features\\nm=0.51, 3.62, 0.55 for 1st performance, #pass attempts, and avg performance\\n% er sessions whose prior pass attempt passed the level\\nm=45.65%, sd=40.69%\\nnote. statistics are reported at the student granularity, which are calculated through averaging across all objectives played by a student,\\nand then averaged across all students who electively replayed. this means each student contributes equally to the average, regardless of\\nhow many objectives s/he played.\\n\\n3.3.3\\n\\nstudent grouping from er features\\n\\nwe created student groups to encapsulate the circumstances\\nunder which er occurred, based on students’ majority er\\nand er sessions. based on prior literature, we hypothesized\\nthat er is a habitual behavior that arises from individual\\nneeds, such as gaining higher scores [3], avoiding progress on\\nthe current task [12], or taking a mental break from negative emotions [17]. thus, grouping students based upon the\\ncircumstances of replay based on their majority behaviors\\nprovides high level profiles to investigate characteristics of\\nstudents who engaged in er and benefited from er.\\nwe characterized er by the timing relative to the student’s\\ncurrent learning objectives and gameplay. the first grouping describes whether the majority er sessions started before (group b) or after (group a) passing the previous attempted level (current learning objective). if there is a tie\\nbetween the two types of replay session, the student belongs to neither group. for example, table 2 describes a\\ngroup b student, who has two replay sessions before passing\\n“division-level3,” and one replay session after passing this\\nlevel but before moving on to the next level.\\nthe second grouping describes whether an er followed plays\\non the same level (sl), a different level under the same objective (dlso), or a different objective (do). for our example in table 2, the student’s pass attempts on “divisionlevel3” was interrupted twice on the third and fifth plays, by\\nreplays on “division-level1”(dlso). after passing “division-\\n\\nlevel3”, the student replayed the same level(sl) once during\\nthe seventh play, and a different objective “ subtractionlevel1” (do) once during the eighth play. this group b\\nstudent had two dlso replays, one sl, and one do replays.\\nthus, this student also belongs to group dlso, because the\\ntwo groupings are independent of each other.\\n\\n4. methods & results\\n4.1 who engaged in elective replay?\\nwe first investigated the demographic characteristics of students who engaged in elective replay. we found that males\\ndid so more often than females (male: 63.2%, female: 57.0%,\\nc2(1, n=4827) = 17.99, p<.001). we also found that english\\nlanguage learners (ell) did so more often than their nonell peers (ell: 62.3%, non-ell: 57.1%, c2(1, n=4827)\\n= 12.69, p<.001 ), as did students with reported disabilities (disability: 68.7%, non disability: 59.1%, c2(1, n =\\n4827) = 18.17, p<.001). there were no statistically significant differences in the frequencies of er based on race\\nwhen operationalized as hispanic/non hispanic, or based\\non free/reduced lunch eligibility. the frequency of er was\\nnot found to be correlated with other out-of-game student\\nfactors, such as state standardized math test scores.\\nthe frequency of er was also not correlated with in-game\\npre-test accuracy and confidence at the objective granularity. next, we investigated the gameplay characteristics of\\nstudents who electively replayed. we first separated students into groups based on their replay patterns. the first43\\n\\n\\x0ctable 4: mann-whitney u tests comparing gameplay characteristics between er pattern student groups\\ngroup (# stu- pre-test\\npre-test\\navg pass at- avg 1st at- #pass at- er perfordents)\\naccuracy\\nconfidence tempts’ per- tempt per- tempts\\nmance\\nformance\\nformance\\nbase:no er\\n(n=1938)\\ner (n=2889)\\ngroup a\\n(n=1114)\\ngroup b\\n(n=1464)\\ngroup sl\\n(n=173)\\ngroup dlso\\n(n=983)\\ngroup do\\n(n=1399)\\n\\nm=0.61\\nsd=0.17\\n*m=0.57\\nsd=0.17\\nm=0.62\\nsd=0.16\\n*m=0.52\\nsd=0.17\\nm=0.61\\nsd=0.17\\n*m=0.54\\nsd=0.18\\n*m=0.58\\nsd=0.16\\n\\nm=0.75\\nsd=0.23\\nm=0.74\\nsd=0.24\\nm=0.77\\nsd=0.22\\n*m=0.72\\nsd=0.25\\nm=0.75\\nsd=0.23\\nm=0.73\\nsd=0.24\\nm=0.75\\nsd=0.23\\n\\nm=0.88\\nsd=0.08\\n*m=0.87\\nsd=0.07\\n*m=0.90\\nsd=0.05\\n*m=0.84\\nsd=0.07\\nm=0.88\\nsd=0.07\\n*m=0.84\\nsd=0.08\\nm=0.88\\nsd=0.06\\n\\nm=0.81\\nsd=0.11\\n*m=0.80\\nsd=0.10\\n*m=0.84\\nsd=0.08\\n*m=0.75\\nsd=0.09\\nm=0.81\\nsd=0.09\\n*m=0.76\\nsd=0.10\\nm=0.81\\nsd=0.08\\n\\nm=1.82\\nsd=0.84\\n*m=1.92\\nsd=0.78\\n*m=1.62\\nsd=0.52\\n*m=2.28\\nsd=1.09\\nm=1.82\\nsd=0.81\\n*m=2.27\\nsd=1.16\\nm=1.80\\nsd=0.71\\n\\nna\\nm=0.72\\nsd=0.29\\n*m=0.77\\nsd=0.27\\n*m=0.67\\nsd=0.29\\n*m=0.84\\nsd=0.29\\n*m=0.67\\nsd=0.32\\nm=0.73\\nsd=0.26\\n\\nnote. 1) green and red indicate statistically significances higher and lower than the base class, with *p < .001, +p < .01 2)\\ngroup a, b: most er sessions happened before (b), after (a) passing the prior non-replay level. group sl, dlso, do: most\\ner followed pass attempts on the same level(sl), different level in same objective(dlso), or different objective (do)\\n\\n5 columns of table 4 shows the results of mann-whitney u\\ntests with benjamini-hochberg correction to compare each\\ngroup in-game performance to the students who never electively replayed any levels (the base group). the last column\\ncompares the averaged er performance of each group to the\\nrest of students who electively replayed.\\ncompared to the base group, students for whom most replays happened before passing the prior non-replay level\\n(group b) and students for whom most replays followed a\\ndifferent level on the same objective (group dlso) started\\nwith significantly lower pre-test scores and did worse in gameplay, as measured by the three pass attempt features described in section 3.3.2. for example, students in group\\nb started with lower accuracy and confidence at pre-test,\\ntook an average 0.5 more attempts to pass a level, and had\\nlower performance on the 1st pass attempt and all pass attempts (including the 1st). it seems that group b students\\nwho replayed earlier levels before passing the current one\\nhad less prior knowledge, and struggled more in the game.\\nby contrast, students in group a, for whom most replay\\nhappened after passing the current level, did slightly better in gameplay compared to students who never electively\\nreplayed (the base group). because these students started\\nwith pre-test scores that were not statistically significantly\\ndifferent from the base group, their replay patterns are associated with higher gameplay performance.\\n\\n4.2\\n\\nwhat gets replayed, and when?\\n\\nnext, we studied what levels get replayed, and under what\\ncircumstances. we used a decision tree classifier which allowed us to identify which factors are most important in\\nrelative to er. our goal was not to find precise predictive\\nmodels, but to augment our understanding of performance\\nand its relationship to er. we used r’s rpart package with\\nparameters minsplit=5% and cp=0.02 to build trees to classify levels that were replayed from levels that were not replayed, and levels whose pass attempts were interrupted or\\n\\nfollowed by replay from levels that were not interrupted or\\nfollowed by replay. we randomly undersampled the majority class (levels without replay, levels were not interrupted\\nor followed by replay), so that each class represented half of\\nthe observations. we used pass attempt features at the level\\ngranularity together with pre-test results, objective, and demographic information to build our tree. we used 10-fold\\ncross validation to access the trees’ accuracies.\\ntable 5 reports the trees and the importance of the features.\\nwe found that a student’s performance on a particular level\\ninfluenced whether replay happened during/after the level’s\\npass attempts. for example, a student was more likely to\\nreplay a different level under the same objective (dlso)\\nif they took more than two attempts to pass the current\\nlevel. this result is related to the previous result in table 4,\\nshowing that, at the student level, those with lower gameplay performance were more likely to replay another level\\nunder the same objective.\\non the other hand, the objective to which a level belongs\\ninfluences whether or not a level would be ered. we built\\ntrees to predict if a level is replayed following the same level\\n(same condition of the last row in table 5, n=1,776), the\\nsame objective but a different level (n=12,616), or a different objective (n=31,852). for all three conditions, the\\ntrees only contains a single node – objective, with accuracy\\nof 55.2%, 62.0%, and 66.9% respectively. this er decision\\ncould have been influenced by either the content or timing\\nof the objectives. in our tree node, we noticed that many\\nobjectives with a higher chance of er occurred earlier in\\nthe curriculum, this could be because students had more\\ntime in which these objectives were available for er. our\\ntree model also had only 55.2% accuracy when predicting\\nwhether a level would be ered following the pass attempts\\nof itself. one explanation is that we do not have puzzle\\ngranularity data on how many lives a student actually lost.\\nfrom prior literature [4] [7], students may replay the same44\\n\\n\\x0ctable 5: decision trees to predict levels whose pass\\nattempts were interrupted or followed by er\\ncondition:\\ninter- trees\\nrupted/followed by\\ner from a different\\nlevel in the same objective (n=8,094)\\n\\ner from a different\\nobjective (n=12,506)\\n\\ner on the same level\\n(n=1,766)\\n\\n77.8% accuracy\\n#pass attempts < 2.5, no\\n#pass attempts ≥ 2.5, yes\\n\\n78.7% accuracy\\n1st attempt performance ≥ 0.94\\n-objective group a, no\\n-objective group b, yes\\n1st attempt performance < 0.94\\n-objective group a\\n—# pass attempts < 6.5, no\\n—# pass attempts ≥ 6.5, yes\\n-objective group b, yes\\n55.2% accuracy\\nobjective group a, no\\nobjective group b, yes\\n\\nnote. trees are presented in text format. for example, the first\\ntree shows that if a student passed a level with less than 2.5 pass\\nattempts, the tree predicts this student will not replay another\\nlevel during/after this level.\\n\\nlevel following it pass attempts to get a better score, which\\nmeans losing fewer lives (making fewer errors) at a level. as\\nshown in table 4, group sl students who performed most\\nof their ers after the same level also achieved the highest\\ner performance.\\n\\n4.3\\n\\nis elective replay associated with gains?\\n\\nin this section we will address our second research question.\\nas part of our analysis we considered three gain scores: accuracy gain, confidence gain, and math gain. the first two\\nwere measured by in-game pre- and post-tests. recall that\\nboth before and after a student attempts an objective, st\\nmath logs the students’ correctness and confidence scores\\non each question on the pre- and post-tests. we averaged\\nthese scores across the pre- and post-test questions to compute the first two gain scores. these were assessed at the\\nobjective granularity. math gain was calculated based upon\\nthe difference between the students’ state standardized math\\ntest scores in years 2012 and 2013. this was assessed at the\\nstudent granularity.\\n11.8% of the students were excluded from the math gain\\nanalysis due to missing state math test records. these excluded students performed statistically significantly worse in\\nthe game as measured by the three pass attempt features;\\nthis implies that we excluded weaker students. 8.5% of the\\nobjective observations were excluded from the accuracy and\\nconfidence gain analysis due to missing pre- or post-tests.\\nthese excluded observations were not statistically significantly different from the rest as measured by pass attempt\\nfeatures. the accuracy and confidence gains were significantly correlated (r=0.37, p<0.001), but these two gains\\nwere not strongly correlated with math gain scores at the\\nstudent granularity (r<0.1, p<0.001). table 6 reports the\\npercentage of data points that gained, dropped (mainly for\\navoiding ceiling effect in this data), and did not gain for each\\n\\nfigure 2: decision tree to predict whether a student will gain in state standardized math test\\ntype of gain based on the marx and cummings normalization method [11].\\n\\ntable 6: %observations with gains, no gains, and\\npercentage dropped for the three gains\\ngain types\\ner?\\ngained dropped no gain\\naccuracy\\n(n=75,083)\\nconfidence\\n(n=75,083)\\nmath test\\n(n=4,827)\\n\\ner\\nno er\\ner\\nno er\\ner\\nno er\\n\\n48.10%\\n43.70%\\n28.30%\\n26.40%\\n41.60%\\n40.80%\\n\\n8.60%\\n6.10%\\n42.60%\\n37.40%\\n0.40%\\n0.50%\\n\\n37.90%\\n36.60%\\n23.70%\\n22.70%\\n46.90%\\n45.70%\\n\\nnote. 1)observations in the ’dropped’ column (pre- and posttests were both 0 or 1) were excluded from analysis. 2)accuracy and confidence gains were measured at objective granularity, math gain was measured at student granularity. 3)er and no\\ner were collapsed across level.\\n\\nwe first constructed decision trees to partition our data to\\nsee which factors influence gains, using the method described\\nin the prior section. no sampling was necessary because the\\ngroups had similar sizes. we used pass attempt features,\\ner features, pre-test results, and demographics. for student granularity, we also added the percentage of required\\nobjectives attempted by the student.\\nat the objective granularity, we found that pre-test accuracy\\nand confidence were the only selected nodes that predicted\\naccuracy (70.0% accuracy) and confidence gain (74.1% accuracy). students with a pre-test accuracy of < 0.71 (at least 2\\nquestions wrong out of 5-10) had a 64.7% chance of positive\\naccuracy gain in the same objective, while the remainder of\\nthe students had only a 25.9% chance. students with high\\npre-test confidence (≤0.95, indicated confidence on almost\\nall questions) had a 62.5% chance of positive confidence gain\\nin the same objective. it could be that these in-game tests\\nwere too easy, as 18.9% of pretests achieved full scores in\\naccuracy and 54.5% achieved full scores in confidence.\\nour decision tree for the student granularity is shown in\\nfigure 2, with a cross-validated accuracy of 57.8%. students who started with medium level of math abilities (2012\\nstate test math scores <474, and ≥ 347) improved their\\nscores when they performed well in st math (average pass\\nattempts performance > 0.8857). this shows that the game-45\\n\\n\\x0cplay data in st math has predictive power for assessment\\noutside of the game. however, for all three gain scores, the\\ner features were not selected for inclusion in the decision\\ntree nor was any correlation found with the students gains.\\n\\nresearch questions: q1: what are the characteristics of students who electively replay? q2: what gets replayed, and\\nunder what circumstances? and q3: is elective replay associated with improvements in students’ accuracy on math\\nobjectives, confidence, and general math ability?\\n\\ntable 7: mann-whitney u tests comparing gains\\nbetween er pattern student groups.\\ngroup\\n(# math\\naccuracy confidence\\nstudents)\\n(max=600) (max=1) (max=1)\\n\\nwe concluded that, with over half of students who electively\\nreplayed at least one level, er is a common behavior in st\\nmath. moreover, examining elective replay can enhance our\\nunderstanding about how students play and the characteristics of successful play. for example, we found that students who did poorly on the current level were more likely\\nto electively replay a different level during/after the level’s\\npass attempts. we also found that students who generally\\nengaged in elective replay before passing the current level\\n(group b) started with lower pre-test scores, did worse during gameplay, and had the lowest objective-level accuracy\\nand confidence gain and math gains. one explanation for\\nthis result is that weaker students used er as a work avoidance tactic, as found in mostow et al. [12], and that instances of er stand in for lower motivation or engagement\\nfor the objective topic, st math, or mathematics overall.\\n\\nbase:no er\\n(n=1938)\\ner (n=2889)\\ngroup a\\n(n=1114)\\ngroup b\\n(n=1464)\\ngroup sl\\n(n=173)\\ngroup dlso\\n(n=983)\\ngroup do\\n(n=1399)\\n\\nm=31.5\\nsd=146.6\\nm=27.3\\nsd=139.7\\nm=53.4\\nsd=167.9\\n+m=6.7\\nsd=109.0\\nm=46.2\\nsd=161.2\\nm=21.4\\nsd=123.0\\nm=32.3\\nsd=150.6\\n\\nm=0.31\\nsd=0.25\\nm=0.30\\nsd=0.25\\n*m=0.35\\nsd=0.24\\n*m=0.24\\nsd=0.25\\nm=0.31\\nsd=0.28\\n*m=0.25\\nsd=0.26\\nm=0.32\\nsd=0.23\\n\\nm=0.33\\nsd=0.38\\nm=0.32\\nsd=0.37\\n+m=0.38\\nsd=0.36\\n*m=0.26\\nsd=0.37\\nm=0.31\\nsd=0.37\\n*m=0.27\\nsd=0.37\\nm=0.34\\nsd=0.36\\n\\nnote. green and red indicate statistically significances higher\\nand lower than the base class, with *p < .001, +p < .01\\n\\nfinally, we investigated how er patterns relate to gains.\\ntable 7 reports the result from separating students into 6\\ngroups based on er patterns and conducting mann-whitney\\nu tests with benjamini-hochberg correction (as in the previous section). moreover, although decision trees constructed\\nfrom the complete dataset show that low pre-test results\\nled to more gains, some er pattern groups showed opposite\\ntrends. for example, group b, who primarily ered before\\npassing the current level, started with lower pre-test scores,\\ndid worse in the game, and had less gains, which were statistically significant, in all three gain measures. the same\\napplies to group dlso. these two groups of students also\\nhad the lowest er performance.\\non the other hand, the base group and group a (who\\nmostly ered after passing the current level) started with\\npre-test accuracy and confidence scores that are not significantly different (table 4), but group a did significantly\\nbetter in game, and had higher gains in accuracy and confidence, which were statistically significant. because the mean\\npre-test score for the base and a groups is approximately\\n0.6, these students were reasonably familiar with the objective before they began playing it. the difference in accuracy\\nand confidence gains suggest that er after students successfully pass a level helped students learn, or implied better\\nlearning in the previous gameplay.\\n\\n5. discussion and conclusions\\nthis work presents a significant extension on prior studies of\\nreplay which have typically taken place over a short period of\\ntime and have assessed replay via intentional questionnaires\\nnot observed behaviors [14, 5]. this work analyzed logged\\nstudent-initiated elective replay from a sample of 4,827 3rd5th graders during school year 2012-2013 in st math in\\na natural educational setting. we sought to answer three\\n\\non the other hand, compared to students who didn’t er,\\nstudents who mostly electively replayed after passing the\\ncurrent level (group a) started with pre-test scores that\\nwere not significantly different, did better in the game, and\\nhad higher learning and confidence gains. one reason could\\nbe that these students electively replayed for a better score,\\nas we also found that students who mostly replayed the\\nsame level immediately after passing it (group sl) had the\\nhighest er performance. this association is especially true\\namong achiever-type players [3] that prefer to gain concrete\\nmeasurements of success. because losing fewer lives in st\\nmath requires better mastery of the math content, er may\\nhave helped these students learn. another explanation is\\nthat these students’ ers could imply better learning during\\nprior gameplay, as table 4 also shows that group a students\\nhad better pass attempt performance. possibly, successful\\nprior performance motivated these students to electively replay more of the game. moreover, because successful prior\\nperformance feeds self-efficacy [2, 13], confidence gains in\\ngroup a students, who chose more er, may be linked to\\nelectively replaying levels they have already mastered.\\nfrom the application perspective, as expected from this complex environment, our effect-sizes are too small to claim er\\nitself as a powerful intervention for learning. instead, our\\nfindings suggest the potential of using er patterns to identify weaker students and their struggling moments for intervention. for example, students with group b er patterns\\nstarted weaker, did poorly in the game, and had lower gains\\nin learning, confidence, and math state test scores. it may\\nbe the case that group b er (before passing a level) is a\\nsignal that students are struggling in current content and\\nare in need of a mental break [17] or help. if this is the case,\\nit would be beneficial upon detecting these er patterns for\\nst math to alert teachers or to provide interventions, such\\nas suggesting the student to take a break or providing supplemental resources to further explain the math concepts\\nfrom the pass attempts interrupted by er. our results also\\nsuggest avenues for experimental studies that designs a more\\neffective er experience, such as preventing work-avoidance46\\n\\n\\x0cin er. for example, changing the number of lives students\\nhave at each replay, or constraining the problems offered\\neach time they are replayed to be isomorphic but not identical.\\nthis work has several limitations. first, the in-game prepost- tests may be too easy for students, as 18.9% of pretests\\nachieved a full score in accuracy, and 54.5% achieved a full\\nscore in confidence. the high percentage of students with\\nnon-positive learning and accuracy gain could also be caused\\nby students’ slipping or guessing in multiple-choice questions\\n(e.g., 1 incorrect answer reduces accuracy by 14%-20%). the\\naccuracy of the pre- and post-test questions for assessing\\nknowledge might be improved by using short answer questions. the second limitation is that we did not have puzzle\\ngranularity data on how many lives a student actually lost\\nor the types of errors they made. third, the grouping of students based on the majority of elective replay assumes that\\nelective replay is a habitual and consistent behavior. future\\nresearch should investigate other groupings, as well as examining whether there were changes in how students used\\nreplay, and what caused the changes. fourth, future work\\nmay also include creating quantified features to compare the\\ncontent and game features across objectives so we may better understand how the game’s content influence students’\\ndecision to engage in elective replay.\\nin summary, this work adds new insights to our understanding of elective replay in educational games. our work reveals\\ndifferential associations between elective replay and performance when replay is categorized by the timing in relation to\\nthe student’s current learning objectives and gameplay. our\\nwork suggests that low-performing students did not benefit\\nfrom er; high-performing students both chose er at better\\ntimes and their ers were associated with benefits from either er or previous gameplay, which supports the results of\\nprior self-regulation research by aleven et al [1]. this work\\npresents prospects for both examining more detailed characteristics of replay and utilizing experimental manipulations.\\n\\n6.\\n\\nacknowledgements\\n\\nthis work was supported by nsf grant iuse #1544273\\n“evaluation for actionable change: a data-driven approach”\\nteomara rutherford pi, tiffany barnes & collin f. lynch\\nco-pis.\\n\\n7.\\n\\nreferences\\n\\n[1] v. aleven, e. stahl, s. schworm, f. fischer, and\\nr. wallace. help seeking and help design in\\ninteractive learning environments. review of\\neducational research, 73(3):277–320, 2003.\\n[2] a. bandura. perceived self-efficacy in cognitive\\ndevelopment and functioning. educational\\npsychologist, 28:117–148.\\n[3] r. bartle. hearts, clubs, diamonds, spades: players\\nwho suit muds. journal of mud research, 1(1):19,\\n1996.\\n[4] a. boyce, k. doran, a. campbell, s. pickford,\\nd. culler, and t. barnes. beadloom game: adding\\ncompetitive, user generated, and social features to\\nincrease motivation. in the 6th international\\nconference on foundations of digital games, pages\\n\\n139–146. acm, 2011.\\n[5] c. burgers, a. eden, m. d. van engelenburg, and\\ns. buningh. how feedback boosts motivation and play\\nin a brain-training game. computers in human\\nbehavior, 48:94–103, 2015.\\n[6] s. l. calvert, b. l. strong, and l. gallagher. control\\nas an engagement feature for young children’s\\nattention to and learning of computer content.\\namerican behavioral scientist, 48(5):578–589, 2005.\\n[7] d. b. clark, b. c. nelson, h. y. chang,\\nm. martinez-garza, k. slack, and c. m. d’angelo.\\nexploring newtonian mechanics in a\\nconceptually-integrated digital game: comparison of\\nlearning and affective outcomes for students in taiwan\\nand the united states. computers education,\\n57(3):2178–2195, 2011.\\n[8] d. i. cordova and m. r. lepper. intrinsic motivation\\nand the process of learning: beneficial effects of\\ncontextualization, personalization, and choice. journal\\nof educational psychology, 88(4):715, 1996.\\n[9] j. p. gee. what video games have to teach us about\\nlearning and literacy. st. martin’s griffin - macmillan,\\nnew york, usa, 2007.\\n[10] g. a. gunter, r. f. kenny, and e. h. vick. taking\\neducational games seriously: using the retain model\\nto design endogenous fantasy into standalone\\neducational games. educational technology research\\nand development, 56(5-6):511–537, 2008.\\n[11] j. d. marx and k. cummings. normalized change.\\namerican journal of physics, 75(1):87–91, 2007.\\n[12] j. mostow, j. beck, r. chalasani, a. cuneo, p. jia,\\nand k. kadaru. a la recherche du temps perdu, or as\\ntime goes by: where does the time go in a reading\\ntutor that listens? in in international conference on\\nintelligent tutoring systems, pages 320–329, 2002.\\n[13] f. pajares. self-efficacy beliefs in academic setting.\\nreview of educational research, 66:543–578.\\n[14] j. l. plass, p. a. o’keefe, b. d. homer, j. case, e. o.\\nhayward, m. stein, and k. perlin. the impact of\\nindividual, competitive, and collaborative\\nmathematics game play on learning, performance, and\\nmotivation. journal of educational psychology,\\n105(4):1050, 2013.\\n[15] m. prensky. computer games and learning: digital\\ngame-based learning. in handbook of computer games\\nstudies. the mit press, cambridge, ma, usa, 2005.\\n[16] t. rutherford, g. farkas, g. duncan, m. burchinal,\\nm. kibrick, j. graham, l. richland, n. tran,\\ns. schneider, l. duran, and m. martinez. a\\nrandomized trial of an elementary school mathematics\\nsoftware intervention: spatial-temporal math. journal\\nof research on educational effectiveness,\\n7(4):358–383, 2014.\\n[17] j. l. sabourin, j. p. rowe, b. w. mott, and j. c.\\nlester. considering alternate futures to classify\\noff-task behavior as emotion self-regulation: a\\nsupervised learning approach. jedm-journal of\\neducational data mining, 5(1):9–38, 2013.\\n[18] s. thomas, g. schott, and m. kambouri. designing\\nfor learning or designing for fun? setting usability\\nguidelines for mobile educational games. learning with\\nmobile devices: a book of papers, pages 173–181, 2004.',\n",
       " '111\\n\\n\\x0ctowards closing the loop: bridging machine-induced\\npedagogical policies to learning theories\\nguojing zhou, jianxun wang, collin f. lynch, min chi\\ndepartment of computer science\\nnorth carolina state university\\nraleigh, nc 27695\\n\\n{gzhou3,jwang75,cflynch,mchi}@ncsu.edu\\nabstract\\nin this study, we applied decision trees (dt) to extract\\na compact set of pedagogical decision-making rules from\\nan original full set of 3,702 reinforcement learning (rl)induced rules, referred to as the dt-rl rules and full-rl\\nrules respectively. we then evaluated the effectiveness of\\nthe two rule sets against a baseline random condition in\\nwhich the tutor made random yet reasonable decisions. we\\nexplored two types of trees (weighted and unweighted) as\\nwell as two pruning strategies (pre- and post-pruning). we\\nfound that post-pruned weighted trees produced the best results with 529 dt-rl rules. the empirical evaluation was\\nconducted in a classroom study using an existing intelligent\\ntutoring system (its) named pyrenees. 153 students were\\nrandomly assigned to three conditions. the procedure was\\nthe same for all students with domain content and required\\nsteps strictly controlled. the only substantive differences\\nbetween the three conditions were the policy: (full-rl vs.\\ndt-rl vs. random). our result showed that as expected\\nthe machine induced policies (full-rl and dt-rl) are significantly more effective than the random policy; more importantly, no significant difference was found between the\\nfull-rl and dt-rl policies though the number of dt-rl\\nrules is less than 15% of the number of the full-rl rules\\nand the former group also took significantly less time than\\nthe latter.\\n\\n1.\\n\\nintroduction\\n\\nintelligent tutoring systems (itss) are interactive e-learning\\nenvironments that support students’ learning by providing\\ninstruction, scaffolded practice, and on-demand help. the\\nsystem’s behaviors can be viewed as a sequential decisionmaking process where at each step the system chooses an\\nappropriate action from a set of options. pedagogical strategies are the policies used to decide what action to take next\\nin the face of alternatives. each system decision will affect\\nthe user’s subsequent actions and performance. its impact\\non outcomes cannot always be immediately observed and the\\neffectiveness of each decision depends upon the effectiveness\\n\\nof subsequent actions. ideally, an effective learning environment will adapt its decisions to users’ specific needs [1, 11].\\nhowever, there is no existing well-established theory on how\\nto make these system decisions effectively. generally speaking, prior research on pedagogical policies can be divided\\ninto two general categories: top-down or theory-driven, and\\nbottom-up or data-driven.\\nin theory-driven approaches, itss employ hand-coded pedagogical rules that seek to implement existing cognitive or\\nlearning theories [1, 10, 17]. while existing learning literature gives helpful guidance on the design of pedagogical\\nrules, such guidance is often too general to implement as\\neffective immediate decisions. for example, the aptitudetreatment interaction (ati) theory states that instructors\\nshould match their interventions to the aptitude of the learner\\n[5]. while the principle behind this theory is understandable, it is not clear how to implement that rule for each\\ndecision. how do we represent learner’s aptitude for each\\nequation, how exact should be the system’s adaptation, and\\nso on.\\ndata-driven approaches, on the other hand, derive pedagogical policies directly from prior data. here the policies\\nspecify the pedagogical decisions at a detailed level. reinforcement learning (rl), which we use here, is one popular\\napproach that is able to derive pedagogical policies directly\\nfrom student-system interaction logs. these policies are defined as a set of state-action mapping rules, which give the\\nbest decision to take in each state. the states are typically\\nrepresented as sets of features and the actions are pedagogical actions such as presenting a worked example (we) or\\nrequiring the student to solve problems (ps). when the system presents a worked example, the students will be given a\\ndetailed example showing a complete expert solution for the\\nproblem or the best step to take given their current solution\\nstate. in problem solving, by contrast, students are tasked\\nwith solving a problem using the its or with completing an\\nindividual problem-solving step.\\nfor this project, our original complete rl-induced policy involves the following seven features representing the students’\\nlearning process from different perspectives1 .\\n\\n1\\nin the format of: [feature-name] (discretization procedure): explanation of the feature.112\\n\\n\\x0c1. [nwesinceps] (0 → 0; (0, 1] → 1; (1, +∞) → 2):\\nthe number of worked example (we) steps received\\nsince the last problem solving (ps) step.\\n2. [timeinsession] ([0, 2290] → 0; (2290, 4775] → 1;\\n(4775, 7939] → 2; (7939, +∞) → 3): the total time\\nspent in the current session.\\n3. [avgtimeonstepps] ([0, 29.01] → 0; (29.01,\\n48.71] → 1; (48.71, +∞) → 2): the average amount of\\ntime spent on each ps step.\\n4. [avgtimeonstepsessionps] ([0, 23.51] → 0;\\n(23.51, 36.56] → 1; (36.56, 55] → 2; (55, +∞) → 3):\\nthe average amount of time spent on each ps step in\\nthe current session.\\n5. [nstepsincelastwrongkc] ([0, 1] → 0; (1, 7]\\n→ 1; (7, 25] → 2; (25, +∞) → 3): the number of steps\\nreceived since the last wrong ps step on the current\\nknowledge component (kc).\\n6. [nwestepsincelastwrong] ([0, 1] → 0; (1, 4]\\n→ 1; (4, 10] → 2; (10, +∞) → 3): the number of we\\nsteps since the last wrong ps step.\\n7. [ncorrectpsstepsincelastwrongkcsession]\\n(0 → 0; (0, 3] → 1; (3, 10] → 2; (10, +∞) → 3): the\\nnumber of correct ps steps since the last wrong ps\\nstep on the current kc in the current session.\\nwith this feature set, a state can be represented as a 7dimensional vector where each element denotes a discretized\\nfeature value. then, the rules can then be represented as:\\n(0:0:0:0:0:0:0) -> ps\\n(0:0:0:0:0:0:1) -> ps\\n(0:0:0:0:0:1:0) -> ps\\n(0:0:0:0:0:1:1) -> we\\nin this study we discretized the features into three-four values producing a seven-feature state. this results in a state\\nspace of 32 ∗ 45 = 9216, that is 9216 rules in one rl-induced\\npolicy. while these types of polices can specify the exact\\naction to take in each case, they are usually too narrow to\\nbe aligned to existing learning theories. each of the rules\\ncovers only a very specific case and the relationship between\\nrules is unknown. thus it is impossible to explain the power\\nof those rules from the perspective of learning theory. the\\nopacity of those induced rules not only hinders us in improving data-driven methodologies when they go wrong, it also\\nprevents us from advancing learning science research more\\ngenerally. moreover, it is possible that some of the decisions\\nare environment-specific and may not generalize to other\\ncontexts. this in turn prevents translating these induced\\npolicies to environments other than the one from which they\\nare induced. therefore, a general method is needed to shed\\nsome light on the extracted detailed data-driven policies.\\ndecision tree (dt) induction is a robust data mining approach which can be used to extract a compact set of rules\\nfrom a set of specific examples. it builds a tree-like hierarchical decision-making pattern which represents the knowledge it learned. each path from root to leaf represents a\\nsingle rule which may be dealt with separately. prior studies have shown that dts can match training examples in\\nmost cases, even with relatively small trees. davidson et\\n\\nal., for example, built a dt for predicting the extinction\\nrisk of mammals [6]. each of the species was described by\\n11 ecological features (e.g body mass, geographic range and\\npopulation density) and were labeled with their extinction\\nrisk (threatened vs. non-threatened). their tree contained\\n20 general rules which covered 4500 training examples, with\\na decision accuracy over 80%. additionally, reinchard et al.\\nbuilt a dt for predicting the invasiveness of woody plants\\n[13]. the resulting dt encoded 15 rules from 235 examples,\\nwith a decision accuracy over 76%. therefore, in our study,\\nwe will apply dt to extract general pedagogical decisionmaking rules from the detailed rl-induced policies.\\nin short, our primary research question is: is dt an effective methodology for extracting more general pedagogical\\nrules from the detailed rl-induced pedagogical rules? in order to investigate this question, we will build dts using the\\nrules in a rl-induced policy as training examples and empirically evaluate the effectiveness of the extracted set of dt\\nrules by comparing it to the full set of rl-induced rules in a\\nclassroom study. the state features in the rl-induced policies are the input features for the dt and the pedagogical\\nactions are the output labels. in our empirical evaluation,\\nwe separate the pedagogical decisions from the instructional\\ncontent, strictly controlling the content so that it is equivalent for all participants by 1) using an its which provides\\nequal support for all learners; and 2) focusing on tutorial\\ndecisions that cover the same domain content, in this case\\nwe versus ps.\\n\\n2. background\\n2.1 applying rl to itss\\nbeck et al. applied rl to induce pedagogical policies that\\nwould minimize the time students take to complete problems on animalwatch, an its for grade school arithmetic\\n[2]. they trained the model with simulated students. the\\nlow cost of generated data allowed them to apply a modelfree rl method, temporal difference learning. during the\\ntest phase, the induced policies were added to animalwatch\\nand the new system was empirically compared with the original system. their results showed that the policy group\\nspent significantly less time per problem than their no-policy\\npeers. note that their primary goal was to reduce the amount\\nof time per problem, however faster problem-solving does\\nnot always result in better learning performance. nonetheless, their results showed that rl can be successfully applied\\nto induce pedagogical policies for itss.\\niglesias et al., on the other hand, focused on applying rl to\\nimprove the effectiveness of an intelligent educational system that teaches students database design [8, 9]. they\\napplied another model-free rl algorithm, q-learning to induce policies that provide students with direct navigation\\nsupport through the system’s content. they used simulated\\nstudents to induce the policy and empirically evaluated its\\neffectiveness on real students. their results showed that\\nwhile the policy led to more effective system usage behaviors from students, the policy students did not outperform\\nthe no-policy peers in terms of learning outcomes.\\nshen investigated the impact of both immediate and delayed reward functions on rl-induced policies and empirically evaluated the effectiveness of the induced policies within113\\n\\n\\x0can intelligent tutoring system called deep thought [15].\\nthe induced pedagogical policies are used to decide whether\\nthe next task should be we or ps. they found that some\\nlearners benefited significantly more from effective pedagogical policies than others.\\nfinally, chi et al. applied model-based rl to induce pedagogical policies to improve the effectiveness of an intelligent\\nnatural language tutoring system for college-level physics\\ncalled cordillera [4]. the authors collected an exploratory\\ncorpus by training human students on an its that makes\\nrandom decisions and then applied rl to induce pedagogical policies from the corpus. they showed that the induced\\npolicies were significantly more effective than the prior ones.\\nin short, prior studies have shown that rl-induced pedagogical policies can improve students’ learning or reduce\\ntraining time. however, all of these studies focused on the\\neffectiveness of the rl-induced policies. none of them considered extracting more general rules from the induced policies.\\n\\n2.2\\n\\nextracting general rules\\n\\nin addition to the work of davidson et al. [6] and reinchard\\net al. [13], dts have been used for other tasks. vayssiers\\net al., for example, applied classification and regression\\ntrees to predict the presence of 3 species of oak in california [18]. their training examples were vegetation type map\\nrecords for 2085 unique locations. each record consisted of\\n25 climatic and geographic features as well as 3 labels showing the presence of the species (quercus agrifolia, quercus\\ndouglasii and quercus lobata). one dt was induced for\\neach type. the dts were tested on another dataset which\\ncontains the same type of records for 2016 locations. for\\nquercus agrifolia, the induced tree had 10 leaf nodes and\\n94.9% of its predictions are correct for the locations that\\nhave the presence of this oak (sensitivity) while 86.7% of\\nits predictions are correct for cases without the oak (specificity). for quercus douglasii, the induced tree had 22 leaf\\nnodes and a sensitivity and specificity of 87% and 79.9%\\nrespectively. for quercus lobata, the tree had 6 leaves but\\nreached a sensitivity of 77% and a specificity of 73.3%.\\nthus, prior studies have shown that dt can effectively extract a small set of general decision-making rules from a\\nlarge set of specific examples. however, all the examples\\nused by these studies were observations of existing phenomena. so far as we know, this work is the only relevant research on the application of dt to extract a compact set\\nof decision-making rules directly from full rl-induced rules\\nand empirically evaluated the two sets of the rules.\\n\\n2.3\\n\\napplying dt to rl\\n\\nprior research on incorporating dt with rl has largely\\nfocused on seeking a better representation of state space\\nor policy for rl. boutilier et al [3]. proposed representational and computational techniques for markov decision\\nprocesses (mdps) to reduce the size of the state space.\\nthey used dynamic bayesian networks and dts to represent stochastic actions as well as dts to represent rewards.\\nbased upon this representation, they then developed algorithms to find conditional optimal policies. their method\\nwas empirically evaluated on several planning problems and\\n\\nthey showed significant savings in both time and space for\\nsome types of problems. gupta et al. proposed the policy\\ntree algorithm for rl. this algorithm is designed to directly\\ninduce a functional representation of the conditional optimal\\npolicies as a dt. they evaluated it on a variety of domains\\nand showed that it was able to make splits properly [7].\\nin short, prior researchers have shown that properly combining dt with rl can result in a large amount of savings\\nin time and space for finding good policies. however, none\\nof these studies directly applied dt on rl-induced policies.\\n\\n3.\\n\\ninduce full set of rl-policy\\n\\npreviously, researchers have typically used the markov decision process (mdp) [16] framework to model user-system\\ninteractions. the central idea behind this approach is to\\ntransform the problem of inducing effective pedagogical policies on what action the agent should take to the problem of\\ncomputing an optimal policy for an mdp.\\n\\n3.1 markov decision process\\nan mdp is a mathematical framework for representing an\\nrl task. it is defined by: a tuple hs , a, t , ri. where s =\\n{s1 , s2 , ..., sn } denotes the state space; a = {a1 , a2 , ..., am }\\nrepresents a set of agent’s possible actions; and t : s × a ×\\ns → [0, 1] is a transition probability table, where each element is tsai sj = p(sj |si , a). this in turn indicates the\\nprobability of transiting from state si to state sj by taking an action a while r : s × a × s → r assigns rewards\\nto state transitions given actions. the policy is defined as\\nπ : s → a, mapping state s into action a with the goal of\\nmaximizing the expected reward.\\nafter defining an mdp, we can transfer the student-system\\ninteraction dialog into the trajectory which can then be represented as follows:\\na ,r\\n\\na ,r\\n\\na ,r\\n\\n1\\n2\\n3\\ns1 −−1−−→\\ns2 −−2−−→\\ns3 −−3−−→\\n... → sn\\n\\na ,r\\n\\ni\\nwhere si −−i−−→\\nsi+1 means that the tutor executed action\\nai and received reward ri in state si , and then transferred\\nto the next state si+1 . in general, the reward can be divided\\ninto two categories, immediate and delayed, where immediate rewards are received during the state transition, and\\ndelayed are available after reaching to goal state.\\n\\n3.2 training datasets\\nour training dataset was collected from three exploratory\\nstudies in which students were trained on an its which made\\nrandom yet reasonable pedagogical decisions. the studies\\nwere given as homework assignments during csc226: discrete mathematics, a core cs course offered at ncsu during the fall 2014, spring 2015 and fall 2015 semesters. the\\ndataset contains a total of 149 students’ interaction logs.\\nall students used the same its, followed the same general\\nprocedure, studied the same training materials, and worked\\nthrough the same training problems. in order to model the\\nstudents’ learning process, we extracted a total of 142 state\\nfeature variables, which can be grouped into five categories:\\n1. autonomy (am): the amount of work done by the student: such as the number of problems solved so far pscount\\nor the number of hints requested hintcount.114\\n\\n\\x0c2. temporal situation (ts): the time related information about the work process: such as the average time taken\\nper problem avgtime, or the total time spent solving a problem totalpstime.\\n3. problem solving (ps): information about the current\\nproblem solving context, such as the difficulty of the current\\nproblem probdiff, or whether the student changes the difficulty level newlevel.\\n4. performance (pm): information about the student’s\\nperformance during problem solving: such as the number of\\nright application of rules rightapp.\\n5. student action (sa): the statistical measurement of\\nstudent’s behavior: such as the number of non-empty-click\\nactions that students take actioncount, or the number of\\nclicks for derivation appcount.\\n\\nbased methods and an ensemble method and capped the\\nmaximum number of state feature size to be eight. more\\ndetails of our feature selection methods are described in [14].\\nthe final resulting rl policy involves seven state features\\nand 3706 rules.\\n\\n3.3\\n\\n4.1 unweighted vs. weighted tree\\n\\ninducing rl policies\\n\\nin order to apply rl to induce pedagogical policies, we\\nfirst defined the pedagogical decision-making problem as an\\nmdp. the state representation includes all of the relevant\\nfeatures available at the beginning of each step. the actions are we and ps at the step level. the transition tables were calculated on our training dataset, and our reward\\nfunction includes two types of reward: delayed and immediate. our most important reward is based on normalized\\n), which measures the\\nlearning gain (nlg) ( posttest−pretest\\n1−pretest\\nstudents’ learning gains irrespective of their incoming competence. this reward was given as a delayed reward as nlg\\nscores can only be calculated after students finish the entire\\ntraining process. however, shen et al. [15] showed that giving immediate rewards can lead to the production of more\\neffective policies when compared to delayed rewards. this\\nis known as the credit-assignment problem. the more that\\nwe delay success measures from a series of sequential decisions, the more difficult it becomes to identify which of the\\ndecision(s) in the sequence are responsible for our final success or failure. therefore, for the purposes of this study we\\nalso assigned immediate rewards based upon the students’\\nperformance during training on the system.\\nthe value iteration algorithm was applied to find the optimal\\npolicy. this algorithm operates by finding the optimal value\\nfor each state v ∗ (s). the optimal value for a given state is\\nthe expected discounted reward that the agent will gain if\\nit starts in s and follows the optimal policy to the goal.\\ngenerally speaking, v ∗ (s) can be obtained by the optimal\\nvalue function for each state-action pair q∗ (s, a) which is\\ndefined as the expected discounted reward the agent will\\ngain if it takes an action a in a state s and follows the optimal\\npolicy to the end. the optimal state value v ∗ (s) and value\\nfunction q∗ (s, a) can be obtained by iteratively updating\\nv (s) and q(s, a) via equations 1 and 2 until they converge:\\nx\\nq(s, a) := r(s, a) + γ\\np(sj |si , a)v (s0 )\\n(1)\\nv (s)\\n\\n:=\\n\\ns0 ∈s\\n\\nmax q(s, a)\\na\\n\\n(2)\\n\\nhere, p(sj |si , a) is the estimated transition model t , r(s, a)\\nis the estimated reward model and 0 ≤ γ ≤ 1 is a discount\\nfactor.\\nto induce effective pedagogical policies, we combined rl\\nwith various feature selections including 10 types of correlation-\\n\\n4.\\n\\nextracting compact dt-rl sets\\n\\nin order to extract a more compact set of decision-making\\nrules from the full set of rl-induced rules, we implemented\\nthe id3 algorithm to build dts [12]. each rule in the final\\nrl-induced policy was used as a training example. two\\ntypes of decision trees were built: unweighted and weighted,\\nas well as two types of pruning strategies were implemented:\\npre- and post-pruning. next, we will discuss each of them\\nin turn.\\n\\nthe decision to give a we vs. ps may impact students’\\nlearning differently in different situations. we therefore built\\ntwo types of decision trees: unweighted and weighted. unweighted trees treated each decision equally while weighted\\ntrees take account of the relative importance of each pedagogical rule. when applying the value iteration algorithm\\nto induce the optimal policy, we generate the optimal value\\nfunction q∗ (s, a), which gives the expected discounted reward each agent will gain if it takes an action a in a state s\\nand follows the optimal policy to the end. for a given state\\ns, a large difference between the values of q(s, “p s”) and\\nq(s, “w e”) indicates that it is more important for the its\\nto follow the optimal decision in the state s. we therefore\\nused the absolute difference between the q values for each\\nstate s to weight each rl pedagogical rule.\\nthe id3 algorithm builds a tree recursively from root to\\nleaves. on each iteration of the construction process the\\nalgorithm will check the state of the dataset for the current\\nbranch. it will then select a test feature for the current\\nnode based upon the weighted information gain. the current\\nnode will then be expanded by adding branches to it, each\\nof which represents a possible value for the selected feature.\\nthe data will be partitioned over the branches according to\\nthe value of the test feature. the selected feature cannot\\nbe used again by its children. weighted information gain is\\ndefined by the difference between the weighted entropy of the\\nexamples before it is selected and after they are separated\\nby feature value. the weighted entropy of a node can be\\ncalculated by equation 3\\nh(g) = −\\n\\nj\\nx\\n\\np(i|g)log2 p(i|g)\\n\\n(3)\\n\\ni=1\\n\\nj is the total number of output label classes. in our case,\\nit is the number of pedagogical actions (we or ps) which\\nis 2 . p(i|g) is the\\nweighted frequency defined by the equap\\np\\nw\\ntion: p(i|g) = p x∈i wxy .\\nx∈i wx is the total weight of the\\ny∈g\\nexamples\\nwhich\\nare\\nin\\nnode\\ng and which belong to class i.\\np\\nand y∈g wy is the total weights of examples in node g.\\nthe information gain of spliting the current set of training\\nexamples using feature f can be calculated by equation 4:\\nig(f, g) = h(g) −k\\nx\\nj=1\\n\\np(tj |g)h(tj )\\n\\n(4)\\n\\n115\\n\\n\\x0cp(tj |g) is the\\nweighted frequency of the examples in node g:\\np\\np\\nxf =t,x∈g wx\\np\\np(tj |g) =\\n.\\nxf =t,x∈g wx is the total weights\\ny∈g wy\\nof\\nexamples\\nin\\nnodes\\ng\\nwhose\\nvalue of feature f is j and\\np\\ny∈g wy is the total weight of examples in nodes g.\\n\\n4.2\\n\\npre-pruning and post-pruning\\n\\nto control the size of rules induced by dt, we examined\\ntwo types of pruning strategy: pre- and post-pruning. the\\npre-pruning is conducted during the process of building the\\ntree and it used the information gain to determine whether\\nto expand or to terminate. only nodes with an information\\ngain greater than a threshold times its depth: ig(f, g) ≥\\nθ × dg will be expanded and others will be made as a leaf.\\nθ is a fixed threshold and dg is the depth of node g.\\npost-pruning is conducted after the whole decision tree is\\nbuilt and it used the error rate as the pruning measure. the\\nerror\\nrate before a node is expanded is defined as: eg =\\np\\ni∈i wi\\n. i is the set of the decisions incorrectly classified\\n|g|\\nby node g and |g| is the total number of examples in the\\nnode g. the\\np error\\np rate after a node is expanded is defined\\nwi\\nas: ec = c∈c |g|j∈ic . c is the set of children nodes\\nof g after it is expanded and ic is the set of the decisions\\nincorrectly classified by the node c. in post-pruning, if the\\ndifference of a node’s error rate from before to after split is\\nless than a threshold, the node will be pruned by removing\\nall of its branches to make it a leaf node.\\n\\n4.3\\n\\nthe compact set of dt-rl rules\\n\\nin order to induce a compact set of dt-rl rules, we applied the dts to the full set of 3706 rl-induced rules. the\\ninduced unweighted and weighted dts without pruning has\\n2527 and 2456 rules (leaf nodes) respectively. thus, without pruning, dts are already able to extract a smaller set\\nof rules: it reduced the total number of rules by over 1000.\\nfigure 1 shows the relationship between the number of leaf\\nnodes (x-axis) and the inverted weighted accuracy (y-axis).\\nweighted accuracy(w a) is the weighted percentage of decisions correctlypmade, which can be calculated by the equation: w a =\\n\\ndi ∈t\\n\\np\\n\\ndi\\n\\nwi\\n\\nwi\\n\\n. t is the set of correct predictions\\n\\nmade by a dt and wi is the weight of decision i. the inverted weighted accuracy (iw a) is iw a = w a−10 , the\\nlower the better. since our goal is to find a good balance\\npoint between the iwa and the number of leaf nodes, we\\napplied a widely used strategy called the elbow method,\\nto select the best tree. as we can see in the figure, the\\nelbows for the two unweighted tree approaches are around\\n800 and 1700 rules (x-axis) for the pre and post pruning\\nrespectively while the elbows for the two weighted tree approaches are around 250 and 500 for the pre and post pruning respectively. so it seems that weighted tree can extract\\nmore compact set of rules than the unweighted trees. while\\nthe weighted pre-pruning approach has around 250 rules,\\nits iwa is much higher than the weighted post-pruning approach. therefore, we chose the weighted tree with postpruning strategy which has the an elbow at about 500 leaf\\nnodes and reasonable iwa.\\nto further justify our dt choice, table 1 shows the relationship between the pruning thresholds, w a and the number\\n\\nfigure 1: leaf nodes - accuracy\\nof leaf nodes for the weighted tree with post-pruning. table 1 shows that the tree with the closest number of leaves\\nto 500 is the 529 one. it can be obtained by apply a pruning\\nthreshold of 0.8 and the result tree has a weighted accuracy\\nof 0.76. the rules in the resulted tree will be the rules used\\nin the dt-rl condition.\\nin short, we applied dt on rl-induced pedagogical policies\\nto extract a more compact set of decision-making rules. the\\neffectiveness of the original full set and the compact set of\\npolicies were empirically compared against a baseline policy\\nwhich makes random yet reasonable decisions: ps vs. we.\\nthus, we have three conditions:\\n1. full-rl: the full set of 3706 rl-induced rules.\\n2. dt-rl: the compact set of 529 dt-induced rl rules.\\n3. random: the random yet reasonable policy.\\n\\n5.\\n\\nempirical experiment\\n\\nparticipants: this study was conducted in the undergraduate discrete mathematics course at the department\\nof computer science at nc state university in the fall of\\n2016. 153 students participated in this study, which was\\ngiven as their final homework assignment.\\nconditions: students in the study were assigned to three\\nconditions via balanced random assignment based upon their\\ncourse section and performance on the class mid-term exam.\\nsince the primary goal of this work is to examine the effectiveness of the two rl based policies, we assigned more\\nstudents to the full-rl and dt-rl conditions than in the\\nrandom condition. the final group sizes were: n = 61 (fullrl), n = 51 (dt-rl), and n = 41 (random).\\ndue to preparations for exams and length of the experiment,\\n126 students completed the experiment. 5 students were\\nexcluded from the subsequent analysis due to perfect pretest\\nscores, working in group or gaming the system during the\\ntraining. the remaining 121 students were distributed as\\nfollows: n = 45 for full-rl; n = 41 for rl-dt; n = 35\\nfor random. we performed a χ2 test of the relationship\\nbetween students’ condition and their rate of completion\\nand found no significant difference among the conditions:\\nχ2 (2) = 0.955, p = 0.620.\\nprobability tutor: pyrenees is a web-based its for probability. it covers 10 major principles of probability, such\\nas the complement theorem and bayes’ rule. pyrenees116\\n\\n\\x0cthreshold\\nwa\\nleaves\\n\\ntable 1: weighted dt with post-pruning\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n1.00 0.99 0.98 0.96 0.93 0.89 0.85 0.79\\n2456 2217 2029 1809 1608 1383 1043 758\\n\\nprovides step-by-step instruction and immediate feedback.\\npyrenees can also provide on-demand hints prompting the\\nstudent with what they should do next. as with other systems, help in pyrenees is provided via a sequence of increasingly specific hints. the last hint in the sequence, the\\nbottom-out hint, tells the student exactly what to do. for\\nthe purposes of this study we incorporated three distinct\\npedagogical decision modes into pyrenees to match the three\\nconditions.\\nprocedure: in this experiment, students were required to\\ncomplete 4 phases: 1) pre-training, 2) pre-test, 3) training on\\npyrenees, and 4) post-test. during the pre-training phase,\\nall students studied the domain principles through a probability textbook, reviewed some examples, and solved certain\\ntraining problems. the students then took a pre-test which\\ncontained 14 problems. the textbook was not available at\\nthis phase and students were not given feedback on their answers, nor were they allowed to go back to earlier questions.\\nthis was also true of the post-test.\\nduring phase 3, students in all three conditions received\\nthe same 12 rather complicated problems in the same order\\non pyrenees. each main domain principle was applied at\\nleast twice. the minimal number of steps needed to solve\\neach training problem ranged from 20 to 50. these steps\\nincluded defining variables, applying principles, and solving equations. the number of domain principles required to\\nsolve each problem ranged from 3 to 11. all of the students\\ncould access the corresponding pre-training textbook during this phase. each step in the problems could have been\\nprovided as either a we or ps based upon the condition\\npolicy. finally, all of the students completed a post-test\\nwith 20 problems. 14 of the problems were isomorphic to\\nthe pre-test given in phase 2. the remaining six were nonisomorphic complicated problems.\\ngrading criteria: the test problems required students to\\nderive an answer by writing and solving one or more equations. we used three scoring rubrics: binary, partial credit,\\nand one-point-per-principle. under the binary rubric, a solution was worth 1 point if it was completely correct or 0\\nif not. under the partial credit rubric, each problem score\\nwas defined by the proportion of correct principle applications evident in the solution. a student who correctly applied 4 of 5 possible principles would get a score of 0.8. the\\none-point-per-principle rubric in turn gave a point for each\\ncorrect principle application. all of the tests were graded in\\na double-blind manner by a single experienced grader. the\\nresults presented below are based upon the partial-credit\\nrubric but the same results hold for the other two. for\\ncomparison purposes, all test scores were normalized to the\\nrange of [0,1].\\n\\n6.\\n\\n0.8\\n0.76\\n529\\n\\n0.9\\n0.68\\n231\\n\\nempirical results\\n\\nsince both the full-rl and dt-rl policies are based on an\\nrl-induced policy, we combined the two conditions together\\nas the induced group to evaluate the effectiveness the rlinduced policy. the evaluation was conducted by comparing\\nthe induced group with the baseline random condition on\\nlearning performance and training time. moreover, in order to further discover to what extent the compact policy\\nretained the power of the full policy, we compared the fullrl and dt-rl conditions on the same measures. next, we\\nwill discuss each of the comparisons in turn.\\n\\n6.1 induced vs. random\\nwe measured students’ incoming competence via the pretest scores collected before training took place. table 2\\nshows a comparison between the induced group and the\\nrandom group in terms of learning performance. the parenthesized values following the group names in row 1 denote\\nthe number of students in each group. the second row in this\\ntable shows the pre-test scores. the last column shows the\\npairwise t-test results. pairwise t-tests on students’ pre-test\\nscores show that there is no significant difference between\\nthe two groups: t(119) = −0.346, p = 0.730, d = 0.069.\\nthus, despite attrition, the two groups remained balanced\\nin terms of incoming competence. next, we will compare the\\ntwo groups in terms of learning performance in the post-test\\nand training time.\\nrows 2 - 4 in table 2 show a comparison of the pre-test, isomorphic post-test (14 isomorphic questions), and adjusted\\npost-test scores between the two groups along with the mean\\nand sd for each. in order to examine the students’ improvement through training on pyrenees, we compared their\\nscores on the pre-test and isomorphic post-test questions.\\na repeated measures analysis using test type (pre-test and\\nisomorphic post-test) as factors and test score as the dependent measure showed a main effect for test type: f (1, 119) =\\n98.75, p < 0.0001. further comparisons on group by group\\nbasis showed that on the isomorphic questions, both groups\\nscored significantly higher in the post-test than in the pretest: f (1, 85) = 81.30, p < 0.0001 for induced and f (1, 34) =\\n18.30, p = 0.0001 for random respectively. this suggests\\nthat the basic practice and problems, domain exposure, and\\ninteractivity of our its might help students to learn even\\nwhen pedagogical decisions are made randomly.\\nin order to investigate the effectiveness of the induced policies, we compared students’ overall learning performance,\\nwhich was evaluated by their adjusted post-test scores, between the two groups. a one-way ancova analysis was\\nconducted on their overall post-test scores (20 questions),\\nusing the pretest scores as a covariate to factor out the influence of their incoming competence. the result shows a\\nsignificant main effect: f (1, 118) = 4.628, p = 0.033. that\\nis, the induced group significantly outperformed the random group on adjusted post-test scores, which is shown in117\\n\\n\\x0ccond\\npre\\niso post\\nadjusted post\\ntime\\nwe steps\\nps steps\\nwe pct(%)\\n\\ntable 2: induced vs. random\\ninduced(86)\\nrandom(35)\\nt-test result\\n.686(.194)\\n.699(.171)\\nt(119) = −0.346, p = 0.730, d = 0.069\\n.851(.155)\\n.812(.195)\\nt(119) = 1.141, p = 0.256, d = 0.229\\n.751(.144)\\n.689(.138)\\nt(119) = 2.162, p = 0.033, d = 0.433\\n105.87(34.30) 111.18(27.33) t(119) = −0.815, p = 0.417, d = 0.163\\n205.74(62.73) 189.46(11.39)\\nt(119) = 1.522, p = 0.131, d = 0.305\\n173.69(61.14) 190.26(10.28) t(119) = −1.591, p = 0.114, d = 0.319\\n54.16(16.35)\\n49.89(2.78)\\nt(119) = 1.532, p = 0.128, d = 0.307\\n\\nthe fourth row of table 2. therefore, the results showed that\\nthe induced policies are significantly more effective than the\\nrandom policy.\\nthe fifth row in table 2 shows the average amount of total\\ntraining time (in minutes) students spent on our its for each\\ngroup. pairwise t-test showed no significant difference in\\ntraining time between the two groups: t(119) = −0.815, p =\\n0.417, d = 0.163. the results suggest that when compared\\nto the random policy, the induced policies generally do not\\nhave a significant different impact on students’ training time.\\nthe last three rows in table 2 show the number of we\\nand ps steps given as well as the percentage of we steps\\nreceived by the induced and the random group. pairwise\\nt-tests showed that there is no significant difference between\\nthe two groups on these three measures.\\n\\n6.2\\n\\nfull-rl vs. dt-rl\\n\\nwe then performed the same comparison between the fullrl and dt-rl conditions in order to examine the effectiveness of the dt-extracted compact policy. the second row\\nin table 3 shows the pre-test scores for each condition. a\\npairwise t-test on the scores shows no significant difference\\nbetween the two conditions: t(84) = −0.168, p = 0.867,\\nd = 0.036. thus the two conditions were balanced in terms\\nof incoming competence.\\nthe pre-test, isomorphic post-test and adjusted post-test\\nscores are shown in rows 2 - 4 of table 3. a repeated measures analysis using test type (pre-test and isomorphic posttest) as factors and test score as dependent measure showed\\na main effect for test type: f (1, 85) = 81.30, p < 0.0001.\\nfurther comparisons on group by group basis showed that\\nboth conditions scored significantly higher in isomorphic\\npost-test than in pre-test: f (1, 44) = 42.16, p < 0.0001\\nfor full-rl and f (1, 40) = 39.16, p < 0.0001 for dt-rl.\\nthese results suggest that the students can effectively learn\\nfrom pyrenees with the full and compact policies.\\nin order to discover to what degree the compact policy retained the effectiveness of the full policy, we compared the\\npost-test scores between the two conditions. the results\\nof a pairwise t-test showed no significant different between\\nthem on isomorphic post-test: t(84) = 0.505, p = 0.615,\\nd = 0.109. we also conducted an ancova analysis on the\\noverall post-test scores using the pretest scores as a covariate and still found no significant different between the two\\nconditions: f (1, 83) = 0.348, p = 0.557. in short, while on\\npost-test scores, the dt-rl condition scored slightly lower\\nthan the full-rl condition, the difference is not significant.\\n\\nthe fifth row of table 3 shows the average amount of time\\nstudents spent on training. as the row shows, the fullrl condition spent significantly more time than the dt-rl\\ncondition: t(84) = 3.829, p = 0.0002, d = 0.827. thus\\nthe full-rl and dt-rl policies have significant different\\nimpact upon the students’ training time.\\nthe last three rows of table 3 show the number of we\\nand ps steps given and the percentage of we steps received by the full-rl and the dt-rl condition. pairwise t-tests showed that comparing to the dt-rl condition, the full-rl condition received significantly fewer we\\nsteps: t(84) = −4.952, p < 0.0001, d = 1.069; received a\\nlower percentage of we steps: t(84) = −4.955, p < 0.0001,\\nd = 1.070; and completed more ps steps: t(84) = 4.999,\\np < 0.0001, d = 1.079. these results suggest that the pedagogical decisions made by the compact and full policies are\\nsubstantively different.\\n\\n7.\\n\\ndiscussion\\n\\nin this study, we applied dt to extract a compact set of\\npedagogical rules from the full set of rl-induced rules and\\nempirically evaluated the effectiveness of two sets of rules in\\na classroom study. our goal was to shed some light on the\\nrl-induced policies and we think this is only the first step\\ntowards narrowing the gap and building a bridge between\\nmachine-induced pedagogical policies and learning theories.\\nin order to find the best dt, we explored two types of tree:\\nunweighted and weighted; and for each of them, we conducted two types of pruning strategy: pre- and post-pruning.\\nafter comparing the performance among them, we selected\\nthe weighted tree with the post-pruning strategy to perform\\nthe extraction of general decision-making rules. the rlinduced policy contains 3706 specific rules, and the compact\\ndt-rl consisted of 529 rules with a weighted decision accuracy of 76%.\\nin our empirical experiment, we were able to strictly control\\nthe domain content and thus to isolate the impact of pedagogy from content. based on this isolation, we compared\\nstudents’ performance with the full-rl policy, the dt-rl\\npolicy and the baseline random policy. our results showed\\nthat students in all three conditions learned significantly after training on pyrenees, this suggests that the basic training\\nof the its is effective, even when the pedagogical decisions\\nare made randomly. to evaluate the effectiveness of the two\\nmachine induced policies (full-rl policy and dt-rl policy), we combined the full-rl and dt-rl condition as the\\ninduced group and compared its learning performance with\\nthe random group. our results showed that the induced118\\n\\n\\x0ccond\\npre\\niso post\\nadjusted post\\ntime\\nwe steps\\nps steps\\nwe pct(%)\\n\\ntable 3: full-rl vs. dt-rl\\nfull-rl(45)\\ndt-rl (41)\\nt-test result\\n.683(.205)\\n.690(.184)\\nt(84) = −0.168, p = 0.867, d = 0.036\\n.859(.145)\\n.842(.168)\\nt(84) = 0.505, p = 0.615, d = 0.109\\n.757(.144)\\n.739(.145)\\nt(84) = 0.594, p = 0.554, d = 0.128\\n118.42(35.000) 92.10(27.95)\\nt(84) = 3.829, p = 0.0002, d = 0.827\\n177.44(48.86) 236.80(62.03) t(84) = −4.952, p < 0.0001, d = 1.069\\n201.47(47.22) 143.20(60.57)\\nt(84) = 4.999, p < 0.0001, d = 1.079\\n46.77(12.78)\\n62.26(16.13) t(84) = −4.955, p < 0.0001, d = 1.070\\n\\ngroup significantly outperform the random group. these\\nresults suggest that the machine induced policies are indeed\\nmore effective than the random policy.\\nfinally, in order to examine to what extent the compact dtrl policy retained the power of the full rl-induced policy,\\nwe compared the learning performance of the full-rl and\\nthe dt-rl conditions. our results suggest that while some\\nof the power was lost in the general rules extraction, the relative performance difference between the full-rl and the\\ndt-rl condition is not significant. in addition, our results\\non the pedagogical decisions made in training revealed that\\nthe compact dt-rl policy selected significant more we\\nthan the full-rl policy. this suggests that the two sets\\nof policies indeed made materially different decisions. however, since the weighted dt took account of the importance\\nof each rule, the dt-rl policy aims to retain maximal decision effectiveness from the full-rl policy while the size of\\nthe former is less than 15% of the size of the full-rl rules.\\nin the future, we will apply existing learning theories to the\\ndecision-making process generated by decision tree to find\\na theoretical basis for the dt-induced general pedagogical\\ndecision-making rules.\\n\\n8.\\n\\nacknowledgements\\n\\nthis research was supported by the nsf grant #1432156:\\n“educational data mining for individualized instruction in\\nstem learning environments” and #1651909: “improving\\nadaptive decision making in interactive learning environments”.\\n\\n9.\\n\\nreferences\\n\\n[1] j. r. anderson, a. t. corbett, k. r. koedinger, and\\nr. pelletier. cognitive tutors: lessons learned. the\\njournal of the learning sciences, 4(2):167–207, 1995.\\n[2] j. beck, b. p. woolf, and c. r. beal. advisor: a\\nmachine learning architecture for intelligent tutor\\nconstruction. aaai/iaai, 2000:552–557, 2000.\\n[3] c. boutilier, r. dearden, and m. goldszmidt.\\nstochastic dynamic programming with factored\\nrepresentations. artificial intelligence, 121(1):49–107,\\n2000.\\n[4] m. chi, k. vanlehn, d. litman, and p. jordan.\\nempirically evaluating the application of\\nreinforcement learning to the induction of effective\\nand adaptive pedagogical strategies. user modeling\\nand user-adapted interaction, 21(1-2):137–180, 2011.\\n[5] l. j. cronbach and r. e. snow. aptitudes and\\ninstructional methods: a handbook for research on\\ninteractions. irvington, 1977.\\n\\n[6] a. d. davidson and et al. multiple ecological pathways\\nto extinction in mammals. proceedings of the national\\nacademy of sciences, 106(26):10702–10705, 2009.\\n[7] u. d. gupta, e. talvitie, and m. bowling. policy tree:\\nadaptive representation for policy gradient. in aaai,\\npages 2547–2553, 2015.\\n[8] a. iglesias, p. martı́nez, r. aler, and f. fernández.\\nlearning teaching strategies in an adaptive and\\nintelligent educational system through reinforcement\\nlearning. applied intelligence, 31(1):89–106, 2009.\\n[9] a. iglesias, p. martı́nez, r. aler, and f. fernández.\\nreinforcement learning of pedagogical policies in\\nadaptive and intelligent educational systems.\\nknowledge-based systems, 22(4):266–270, 2009.\\n[10] k. r. koedinger and et al. intelligent tutoring goes to\\nschool in the big city. ijaied, 8(1):30–43, 1997.\\n[11] p. phobun and j. vicheanpanya. adaptive intelligent\\ntutoring systems for e-learning systems.\\nprocedia-social and behavioral sciences,\\n2(2):4064–4069, 2010.\\n[12] j. r. quinlan. induction of decision trees. machine\\nlearning, 1(1):81–106, 1986.\\n[13] s. h. reichard and c. w. hamilton. predicting\\ninvasions of woody plants introduced into north\\namerica. conservation biology, 11(1):193–203, 1997.\\n[14] s. shen and m. chi. aim low: correlation-based\\nfeature selection for model-based reinforcement\\nlearning. edm, 2016.\\n[15] s. shen and m. chi. reinforcement learning: the\\nsooner the better, or the later the better? in umap,\\npages 37–44. acm, 2016.\\n[16] r. s. sutton and a. g. barto. reinforcement learning:\\nan introduction, volume 1. mit press cambridge,\\n1998.\\n[17] k. vanlehn. the behavior of tutoring systems.\\nijaied, 16(3):227–265, 2006.\\n[18] m. p. vayssières, r. e. plant, and b. h. allen-diaz.\\nclassification trees: an alternative non-parametric\\napproach for predicting species distributions. journal\\nof vegetation science, 11(5):679–694, 2000.',\n",
       " '95\\n\\n\\x0caddressing student behavior and affect\\nwith empathy and growth mindset\\nshamya karumbaiah\\n\\nuniversity of massachusetts\\namherst\\n140 governors drive\\namherst, ma 01003-9264\\n\\nshamya@cs.umass.edu\\nbeverly woolf\\nuniversity of massachusetts\\namherst\\n140 governors drive\\namherst, ma 01003-9264\\n\\nbev@cs.umass.edu\\n\\nrafael lizarralde\\n\\ndanielle allessio\\n\\nuniversity of massachusetts\\namherst\\n140 governors drive\\namherst, ma 01003-9264\\n\\nuniversity of massachusetts\\namherst\\n140 governors drive\\namherst, ma 01003-9264\\n\\nworcester polytechnic institute\\n100 institute rd\\nworcester, ma 01609\\n\\nworcester polytechnic institute\\n100 institute rd\\nworcester, ma 01609\\n\\nrezecib@cs.umass.edu\\nivon arroyo\\niarroyo@wpi.edu\\n\\nabstract\\nwe present results of a randomized controlled study that\\ncompared different types of affective messages delivered by\\npedagogical agents. we used animated characters that were\\nempathic and emphasized the malleability of intelligence and\\nthe importance of effort. results showed significant correlations between students who received more empathic messages and those who were more confident, more patient, exhibited higher levels of interest, and valued math knowledge\\nmore. students who received more growth mindset messages, tended to get more problems correct on their first\\nattempt but valued math knowledge less and had lower\\nposttest scores. students who received more success/failure\\nmessages tended to make more mistakes, to be less learningoriented, and stated that they were more confused. we conclude that these affective messages are powerful media to\\ninfluence students’ perceptions of themselves as learners, as\\nwell as their perceptions of the domain being taught. we\\nhave reported significant results that support the use of empathy to improve student affect and attitudes in a math\\ntutor.\\n\\nkeywords\\nstudent affect, empathy messages, growth mindset, pedagogical agents, intelligent tutor, confidence\\n\\n1. introduction\\nstudents experience many emotions while studying and taking tests [16]. students’ emotions (such as confidence, boredom, and anxiety) can influence achievement outcomes [10,\\n18] and predispositions (such as low self-concept and pessimism) can diminish academic success [5, 14].\\n\\nallessio@umass.edu\\nnaomi wixon\\nmwixon@wpi.edu\\n\\npekrun’s control-value theory of emotion has been experimentally validated by classroom experiments that used student self-reports (answers to 5-point scale survey questions).\\nthese experiments provide evidence that educational interventions can reduce students’ anxiety [16, 19].\\ndweck’s growth mindset theory suggests that students who\\nbelieve that intelligence can be increased through effort and\\npersistence tend to seek out academic challenges, compared\\nto those who view their intelligence as immutable [8, 9].\\nstudents who are praised for their effort (as opposed to performance) are more likely to view intelligence as being malleable, and their self-esteem remains stable regardless of how\\nhard they have to work to succeed at a task.\\nhattie and timperley [13] studied which types of feedback\\nand conditions enable learning to flourish and which cases\\nstifle growth. according to their study feedback is intended\\nto help a student get from where they are to where they need\\nto be. graesser et al., [12] reported that there are significant\\nrelationships between the content of feedback dialogue and\\nthe emotions experienced during learning. they found significant correlations between dialog and the affective states\\nof confusion, eureka (delight) and frustration.\\npekrun et al., [17] tested a theoretical model positing that\\na student’s anticipated achievement feedback in a classroom\\nsetting influences his/her achievement goals and emotions.\\nfor example, self-referential feedback, in which a student’s\\ncompetence is defined in terms of self-improvement, had a\\npositive influence on a student’s mastery goal adoption. on\\nthe other hand, normative feedback, in which student competence is defined relative to other students’ mastery goals and\\nperformance goals, had a positive influence on performanceapproach and performance-avoidance goal adoption. furthermore, feedback condition and achievement goals predicted test-related emotions (i.e., enjoyment, hope, pride,\\nrelief, anger, anxiety, hopelessness, and shame).\\nteachers have limited opportunities to recognize and respond to individual student’s affect in typical classrooms.96\\n\\n\\x0cideally, digital learning environments can manage the delicate balance between motivation and cognition, promoting\\nboth interest and deep learning. the overwhelming majority\\nof work on affect-aware virtual tutors has focused on modeling affect, i.e., designing computational models capable of\\ndetecting how students feel while they interact with intelligent tutoring systems [2]. while modeling affect is a critical\\nfirst step, very little research exists on systematically exploring the impact of interventions on students’ performance,\\nlearning, and attitudes, i.e., how an environment might respond to students emotions (e.g., frustration, anxiety, and\\nboredom) as they arise. d’mello and graesser carried out\\nclose research work on empathic characters in autotutor,\\na conversational tutor that uses 3d companions to conduct\\ndialogs in natural language with students [6, 7, 11].\\n\\n1.1\\n\\noriented goals[3]. other results indicate that empathic\\ncharacters can help decrease students’ anxiety and boredom.\\nour results showed that: a) student anxiety and boredom\\ncan be reduced using simple 2d characters, as did d’mello et\\nal., (2007); b) these benefits are due primarily to empathy,\\nand secondarily to growth mindset messages; and c) indicating only success or failure is actively harmful to students,\\nin comparison to emphasizing the learning process and the\\nimportance of effort.\\n\\nmathspring\\n\\nthe testbed for this research is mathspring, an intelligent\\ntutor that personalizes mathematics problems, provides help\\nusing multimedia, and effectively teaches students to improve in standardized test scores [4]. learning companions\\n(figure 1) in mathspring suggest to students that their effort contributes to success, and that making mistakes only\\nmeans more effort is needed. companions use about 20 different messages focused on effort and growth mindset (table 2).\\nto date, mathspring learning companions have provided\\npositive significant effects for the overall population of students and were more effective for lower achieving students\\nand for female students in general [2]. however, characters seemed to have been harmful to some students (e.g.,\\nhigh-achieving males), who had higher affective baselines at\\npretest time and seem to have been distracted by the characters. these results suggest that affective characters should\\nprobably be different for students who are not presently frustrated or anxious (often high achieving males). one possibility is that the behavior of the characters be adaptive to\\nthe affective state of the student.\\n\\n1.2\\n\\nrecognize and respond to affect\\n\\npreviously, we evaluated the hypothesis that tailored affective messages delivered by digital animated characters may positively impact students emotions, attitude, and learning performance. specifically, we identified concrete prescriptive principles about how to respond\\nto student emotion as it occurs during online learning [1, 3].\\nwith models of student emotion, we explored mechanisms to\\naddress negative emotions. our models predict confidence,\\ninterest, frustration, and excitement in real-time, based on\\ndata from hundreds of students. the gold standard was\\nstudents’ self-reported responses to questions, such as “how\\nconfident do you feel right now?”\\nwe found that growth mindset messages based on dweck’s\\ntheory [9] provide an apparent boost in student math\\nlearning [3], resulted in less performance-oriented goals\\n(e.g., beating classmates, instead of a self-referenced focus),\\nand less boredom reported on the posttest. typically\\nonline educational systems only report correctness: “your\\nanswer is correct/incorrect.” we discovered that such success/failure messages are correlated to higher reported anxiety and boredom, and appear to increase performance-\\n\\nfigure 1: learning companions respond to student\\nactions with gestures and messages shown both as\\ntext and audio. above: companion shows high interest while the student views an example problem\\nwith solution steps shown. below: companion provides a growth mindset message, encouraging the\\nstudent to put in effort to become good at math.\\n\\n1.3 research goals\\nthe research questions in this paper focus on identifying\\nmessages that support students’ motivation to persist working on a task. which messages (see table 2) should a tutoring system send to students to encourage them to persist?\\nhow should agents respond to negative emotions? should\\nstudents be praised when they do well? are the benefits to\\nstudent learning and emotion due to empathic or motivational aspects of the companion? what are the results on\\nlearning and emotion of using an empathic or less empathic\\ncompanion in comparison to a companion that indicates only\\nsuccess or failure?97\\n\\n\\x0ctable 1: outcomes variables measured in the experiment. the questions on the pre- and posttest were\\nanswered in a 5-point scale, going from “not at all” to “very much”.\\ninterest - students’ interest in math. “are you interested when solving math problems?”\\nexcitement - how exciting students find math. “do you feel that solving math is exciting?”\\nconfusion - how confused students feel while solving math problems. “do you feel confident that you will\\neventually be able to understand the mathematics material?”\\nfrustration - how frustrating students find math. average of “do you get frustrated when solving math problems?” and “does solving math problems make your feel frustrated?”\\nlearning orientation - how much students focus on learning as opposed to performance. average of “when\\nyou are doing math exercises, is your goal to learn as much as you can?” and “do you prefer learning about things\\nthat make you curious even if that means you have to work harder?”\\nperformance approach goals - “do you want to show that you are better at math than your classmates?”\\nmath value - how important do students think math is. “compared to most other activities, how important is\\nit or you to be good at math?”\\nmath liking - measure of how much students like math. “do you like your math class?”\\nmath test performance - student’s score on math questions that are representative of the content covered in\\nmathspring.\\n\\n2.\\n\\nmethod\\n\\nwe conducted a randomized controlled study to evaluate\\nthree different types of affective messages delivered by pedagogical agents (table 2). the study took place in an urban school district in southern california with sixty-four 6th\\ngrade students in three math classes for four class sessions,\\nduring december 2016. on part of the first and last day,\\nstudents completed a pretest and posttest including questions related to various affective states, and questions about\\nmathematics. outcome variables measured from these questions are provided in table 1.\\nthree conditions of learning companion messages were randomly assigned to students and delivered in both audio and\\nwritten form in order to increase the likelihood of exposure: 1) empathy condition for 24 students, 2) growth\\nmindset condition for 20 students and 3) success/failure\\ncondition for 20 students; see table 2 for examples of the\\ndifferent types of messages. for all conditions, students were\\nasked to self-report their frustration or confidence in a fivepoint scale every five minutes or every eight problems, which\\never came first, but only after a problem was completed.\\nthe prompts were shown on a separate screen and invited\\nstudents to report on their frustration or confidence.\\nthe empathy condition was set to visually reflect positive\\nemotion with a certain probability for each math problem\\nif the last student emotion report had a positive valence.\\nwhen the most recent emotion report had a negative valence, and with a certain probability, the character first visually reflected the negative emotion; then it reported an\\nempathy message such as “sometimes these problems make\\nme feel [frustrated]”, and finally a connector such as “on the\\nother hand”, connected with a growth mindset message such\\nas “i know that putting effort into problem solving and learning from hints will make our intelligence grow.” note that\\nonly students experiencing negative emotions were exposed\\nto growth mindset messages, as opposed to the following\\ncondition.\\nthe growth mindset condition emphasized messages that\\naccentuate the importance of effort and perseverance in achieving success. the growth mindset condition was set to pro-\\n\\nvide one of many growth mindset messages after a second incorrect attempt was made (the first incorrect attempt caused\\nthe hint button to flash), regardless of students’ emotions.\\nthis condition also provided occasional growth mindset messages at the beginning of a new problem.\\nthe success/failure condition provided both traditional\\nsuccess/failure messages and some more basic meta-cognitive\\nsupport for when students made mistakes (e.g., acknowledging that their answer was not correct while encouraging them\\nto use a hint). the success/failure condition provided students with a response if they answered a problem correctly\\nand also after they made a second mistake.\\n\\n3.\\n\\nresults\\n\\nout of the 64, three students’ data were discarded due to\\nminimal interaction with mathspring. across the n =\\n61 students, 21066 event log rows were recorded for three\\nclasses over four separate days, from which several behavioral features were derived and used throughout the analysis;\\nour data and processing scripts can be found on github [15].\\nall the students completed a pretest and posttest. students\\nin empathy, growth mindset and success/failure conditions\\nreceived a total of 978, 763, and 882 messages respectively.\\nmeans, standard deviations and percentage shares for each\\ntype of message are given in table 3. it is important to\\nnote that students received messages from all categories but\\ntheir condition emphasized the corresponding message type.\\nfor example, a student in growth mindset condition received\\nsignificantly more growth mindset messages than a student\\nin empathy condition. this distribution of messages means\\nthat different students saw different amounts of each type\\nof message, which allows us to perform partial correlations\\nwith respect to the counts of each message type, separating\\ntheir effects.98\\n\\n\\x0ctable 2: examples of messages spoken by characters.\\ncondition\\nempathy\\ngrowth\\nmindset\\nsuccess/\\nfailure\\n\\nmessage\\n“don’t you sometimes get frustrated trying to solve math problems? i do. but guess what.\\nkeep in mind that when you are struggling with are new idea or skill you are learning\\nsomething and becoming smarter.”\\n“hey, congratulations! your effort paid off, you got it right!”\\n“did you know that when we practice to learn new math skills our brain grows and gets\\nstronger?”\\n“let’s click on help, and i am sure we will learn something.”\\n“very good, we got another one right!”\\n“hmm. wrong. shall we work it out on paper?”\\n\\nfigure 2: time spent on a problem immediately before and after receiving the different categories of messages.\\n\\n3.1\\n\\npartial correlations\\n\\nfirst, we attempted to replicate the results of our previous\\nexploratory work [3]. for the three message types, partial\\ncorrelations of the total number of each messages were measured for the nine posttest measures, controlling for the corresponding pretest measure, time spent in the tutor, and\\nmessage frequency (total messages heard / time spent).\\ntable 4 shows the result of this analysis. we observe that\\nwith exposure to more empathic messages, students exhibited higher levels of interest and valued math knowledge more (rows 1 and 7). increased interest can be viewed\\nas analogous to the high negative correlation with boredom\\nreported in our earlier work. with growth mindset messages, students valued math knowledge less and had\\nlower post test performance scores (rows 7 and 9).\\nwith success/failure messages, students were less learningoriented and claimed to be more confused (rows 6 and 3).\\n\\nas we see in figure 2, students tend to spend less time\\non problems immediately after they receive growth mindset\\nor success/failure messages. in contrast, the time spent on\\na problem increases slightly after receiving empathic messages. students who received more empathic and growth\\nmindset messages tend to answer fewer questions than do\\nstudents who received mostly success/failure message (figure 3). combined with the last plot, it looks like the students\\nin the empathy condition continue to invest more time on\\nsolving problems than rushing through the problem set.\\n\\nfigure 3: problems seen per minute across different\\npedagogies\\n\\nto further understand the dynamics, we derived some intutor variables and performed partial correlations shown in\\ntable 5. the data for this analysis was derived as per student metrics based on their interaction with mathspring.\\nwe observed that students tend to answer significantly more\\nquestions when in the success/failure condition and end up\\nmaking more mistakes as well (rows 4 and 5). it is important\\nto note that they also avoid asking for hints (row 6). it\\nseems like these students tend to rush through the problems\\nwhile being more careless. they also make more mistakes\\nwhen they receive more growth mindset messages (row 5).\\nthis leads to simpler questions which they tend to get right\\nin the first attempt (row 1). it appears that the students\\nin empathy condition continue to invest more time on\\nsolving problems than rushing through the problem set. the\\nnumber of problems seen by these students is significantly\\nless (row 4).99\\n\\n\\x0ctable 3: the distribution of messages seen by students in each pedagogical conditions.\\ncondition\\nempathy\\ngrowth\\nmindset\\nsuccess/\\nfailure\\n\\nn\\n21\\n\\nempathy messages\\nmean\\nstd\\n%\\n7.48\\n7.0\\n16%\\n\\ngrowth mindset messages\\nmean\\nstd\\n%\\n9.95\\n7.2\\n21%\\n\\nsuccess/failure messages\\nmean\\nstd\\n%\\n29.1\\n22\\n62%\\n\\n20\\n\\n0.2\\n\\n0.5\\n\\n0.5%\\n\\n10\\n\\n5\\n\\n26%\\n\\n27.9\\n\\n19.2\\n\\n73%\\n\\n20\\n\\n1.2\\n\\n1.7\\n\\n2.7%\\n\\n4.6\\n\\n4.8\\n\\n10%\\n\\n38.3\\n\\n26.6\\n\\n86%\\n\\ntable 4: partial correlations between different types of messages seen and posttest variables (table 1),\\naccounting for the corresponding pretest value, time spent in tutor and message frequency.\\nvariable\\n(1)\\n(2)\\n(3)\\n(4)\\n(5)\\n(6)\\n(7)\\n(8)\\n(9)\\n\\ninterest\\nexcitement\\nconfusion\\nfrustration\\nperformance\\napproach\\nlearning\\norientation\\nmath value\\nmath liking\\nperformance\\n\\nempathy messages\\ncorr\\np\\n0.28*\\n0.03\\n0.00\\n1.00\\n-0.05\\n0.74\\n0.10\\n0.43\\n\\ngrowth mindset messages\\ncorr\\np\\n0.19\\n0.15\\n-0.07\\n0.60\\n-0.05\\n0.74\\n-0.08\\n0.54\\n\\nsuccess/failure\\ncorr\\n-0.20\\n-0.08\\n0.32*\\n-0.18\\n\\nmessages\\np\\n0.14\\n0.54\\n0.02\\n0.18\\n\\n-0.19\\n\\n0.14\\n\\n-0.05\\n\\n0.70\\n\\n0.20\\n\\n0.12\\n\\n0.02\\n\\n0.85\\n\\n0.02\\n\\n0.88\\n\\n-0.24+\\n\\n0.06\\n\\n0.09\\n0.96\\n0.07\\n\\n-0.10\\n0.05\\n-0.13\\n\\n0.25*\\n0.01\\n-0.01\\n\\n0.05\\n0.96\\n0.93\\n\\n+\\n\\n-0.22\\n0.01\\n-0.23+\\n\\n+\\n\\n0.45\\n0.72\\n0.33\\np ≤ 0.10, * p ≤ 0.05\\n\\ntable 5: partial correlations between different types of messages seen and within-tutor variables, accounting\\nfor time spent in the tutor and message frequency.\\nvariable\\n(1)\\n(2)\\n(3)\\n(4)\\n(5)\\n(6)\\n\\n% problems solved on\\nfirst attempt\\navg problem\\ndifficulty\\nlearning gain\\nproblems seen\\nmistakes made\\nhints per problem\\n\\nempathy messages\\ncorr\\np\\n\\ngrowth mindset messages\\ncorr\\np\\n\\nsuccess/failure messages\\ncorr\\np\\n\\n0.06\\n\\n0.62\\n\\n0.34**\\n\\n0.007\\n\\n-0.01\\n\\n0.94\\n\\n0.07\\n\\n0.61\\n\\n-0.05\\n\\n0.69\\n\\n0.19\\n\\n0.14\\n\\n-0.10\\n-0.23+\\n-0.01\\n0.10\\n\\n0.50\\n0.07\\n0.91\\n0.43\\n\\n-0.07\\n-0.04\\n0.59**\\n0.16\\n\\n0.63\\n0.78\\n6e-7\\n0.22\\n+-0.14\\n0.34\\n0.77**\\n4e-13\\n0.30*\\n0.02\\n0.10\\n-0.22+\\np ≤ 0.10, * p ≤ 0.05, ** p ≤ 0.01\\n\\n100\\n\\n\\x0c3.2\\n\\nmarkov chain analysis\\n\\nas students solve problems in the tutoring system, the learning companion comments on their attempts; the effect of\\nthese messages on student affect is sequential, but the partial correlations do not capture this. to analyze this effect,\\nwe built markov chain models using in-tutor student selfreports of confidence and frustration. each model describes\\ntransitions in affective states, from one self-report to the\\nnext, where students received a particular type of character messages (empathy, growth mindset, and success/failure)\\nbetween self-reports. to reduce the state space, the 5-point\\nscale used in the self-reports was simplified to two values confident (≥ 3), not confident (< 3); similarly for frustration.\\nthe goal of the markov models was not to predict emotional\\nchanges, but rather to examine whether different messages\\nhad significant effects on affect. markov models can show\\nthe probability of transitioning between affective states, but\\nalso have a stationary distribution, which represents the\\namount of students that would be in each state after undergoing many transitions. for example, a group of students were to use the system for many hours and receive\\nonly empathic messages, our model suggests that 99.5% of\\nthem would be confident about learning math (figure 4).\\n\\nfigure 4: state transitions between the confident\\n(c) and not confident (n) affective states. the stationary distribution is shown below each state. only\\nthe empathy model was significant in the likelihood\\nratio test (p ≤ 0.05)\\nafter\\nempathic\\nmessage\\n\\ntable 6: stationary distributions in the markov\\nmodels of confidence and frustration.\\nmessage\\ntype\\nempathy\\ngrowth\\nmindset\\nsuccess/\\nfailure\\n\\nconfidence\\nconf\\nnot\\n99.5%* 0.05%*\\n\\nfrustration\\nfrust\\nnot\\n35%\\n65%\\n\\n74%\\n\\n26%\\n\\n30%\\n\\n70%\\n\\n80%\\n\\n20%\\n\\n25%\\n\\n75%\\n*p ≤ 0.05\\n\\n4. discussion\\nsome of our results support the hypothesis that affective\\nmessages delivered by characters can positively impact students’ emotions and affective predispositions for math problem solving. this is particularly evident for empathy, as\\nthe more empathic messages a student saw the higher their\\ninterest in mathematics problem solving, as well as their beliefs that mathematics is valuable to learn (table 4). an\\nanalysis of student behavior suggests that students who saw\\na high frequency of empathic messages also tended to be\\nmore patient and cautious with problem solving, suggesting\\nthat empathic messages may encourage students to persist\\nthrough adversity. exposure to empathic messages was significantly correlated to investing time in each math problem activity, leading also to fewer problems seen per session. a positive trend is exhibited between high frequency\\nof empathic messages and hints requested, even if not significant (table 5). empirical temporal models generated from\\nstudents’ changes in self-reports of affect, within the tutor,\\nrevealed that students receiving empathic messages have a\\nhigher likelihood to become more confident and to remain\\nconfident.\\n\\n0.01\\n0.99\\n\\nc\\n99.5%\\n\\nn\\n0.50\\n\\n0.50\\n\\n0.5%\\n\\n0.11\\n\\nafter growth\\nmindset\\nmessage\\n\\n0.89\\n\\nafter\\nsuccess/\\nfailure\\nmessage\\n\\n0.90\\n\\nc\\n73%\\n\\nn\\n0.31\\n\\n0.69\\n\\n27%\\n\\n0.10\\nc\\n80%\\n\\nn\\n0.39\\n\\n0.61\\n\\n20%\\n\\nwe used a likelihood ratio test to analyze the significance\\nof these models: the probability of the null model (ignoring\\nmessage type) generating the data divided by the probability\\nof the alternate model (for a particular message type) generating the data gives a p-value. figure 4 shows the state transitions for confidence in the null model and the model for\\nconfidence after receiving empathic messages, which was\\nsignificant with p = 0.0149 (the other models were not significant). we also examined the stationary distributions for\\neach model (table 6).\\n\\nthe response to growth mindset messages delivered by characters yielded mixed results. as students saw more of these\\nkinds of messages they also succeeded more often at solving\\nproblems correctly (on the first attempt); interestingly, at\\nthe same time, they also made more mistakes. this is also\\ndesirable, as growth mindset messages emphasize that making mistakes is okay and can even help learning, legitimizing\\na high frequency of errors. it is possible that students were\\nusing those mistakes and hints to learn and succeed later on;\\na (not significant) positive trend suggests that students receiving more of these kinds of messages also asked for more\\nhints per problem. in contrast, marginally significant effects\\nsuggest that a high frequency of growth mindset messages\\nmight be detrimental to students’ perception of math value,\\nand that their posttest performance is worse when they receive more of this kind of messages. it is hard to conclude\\nthe meaning of these marginally significant effects, especially\\nbecause a previous study suggested that these messages were\\nbeneficial in general [3]. note that empathic messages used\\n’growth mindset’ messages also, in order to resolve the negative emotion (see table 2). one possible explanation is that\\nthe empathic condition was so positive because it was also\\nvery selective at showing growth mindset messages for only\\nthose who experienced negative emotions. it is likely that\\nhigh achieving students, or those who “felt ok”, rejected\\ngrowth mindset messages that they might have perceived to\\nbe unnecessary.101\\n\\n\\x0can important comment is that we did not expect that success/failure messages could be so harmful to students. regardless of whether messages indicated success or failure, as\\nstudents received more of these messages they also exhibited\\nlower levels of mastery/learning orientation at posttest time.\\nthey also reported higher levels of confusion at posttest time\\n(note that the confusion can be positive for learning within\\nthe learning experience, but not after the learning experience has concluded). regarding behavior within the tutor,\\nthe more students were exposed to success/failure messages,\\nthe more they appeared to rush through problems, make\\nmistakes, and request fewer hints per problem.\\nto summarize, empathy messages were associated with variables consistent with methodical work and an increased interest/value of mathematics. however, both growth mindset\\nand success/failure messages appeared to be associated with\\na greater number of mistakes. finally, success/failure messages themselves were associated with a whole host of concerning behaviors such as confusion with the material following posttest, reduced learning orientation, hurried work, and\\na reduced likelihood of requesting hints. this is consistent\\nwith dweck’s findings that growth mindset messages are superior to success/failure messages [8, 9]. whether empathic\\nmessages in fact result in improved student performance pre\\nto posttest will likely require larger samples than this small\\nstudy (n = 61). however, students in non-empathic conditions have demonstrated significantly more mistakes in their\\nwork.\\n\\n5.\\n\\nconclusions\\n\\nthis research emphasizes the importance of understanding\\nan intervention’s effect on a student’s affective state, which\\nin turn is connected to engagement, performance, and learning. although many researchers have focused on modeling\\naffect, very little research effort has been put into systematically measuring the impact of the intervention on the student behavior in an adaptive learning environment. empathic messages that respond to students’ recent emotions\\nhave resulted in superior results both in improving the student interaction with the system and in the overall learning\\nexperience. growth mindset follows next with some positive impact on in-tutor performance but its overall effect\\nin the short-term is questionable. success/failure messages\\nare generally harmful to students: reducing learning orientation, increasing confusion, and making students more\\ncareless during the learning experience.\\nwe conclude that affective messages delivered by characters in online tutoring environments are a very important\\nmedium for building student-tutor rapport in a virtual environment, powerful signals that influence perceptions of students themselves as learners, as well as perceptions of the\\ndomain being taught. we have reported significant results\\nthat support the use of empathy to improve student affect\\nand attitudes in a math tutor. the long-term effect of these\\nmessages needs to be studied when the novelty of this intervention wears off. in the future, we hope to study the\\nimpact of the frequency and content of these messages. to\\nevaluate the generalizability of these results, student populations across different demographics needs to be studied as\\nwell as the applicability of the messages to domains beyond\\nmathematics.\\n\\n6.\\n\\nacknowledgments\\n\\nthis research is supported by the national science foundation (nsf) 1324385 iis/cyberlearning dip: collaborative\\nresearch: impact of adaptive interventions on student affect, performance, and learning. any opinions, findings,\\nand conclusions, or recommendations expressed in this paper are those of the authors and do not necessarily reflect\\nthe views of nsf.\\n\\n7.\\n\\nadditional authors\\n\\nadditional authors: winslow burleson (new york university, 70 washington square south new york, new york,\\n10012; email: wb50@nyu.edu).\\n\\n8.\\n\\nreferences\\n\\n[1] i. arroyo, w. burleson, m. tai, k. muldner, and b. p.\\nwoolf. gender differences in the use and benefit of\\nadvanced learning technologies for mathematics.\\njournal of educational psychology, 105(4):957, 2013.\\n[2] i. arroyo, d. g. cooper, w. burleson, b. p. woolf,\\nk. muldner, and r. christopherson. emotion sensors\\ngo to school. in aied, volume 200, pages 17–24, 2009.\\n[3] i. arroyo, s. schultz, n. wixon, k. muldner,\\nw. burleson, and b. p. woolf. addressing affective\\nstates with empathy and growth mindset. 6th\\ninternational workshop on personalization\\napproaches in learning environments, 2016.\\n[4] i. arroyo, b. p. woolf, w. burelson, k. muldner,\\nd. rai, and m. tai. a multimedia adaptive tutoring\\nsystem for mathematics that addresses cognition,\\nmetacognition and affect. international journal of\\nartificial intelligence in education, 24(4):387–426,\\n2014.\\n[5] l. corno and r. e. snow. adapting teaching to\\nindividual differences among learners. handbook of\\nresearch on teaching, 3(605-629), 1986.\\n[6] s. d’mello and a. graesser. automatic detection of\\nlearner’s affect from gross body language. applied\\nartificial intelligence, 23(2):123–150, 2009.\\n[7] s. d’mello and a. graesser. autotutor and affective\\nautotutor: learning by talking with cognitively and\\nemotionally intelligent computers that talk back.\\nacm transactions on interactive intelligent systems\\n(tiis), 2(4):23, 2012.\\n[8] c. s. dweck. self-theories: their role in motivation,\\npersonality, and development. psychology press, 2000.\\n[9] c. s. dweck. beliefs that make smart people dumb.\\nwhy smart people can be so stupid, 24:41, 2002.\\n[10] d. goleman. emotional intelligence. why it can\\nmatter more than fq. learning, 24(6):49–50, 1996.\\n[11] a. c. graesser, p. chipman, b. c. haynes, and\\na. olney. autotutor: an intelligent tutoring system\\nwith mixed-initiative dialogue. ieee transactions on\\neducation, 48(4):612–618, 2005.\\n[12] a. c. graesser, s. k. d’mello, s. d. craig,\\na. witherspoon, j. sullins, b. mcdaniel, and\\nb. gholson. the relationship between affective states\\nand dialog patterns during interactions with\\nautotutor. journal of interactive learning research,\\n19(2):293, 2008.\\n[13] j. hattie and h. timperley. the power of feedback.\\nreview of educational research, 77(1):81–112, 2007.102\\n\\n\\x0c[14] a. n. kluger and a. denisi. feedback interventions:\\ntoward the understanding of a double-edged sword.\\ncurrent directions in psychological science, 7(3):67–72,\\n1998.\\n[15] r. lizarralde and s. karumbaiah. a collection of\\nscripts for processing mathspring data. https:\\n//github.com/rezecib/mathspringdataprocessing,\\n2017.\\n[16] r. pekrun. emotions and learning. international\\nacademy of education. australia: international\\nbureau of education, 2014.\\n[17] r. pekrun, a. cusack, k. murayama, a. j. elliot, and\\nk. thomas. the power of anticipated feedback:\\neffects on students’ achievement goals and\\nachievement emotions. learning and instruction,\\n29:115–124, 2014.\\n[18] r. pekrun, t. goetz, l. m. daniels, r. h. stupnisky,\\nand r. p. perry. boredom in achievement settings:\\nexploring control-value antecedents and performance\\noutcomes of a neglected emotion. journal of\\neducational psychology, 102(3):531, 2010.\\n[19] r. pekrun, e. vogl, k. r. muis, and g. m. sinatra.\\nmeasuring emotions during epistemic activities: the\\nepistemically-related emotion scales. cognition and\\nemotion, pages 1–9, 2016.',\n",
       " \"55\\n\\n\\x0ctoward the automatic labeling of course questions for\\nensuring their alignment with learning outcomes\\ns. supraja\\n\\nkevin hartman\\n\\nsivanagaraja tatinati\\n\\nandy w. h. khong\\n\\nnanyang technological\\nuniversity\\n50 nanyang ave\\nsingapore 639798\\nssupraja001@e.ntu.edu.sg\\n\\nnanyang technological\\nuniversity\\n50 nanyang ave\\nsingapore 639798\\nkhartman@ntu.edu.sg\\n\\nnanyang technological\\nuniversity\\n50 nanyang ave\\nsingapore 639798\\ntatinati@ntu.edu.sg\\n\\nnanyang technological\\nuniversity\\n50 nanyang ave\\nsingapore 639798\\nandykhong@ntu.edu.sg\\n\\nabstract\\nexpertise in a domain of knowledge is characterized by a greater\\nfluency for solving problems within that domain and a greater\\nfacility for transferring the structure of that knowledge to other\\ndomains. deliberate practice and the feedback that takes place\\nduring practice activities serve as gateways for developing domain\\nexpertise. however, there is a difficulty in consistently aligning\\nfeedback about a learner’s practice performance with the intended\\nlearning outcomes of those activities – especially in situations\\nwhere the person providing feedback is unfamiliar with the\\nintention of those activities. to address this problem, we propose\\nan intelligent model to automatically label opportunities for\\npractice (assessment questions) according to the learning outcomes\\nintended by the course designers. as a proof of concept, we used a\\nreduced version of bloom’s taxonomy to define the intended\\nlearning outcomes. using a factorial design, we employed term\\nfrequency-inverse document frequency (tf-idf) and latent\\ndirichlet allocation (lda) to transform questions from text to word\\nweightages with support vector machine (svm) and extreme\\nlearning machine (elm) to train and automatically label the\\nquestions. we trained our models with 120 questions labeled by the\\nsubject matter expert of an undergraduate engineering course.\\ncompared to existing works which create models based on a selfgenerated dataset, our proposed approach uses 30 untrained\\nquestions from online/textbook sources to validate the performance\\nof our models. exhaustive comparison analysis of the testing set\\nshowed that tf-idf with elm outperformed the other\\ncombinations by yielding 0.86 reliability (f1 measure) with the\\nsubject matter expert.\\n\\nkeywords\\nlearning outcomes, term frequency-inverse document frequency,\\nlatent dirichlet allocation, extreme learning machine, support\\nvector machine\\n\\n1. introduction\\nincreasingly, modern curriculum design in tertiary and adult\\nlearning settings has become a collaborative endeavor between\\nsubject matter experts, learning designers, and learning\\ntechnologists. while these teams employ a variety of process\\n\\nmodels for the planning, execution, and revision of their curriculum\\nand activity designs, often greater attention is paid to the\\nconstruction of a course design and the course content rather than\\nthe assessment practices that measure learning and their ongoing\\nmaintenance.\\nthe algorithms and use case described in this paper exist in a\\nparticular context of outcome-based education. in this context,\\nlearning is defined by observable changes in a learner’s behavior.\\nthese changes commensurate with krathwohl’s model of learning\\nobjectives [1] but learning outcomes go beyond objectives.\\nlearning outcomes are predicated on having learners observably\\ndemonstrate their growing understanding of a topic or proficiency\\nwithin a field [2]. when learning activities become more openended and exploratory, and when learners are offered choices for\\nhow to proceed, learners often look to how they will ultimately be\\nassessed to gauge which learning strategies they should employ [3].\\nwhen a course’s learning activities support its assessment practices\\nand the assessment practices support the types of outcomes that are\\nrelevant to learners in the future, the course’s activities and\\nintended learning outcomes exhibit constructive alignment with\\neach other [2]. adhering to constructive alignment creates a\\nseamless path from learning, to applying, to transferring concepts\\nand relationships when solving novel problems.\\nhowever, the promise of constructive alignment is not easily\\ndelivered upon. oftentimes, a course’s learning outcomes cannot\\nbe measured by its assessment practices, or its assessment practices\\nare decontextualized from the types of activities and practices\\nlearners are actually preparing for [4]. whether in the context of\\nhigher learning or professional development, when thinking about\\ndeveloping flexible, life-long learners it is paramount to have\\nmechanisms in place to support learners as they work to gain\\ndomain expertise. these processes should reliably measure\\nlearning and link assessment practices to authentic activities.\\n\\n1.1 learning design for domain expertise\\nprior work in designing for adaptive domain expertise, the kind of\\nexpertise necessary for learners to function in changing\\nenvironments and flexible job scopes, has shown that learning\\ndesign teams need to be cognizant of three elements which will be\\ndiscussed in turn.\\n\\n1.1.1 levels of learning outcomes\\nlearning outcomes range in sophistication and vary by field. in\\nmedicine, miller’s pyramid [5] lists learning outcomes beginning\\nwith knowing about a subject, progressing to knowing how to do\\nsomething, to being able to actually demonstrate it in a contrived\\nsetting like a role-play with actors, and to being able to demonstrate\\nit in a real environment like a surgical theater [6]. the idea is based\\non the belief that the development of expertise is a progression from56\\n\\n\\x0cthe recall of facts to the execution of skills. however, as research\\non problem based learning has shown, demonstration of skill and\\nthe recall of facts can proceed independently of each other\\ndepending on the learning environment [7].\\nin [8], a field agnostic method of classifying learning outcomes\\nbased on their quality is presented. essentially, the structure of\\nobserved learning outcomes (solo) taxonomy identifies the\\nlevel of cognitive sophistication a learning outcome requires.\\nlower level learning outcomes indicate a learner is capable of\\nremembering facts in isolation. more sophisticated levels require\\nlearners to assimilate information from various sources to make\\nconnections and transform that understanding into something new.\\nperhaps the most popular listing of learning outcomes is bloom’s\\ntaxonomy. similar to miller’s pyramid, bloom’s revised\\ntaxonomy also begins with the retrieval of facts and information\\nas its foundation and builds up to application of knowledge and\\nfurther to analyzing, evaluating, and creating. because of its\\nsimplicity and familiarity with learning designers and subject\\nmatter experts alike, bloom’s taxonomy can easily be used to\\nidentify the levels of learning outcomes in a course [9].\\n\\n1.1.2 opportunities for deliberate practice\\nalong with identifying a learning activity’s intended outcomes,\\nexpertise development requires opportunities for deliberate\\npractice. in contrast to repetitive practice intended for learners to\\ndevelop automaticity in either the recall of information or the\\napplication of a skill, often during time-limited tasks, deliberate\\npractice focuses on mastering the nuances of the domain itself to\\nfine-tune performance [10]. in fact, a learner’s level of grit, a\\ncombination of perseverance and passion, predicts how close to\\nexpert performance a learner will eventually show [11].\\nthe key difference in processes between repetitive practice and\\ndeliberate practice leads to different forms of expertise: adaptive\\nand routine [12]. routine forms of expertise allow a learner to\\nconduct a task at an optimal level. adaptive expertise allows\\nlearners to learn new tasks or solve novel problems at an\\naccelerated rate. in an industrial setting, routine expertise helps a\\nworker complete a particular job function. adaptive expertise\\nenables that same worker to retrain to fill new job functions.\\ntypically, the amount of time necessary to achieve expert\\nperformance in a domain is in the order of years to decades [13].\\nhowever, incremental improvement can be seen in a few practice\\ncycles when activities align to the intended learning outcomes.\\n\\n1.1.3 formative assessments and actionable\\nfeedback\\nhand in hand with creating opportunities for deliberate practice is\\nproviding formative feedback to the learner about how to improve\\nthat practice while that improvement is still relevant. imagine\\nstudents who diligently answer every question in an engineering\\ntextbook but never receive feedback on the quality of their\\nsolutions. in this case, the learners would be unable to gauge their\\nperformance in relation to the course learning outcomes or have an\\nidea about how to improve their performance in the future. now\\nimagine if those same students do receive feedback, but that\\nfeedback arrives after the course’s final examination. if the content\\nof the course is mostly self-contained and will not be revisited, the\\nfeedback is mostly irrelevant.\\nformative feedback consists of two parts: 1) an interpretable\\nindication of a learner’s performance on an assessment of learning\\nwith respect to a standard of performance (learning outcome) and\\n\\n2) the opportunity to improve performance before the final\\nevaluation [14].\\ncognitive tutors provide a clear example of the power of coupling\\nformative assessment and actionable feedback together in the\\ndomain of mathematics learning [15]. by presenting learners with\\na series of structured problems, cognitive tutors are capable of\\nintervening at any point during the problem-solving process to\\nprovide students with feedback about their performance. this\\nfeedback may be the identification of an error, the presentation of\\na hint, or the request for more information about the learner’s\\nreasoning. after the feedback, learners have the opportunity to\\nadjust their problem-solving heuristics to improve their\\nperformance going forward.\\nsuch an interaction sequence works with highly structured tasks\\nwith application-oriented learning outcomes. however, the\\nfeedback cycle is more difficult to manage when the learning\\noutcomes are aligned to higher-order reasoning like evaluation,\\nanalyzing and creating. these outcomes have multiple paths for\\nreaching a satisfactory answer.\\nwith this difficulty in mind, we looked at techniques to automate\\nthe process of identifying the reasoning level of text-based\\nassessment items (questions) with the intention of better aligning\\nquestions to learning outcomes as a first step toward being able to\\nprovide opportunities for deliberate practice. subsequently, the\\noutcome of our proposed work is to link actionable feedback to a\\nlearner’s performance on assessment items.\\n\\n1.2 automated question classification\\ntechniques\\nprior work has shown the viability of automatically labeling\\nquestions in accordance with a course’s learning outcomes.\\nhowever, our work goes beyond labeling existing content to\\nhelping course instructors promote deliberate practice and expertise\\ndevelopment by providing a method of finding new questions that\\nalign to the course designer’s original intended learning outcomes.\\nwe highlight the drawbacks of prior work and how our proposed\\napproach addresses those limitations.\\n\\n1.2.1 labeling questions based on difficulty level\\nearly attempts at automatically labeling questions relied on subject\\nmatter experts to pre-define the difficulty levels of questions.\\nartificial neural network trained by backpropagation then used the\\nquestion features and assigned difficulty levels in the training set to\\nclassify new questions. a five-dimensional feature vector that\\nconsisted of query-text relevance, mean term frequency, length of\\nquestions and answers, term frequency distribution (variance),\\ndistribution of questions and answers in a text were used. the\\nmethod yielded an f1 measure, a classification reliability metric\\nthat measures a test’s accuracy, of 0.78 [16]. however, a major\\npitfall this method is its lack of semantic analysis.\\nentropy-based decision tree has also been used to label questions\\n[17]. the weakness in this strategy is that there is high possibility\\nof overfitting the model during the training phase that then\\nnegatively affects the subsequent prediction performance.\\n\\n1.2.2 labeling questions based on bloom’s\\ntaxonomy using natural language processing\\nnatural language processing (nlp) has been used for the\\ngeneration of assessments, answering questions, supporting users\\nin learning management systems and preparing course materials.\\nthe wordnet package has been used to detect semantic similarity.\\nby performing a rule-based approach, the accuracy of labeling a57\\n\\n\\x0cquestion based on bloom’s taxonomy reaches 82% [18]. to\\nimprove the rule-based approach, a hybrid technique of using an ngram classifier with a rule-based approach has also been explored.\\nrules were based on combining parts-of-speech tagging, and the\\nn-gram classifier found the probabilities of predicting certain\\nwords. such a hybrid method yielded an f1 measure of 0.86 [19].\\n\\nunderstanding) were collapsed into remember. applying\\nremained its own category. all of the higher-order reasoning\\ncategories (analyzing, evaluating, and creating) were collapsed\\ninto transfer. figure 1 shows how our labeling scheme categories\\nmap onto the original categories from bloom’s revised taxonomy.\\n\\n1.2.3 labeling questions based on bloom’s\\ntaxonomy using machine learning techniques\\nmachine learning algorithms can be broadly split into either\\nsupervised or unsupervised training implementations. generally,\\nsupervised training is adopted when, during training, labels have\\nbeen pre-determined and questions are labeled by an expert. the\\nmost commonly used method in such cases is the term frequencyinverse document frequency (tf-idf). the algorithm assigns\\nweightages to individual words in a question statement to define a\\ncustom vector space to each question.\\nmachine learning techniques such k-nearest neighbors, naïve\\nbayes and support vector machine (svm) have been implemented\\nfor labeling questions. when doing a performance comparison\\namong these three techniques, an f1 measure of 0.71 was achieved\\nusing svm [20]. to increase the accuracy level, additional features\\nwere incorporated in future versions of the work. three different\\nfeature selection processes, namely: odd ratio, chi-square statistic\\nand mutual information were used with the three machine learning\\ntechniques. the f1 measure result reached 0.9 [21].\\nfurthermore, an integrated approach of feature extraction has been\\nproposed by using headword, semantic, keyword and syntactic\\nextractions, which are fed into svm [22]. however, this work has\\nnot yet been completed by using a testing dataset to quantify the\\nreliability of prediction.\\na major downside in existing works is that both the training as well\\nas testing questions are part of the same course curriculum; the\\nquestions are generated by the same author/instructor. even when\\na high f1 measure is achieved, it does not enable the algorithm to\\nlabel questions written by another subject matter expert. our work\\nincreases the flexibility of labeling methods by testing our models\\nwith a new set of questions compiled from textbook and online\\nresources.\\nin addition, our work introduces extreme learning machine (elm),\\nwhich has been shown to outperform svm during similar labeling\\ntasks [23]. moreover, we introduce lda as an alternative technique\\nto tf-idf for transforming question statements into numerical\\nword weightages.\\nby comparing combinations of these new techniques with more\\ntraditional techniques, we aim to gauge which combination attains\\nthe highest labeling reliability with the subject matter expert when\\nautomatically labeling untrained questions. for our purposes, using\\nthe combination with the highest f1 measure (fewest false\\nnegatives and false positives) becomes paramount. in our use case,\\na mislabeling by the algorithm will lead to the wrong set of practice\\nquestions to be given to students and diminish the impact of\\ndeliberate practice on reaching the intended learning outcomes.\\n\\n2. methods\\n2.1 materials\\n2.1.1 labeling scheme\\nthe core of this study centers on a labeling scheme for identifying\\nthe sophistication of learning outcomes based on a simplified\\nversion of bloom’s taxonomy. in this labeling scheme, the first\\ntwo levels of bloom’s taxonomy (remembering and\\n\\nfigure 1: mapping of bloom's revised taxonomy [24]\\nwe collapsed the taxonomy into three categories for two reasons.\\nfirst, the subject matter expert tasked with labeling the questions\\nwas unsure about how reliably the questions could be labeled by\\nsomeone without a background in learning design, educational\\npsychology, or curriculum development. collapsing the categories\\nto remember, apply, and transfer made manually labeling\\nhundreds of questions to train the machine learning algorithms\\nmore tractable. second, collapsing the categories had the effect of\\nmaking bloom’s taxonomy more analogous to the successful use\\ncases of miller’s pyramid by subject matter experts in both higher\\neducation and professional development settings [5].\\n\\n2.1.2 question dataset\\nthe dataset consists of a total of 150 questions used for training and\\ntesting the machine learning algorithms based on the content of an\\nundergraduate electrical and electronic engineering course.\\nfor this study, we formed a training set of 120 questions by\\nrandomly selecting 40 remember, apply, and transfer items from\\nthe larger question pool of more than 200 questions used in that\\ncourse. the pool came from a repository of four years’ worth of\\nassignment, homework, quiz and exam questions presented to\\nstudents. these questions prompt students for a range of answer\\ntypes (i.e., open-ended, multiple-choice, short-structured, essay).\\nwe then created a testing set of 30 new questions compiled from\\nexternal sources such as textbooks and online question banks. this\\nset was also balanced with equal representation of remember,\\napply, and transfer questions.\\n\\n2.2 data pre-processing procedures\\nwe pre-processed the raw questions in two phases. first, the subject\\nmatter expert labeled every question according to the labeling\\nscheme described above. second, we transformed the text of every\\nquestion into a machine-readable format before passing them\\nthrough the machine learning algorithms.\\n\\n2.2.1 subject matter expert pre-processing\\nthe subject matter expert manually labeled each question in the\\ntraining set based on its intended learning outcome (remember,\\napply or transfer). the subject matter expert then labeled the 30\\nnew questions in the testing set in the same manner. these new\\nquestions are labeled for the purpose of knowing the ground truth\\nfor performance evaluation. table 1 below shows some examples\\nof the labeled questions.58\\n\\n\\x0ctable 1 - examples of labeled questions\\nremember\\nconsider a signal described by y[n] = 2n +4. what would be the\\namplitude of the signal at sample index n=3?\\napply\\nconsider the following input and output signals: find the transfer\\nfunction and state the poles and zeros of this transfer function.\\ntransfer\\ndescribe how the bandpass filter can be utilized for radar\\napplications.\\n\\n2.2.2 text pre-processing\\nthe text transformation began by excising all equations,\\nmathematical symbols and diagrams from the questions. we only\\nkept the core of the question prompts by removing the descriptive\\nand explanatory text from scenario and hypothetical questions. for\\nexample, if a question began by setting the stage with “peter has\\nbeen asked to perform…”, followed by the question prompt “how\\nmuch voltage should peter expect in the circuit?”, all of the\\ndescriptive text prior to the question prompt was removed to\\nimprove the consistency of word length and usage between items.\\nfor the remaining words in the questions, we changed all of the\\ncharacters to lower case, removed all punctuation marks, numbers,\\nand non-unicode characters. we then stemmed the remaining\\nwords to obtain a list of root words. from this list of root words, we\\nremoved all words with fewer than three letters. because we were\\nunsure of the relationship between the words and the labels, we did\\nnot create a list of stopwords for removal.\\n\\n3. techniques\\nwe tested four combinations (in no particular order) of word\\nweighting and question labeling algorithms, as shown in figure 2,\\nto identify the techniques with the highest reliability for our\\nautomated learning outcome labeler.\\n\\nwe implemented a modified version of tf-idf that used individual\\nquestions as the source of the analysis instead of complete\\ndocuments. this focused the model on finding the relevance of each\\nword within each single question. by converting each question into\\na vector of weightages based on word frequencies, the machine\\nlearning algorithms were then used to label the questions. the\\nmodified tf-idf model can be described by\\n𝑇𝐹 − 𝐼𝐷𝐹(𝑤𝑖 , 𝑞𝑘 ) = #(𝑤𝑖 , 𝑞𝑘 ) × log\\n\\n𝑇𝑅\\n#𝑇𝑅(𝑤𝑖 )\\n\\n(1)\\n\\nwhere wi refers to a particular word i, qk refers to a particular\\nquestion k, #(wi,qk) refers to number of times wi occurs in qk, tr\\nrefers to total number of questions and #tr(wi) refers to question\\nfrequency, or the number of questions in which wi occurs [20].\\nin the case where the term frequency (tf) count is biased towards\\nlonger questions, the tf count is normalized as\\n𝑇𝐹𝑖,𝑘 =\\n\\n𝑛𝑖,𝑘\\n\\n(2)\\n\\n∑𝑗 𝑛𝑗,𝑘\\n\\nwhere ni,k refers to the number of times wi occurs in qk, the\\ndenominator term (size of each question) refers to the sum of the\\nnumber of times each word appears in qk [25].\\nfor our work, the pre-processing procedures registered a total of\\n465 unique stemmed words in our compilation of 120 training\\nquestions and 30 testing questions. this led to each question being\\nrepresented as a vector of 1 row and 465 columns arranged in\\nalphabetical order by stemmed word. when a word is present in a\\nquestion, the normalized weight of that word is assigned to that\\nquestion’s vector element. if a word is not present in the question,\\nthe weight is zero.\\nafter determining the unique word weightage vectors for all 150\\nquestions, the entire matrix is sorted such that for each question, the\\nweightages are arranged in ascending order. the top ten weightages\\nare chosen for each question. the 10 weightages may correspond\\nto different words in each question, but their combinations remain\\nquestion-specific and give a numerical representation of each\\nquestion statement. this new vector of 10 columns per question\\nserves as the input to the machine learning algorithms.\\nas an example, we will use the pre-processed question prompt:\\nfor signal which begin when the one side unilateral ztransform given\\n\\ntable 2 below shows the weightages assigned to the above example\\nafter the application of the tf-idf technique. the weightages are\\nthen arranged in ascending order and the top 10 values are taken.\\ntable 2 - tf-idf weightage arrangement\\nfigure 2: four combinations of algorithms\\nevery word in each question prompt was assigned a weightage\\nvalue based on either term frequency-inverse document frequency\\n(tf-idf) or latent dirichlet allocation (lda). subsequently, the\\nvector values for each question were passed through either support\\nvector machine (svm) or extreme learning machine (elm) to\\nassign a label. all algorithms were implemented in r studio.\\n\\n3.1 term frequency-inverse document\\nfrequency\\nterm frequency-inverse document frequency (tf-idf) is a\\ntechnique for finding the relative frequency of words in a given\\ndocument, and comparing those frequencies with the inverse of\\nhow often each of those words appear in the complete document\\ncorpus. the resulting ratio can be used to signify the relevance of\\neach unique word within a single document.\\n\\nword (alphabetical order)\\n\\nweightage\\n\\nbegin\\n\\n0.392\\n\\nfor\\n\\n0.140\\n\\ngiven\\n\\n0.140\\n\\none\\n\\n0.222\\n\\nside\\n\\n0.356\\n\\nsignal\\n\\n0.116\\n\\nthe\\n\\n0.007\\n\\nunilateral\\n\\n0.392\\n\\nwhen\\n\\n0.279\\n\\nwhich\\n\\n0.230\\n\\nztransform\\n\\n0.21659\\n\\n\\x0c3.2 latent dirichlet allocation\\nlatent dirichlet allocation (lda) is a probabilistic technique for\\ntopic modeling based on the bayesian model. the essential idea of\\nlda is that each document consists of a mixture of topics, with the\\ncontinuous-valued mixture properties distributed in a dirichlet\\nrandom variable, a continuous multivariate probability distribution.\\nagain, in the context of our work, we applied lda to questions in\\nthe dataset by substituting the original notion of documents in the\\nlda algorithm with questions in our modified model. therefore,\\nthe modified model attempted to find k number of topics (k is a\\nuser-defined parameter to determine the desired number of topics,\\nor dimensionality of the dirichlet distribution) for a given set of\\nquestion statements based on the choice and usage of words in each\\nquestion. the joint distribution of a topic mixture, a set of topics\\nand a set of words can be represented by\\n𝑝(𝜃, 𝑡, 𝑤|𝛼, 𝛽) = 𝑝(𝜃|𝛼) ∏𝑀\\n𝑖=1 𝑝(𝑡𝑖 |𝜃)𝑝(𝑤𝑖 |𝑡𝑖 , 𝛽)\\n\\n(3)\\n\\nwhere parameter α is a k-vector with components more than zero,\\nparameter β refers to the matrix of word probabilities, θ refers to a\\nk-dimensional dirichlet random variable, ti refers to a topic, wi\\nrefers to a word [26].\\nfigure 3 shows a graphical model representation of lda. the\\nbigger circle refers to questions while the smaller circle refers to\\nthe repeated choice of topics and words within each question.\\n\\nout of the entire set of stemmed words detected, ten words have\\nbeen identified as topic names. hence, lda automatically\\nassociates the remaining words the above-mentioned ten topics.\\nbased on the words that appear in each question, lda displays the\\nnumber of topics per question. based on the topic assignments, the\\ntopic weightages for each question is generated. for topics not\\npresent in a question, a minimal weightage is given to those topics\\nin lieu of a zero value. the value ensures that the topic weightages\\nfor a question sum to one. similar to the tf-idf output, the new\\nvector of 10 columns per question becomes the input for the\\nmachine learning algorithms.\\n\\n3.3 extreme learning machine\\nextreme learning machine (elm) is a learning algorithm for\\nsingle-hidden layer feedforward neural networks (slfns). elm\\ncan be used for classification, regression, clustering, compression\\nand feature learning. elm randomly chooses the hidden nodes and\\ndetermines the output weights of the neural networks.\\nthe following three-step learning model explains elm. given a\\ntraining set that is labeled (information about the target nodes),\\nhidden node activation function and number of hidden nodes,\\nstep 1: randomly assign hidden node parameters\\nstep 2: calculate the hidden layer output matrix, h\\nstep 3: calculate the output weight 𝛾\\ngiven a set of inputs with unknown labels, the objective is to find\\nthe target outputs [27]. once the inter-layer weights have been\\nfound, the same weights are used during the testing phase. for a\\ngiven set of input samples xk, the target/output is given by tk. for\\nnumber of hidden nodes l and with a certain activation function\\nf(x), the slfn is modeled as\\n∑𝐿𝑗=1 𝛾𝑗 𝑓𝑗 (𝑥𝑘 ) = ∑𝐿𝑗=1 𝛾𝑗 𝑓(𝑤𝑗 ∙ 𝑥𝑘 + 𝑏𝑗 ) = 𝑜𝑘 , 𝑘 = 1, … , 𝐿 (4)\\n\\nfigure 3: graphical model representation of lda\\nsince lda involves topic modeling, an appropriate k value chosen\\nfor our work was ten. this allowed a standard comparison between\\nlda and the top ten weightages from the tf-idf method. the\\ngenerated unique topics (based on the stemmed words) are shown\\nin table 3.\\ntable 3 - topic names generated by lda\\ntopic number\\n\\nstemmed topic name\\n\\n1\\n\\ndiffer\\n\\n2\\n\\ndiscrete\\n\\n3\\n\\nimpulse\\n\\n4\\n\\nsignal\\n\\n5\\n\\nfilter\\n\\n6\\n\\napply\\n\\n7\\n\\ndft\\n\\n8\\n\\noutput\\n\\n9\\n\\nsample\\n\\n10\\n\\nsystem\\n\\nwhere wj refers to the weight vector that stores the weights between\\ninput and hidden nodes, 𝛾j refers to the weight vector that stores the\\nweights between the hidden and output nodes, bj refers to the\\nthreshold of the jth hidden nodes. the objective is that ok and tk\\n(original target) should have zero difference [23] using possible\\nactivation functions that include sigmoid, sine, radial basis and\\nhard-limit.\\nin our case, the output of the elm are three continuous values that\\nrepresent the values assigned to the three learning outcome\\ncategories (remember, apply and transfer). to convert the three\\nvalues into a binary value for comparing the predicted labels with\\nthe actual labels, we set the learning outcome category with the\\nhighest value to one and the remaining two to zero.\\n\\n3.4 support vector machine\\nsupport vector machine (svm) is a mapping of data samples such\\nthat these samples can be distinctly labeled. the concept of svm\\nis derived from margins and subsequently separating data into\\ngroups with large gaps between them. deriving an optimal\\nhyperplane for identifying linearly separable patterns is the key to\\nsvm. this idea is extended to cases where the patterns are nonlinearly separable, by using a kernel function to transform the\\noriginal data samples to map onto a new space [28]. possible\\nkernels are: linear, polynomial, radial basis and sigmoid.\\nfor our work, we used the c-support vector classification type.\\ngiven a set of inputs and targets, the cost function is given by [29]\\n1\\n\\nmin 𝑝𝑇 𝑝 + 𝐶 ∑𝑘𝑗=1 𝜉𝑗\\n\\n𝑝,𝑚,𝜉 2\\n\\n(5)\\n\\nsubject to 𝑦𝑗 (𝑝𝑇 𝜙(𝑣𝑗 ) + 𝑚) ≥ 1 − 𝜉𝑗 , 𝜉𝑗 ≥ 0, 𝑗 = 1, … , 𝑘60\\n\\n\\x0cwhere c>0 is the regularization parameter, m is a constant, p is the\\nvector of coefficients, 𝜉𝑗 refers to parameters that handle the inputs,\\nindex j refers to labeling the k training cases, v refers to the\\nindependent variables, y refers to the class labels, 𝜙 refers to the\\nkernel used that transforms data from the input to the chosen feature\\nspace.\\nfundamentally, support vectors are data points that lie close to the\\ndecision boundary, which are the hardest to classify. svm\\nmaximizes the margin around the hyperplane that separates these\\npoints. the cost function is determined based on the training\\nsamples (support vectors). these support vectors are the basic\\nelements of a training set that would change the position of the\\nhyperplane dividing the dataset. svm becomes an optimization\\nproblem for determining the optimal hyperplane.\\n\\n3.5 performance metrics\\nto evaluate the reliability of our four technique combinations with\\nthe subject matter expert’s labels, we looked at using the f1\\nmeasure. accuracy is the number of correct labels divided by the\\nsize of testing data. the f1 measure is a harmonic mean of two\\nother metrics: precision and recall. precision refers to the\\ncorrectness of questions that have been selected as a particular\\ncategory. recall refers to the correctness of selection of the correct\\ncategory given all the questions that were correctly classified.\\nbecause minimizing the number of false positives and false\\nnegatives was important for accurately assigning new questions to\\nthe correct practice sets, we used the f1 measure as the basis for\\nour algorithm comparisons. to explain the f1 measure, we will step\\nthrough the confusion matrix used to describe the performance of a\\nlabeling model on a set of testing data. there are four concepts used\\nto construct the confusion matrix:\\ntrue positive (tp) refers to the number of questions that the\\nalgorithm correctly identifies as presenting a label.\\nfalse positive (fp) refers to the number of questions that the\\nalgorithm identifies as presenting a label while the subject matter\\nexpert indicates the label was absent.\\ntrue negative (tn) refers to the number of questions that the\\nalgorithm correctly identifies as having a label absent.\\nfalse negative (fn) refers to the number of questions that the\\nalgorithm identifies as having a label absent while the subject\\nmatter expert indicates the label was present.\\nthe f1 measure is calculated as follows [30]\\n𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 =\\n𝑅𝑒𝑐𝑎𝑙𝑙 =\\n𝐹1 𝑚𝑒𝑎𝑠𝑢𝑟𝑒 =\\n\\n𝑇𝑃\\n(𝑇𝑃+𝐹𝑃)\\n𝑇𝑃\\n\\n(𝑇𝑃+𝐹𝑁)\\n\\n2 × 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 × 𝑟𝑒𝑐𝑎𝑙𝑙\\n𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛+𝑟𝑒𝑐𝑎𝑙𝑙\\n\\n(6)\\n(7)\\n(8)\\n\\n4. results and analysis\\n4.1 insights by subject matter expert\\nwhen looking at every question presented to students over the\\ncourse of a semester, the subject matter expert identified the\\nnumber of questions corresponding to remember, apply and\\ntransfer as shown in table 4. just by labeling the course questions,\\nthe subject matter expert realized how misaligned the course’s\\nlearning outcomes were with its assessment practices. a large\\nemphasis on apply questions was expected, but the dearth of\\ntransfer questions was surprising. of those 23 transfer items, most\\nwere presented during the final exam.\\n\\ntable 4 - frequency of questions aligned to learning outcomes\\nlearning outcome\\n\\nfrequency (number of questions)\\n\\nremember\\n\\n62\\n\\napply\\n\\n131\\n\\ntransfer\\n\\n23\\n\\none of the stated learning outcomes of the course was to prepare\\nstudents to flexibly transfer course content to novel problems and\\nnew situations. however, waiting until the final exam to present\\nstudents with such opportunities denied them actionable feedback\\nduring the semester. in response to the pre-processing labeling\\nefforts, the subject matter expert then added 42 new transfer\\nquestions throughout the course for the next semester.\\n\\n4.2 model reliability with subject matter\\nexpert\\nthe objective of this implementation is to evaluate whether the\\ntrained model is able to predict the type of question (remember,\\napply or transfer). based on the trained model using questions\\nfrom the undergraduate course, the testing questions from\\ntextbooks and online sources were passed through our model to\\ndetermine the level of reliability of labeling new questions that\\nwere not generated by the subject matter expert. in our intended use\\ncase, the testing dataset would not need to be manually labeled.\\nhowever, to determine the level of reliability of our labeling\\nalgorithms, the subject matter expert’s manual labels served as a\\nground truth for the f1 measure calculations.\\n\\n4.2.1 parameter selection\\nwe first determined the best set of parameters based on 10-fold\\ncross validation of the training dataset. as there were 120\\nquestions, 90% of the questions (108 questions) were used for\\ntraining and 10% of the questions (12 questions) were used as a\\nvalidation set. this process was done 10 times using 10 different\\nbundles of the 120 questions. the best set of parameters were\\nchosen based on a grid search for both elm and svm.\\nthe parameters that were varied for elm were:\\n1.\\n2.\\n\\nnumber of hidden nodes\\nactivation function (sigmoid / radial basis / hard-limit)\\n\\nthe parameters yielding the best results corresponded to 72 hidden\\nnodes using hard-limit activation function.\\nthe parameters that were varied for svm were:\\n1.\\n2.\\n3.\\n\\nkernel (sigmoid / radial basis)\\ncost value\\ngamma value\\n\\nthe parameters yielding the best results corresponded to sigmoid\\nkernel, cost value = 1, gamma value = 0.26\\n\\n4.2.2 comparing four combinations\\nwith respect to the f1 measure, calculations were done separately\\nfor the three labels. the mean of those calculations was then used\\nas the algorithm’s overall performance measure. with respect to\\nelm, the calculation was repeated 10 times because the\\ninitialization weights are randomly assigned in each iteration. the\\nmean value of the f1 measure was taken.\\ntable 5 below shows the f1 measure values (for each individual\\nclass and overall f1 mean) for the four combinations. “r” refers to\\nremember, “a” refers to apply, “t” refers to transfer and “s.d.”\\nrefers to standard deviation.61\\n\\n\\x0ctable 5 - f1 measure values for four combinations\\ncombination\\n\\nr\\n\\na\\n\\nt\\n\\nmean\\n\\ns.d.\\n\\n1. tf-idf\\nwith svm\\n\\n0.870\\n\\n0.737\\n\\n0.667\\n\\n0.758\\n\\n0.084\\n\\n2. lda with\\nsvm\\n\\n0.400\\n\\n0.593\\n\\n0.556\\n\\n0.516\\n\\n0.084\\n\\n3. tf-idf\\nwith elm\\n\\n0.926\\n\\n0.815\\n\\n0.840\\n\\n0.860\\n\\n0.048\\n\\n4. lda with\\nelm\\n\\n0.467\\n\\n0.520\\n\\n0.647\\n\\n0.545\\n\\n0.076\\n\\ntf-idf with elm achieved the highest mean f1 measure value\\nand the lowest standard deviation – indicating that it was the most\\nreliable combination. it can be seen that the remember label yields\\nthe highest f1 values out of the three labels in combination 3. in\\ngeneral, remember-labeled questions are short, resulting in about\\nfour to five zero values in the tf-idf vector of 10 columns that is\\npassed as an input into the elm. hence, the algorithm identifies\\nremember-labeled questions very accurately due to their size.\\nthe result of high reliability in using elm is as expected because\\nit has already been demonstrated that elm outperforms svm when\\ncomparing in terms of standard deviation of training and testing\\nroot-mean-square values, time taken, network complexity, as well\\nas performance comparison in real medical diagnosis application\\n[23]. on the other hand, although lda has been shown to achieve\\nhigher performance as it groups words together in terms of topics\\ninstead of looking at combinations of individual words which may\\nnot link together, in the context of our work, tf-idf outperforms\\nlda instead. this is because for lda, the goal is to correctly\\nassign each document (or question) to a class label in a reduced\\ndimensional space [31]. however, in our corpus of questions, there\\nare several technical terms involved, without any prior labeling of\\ntopics. hence, lda is not appropriate for our analysis.\\n\\n5. conclusions\\nbased on the comparison of our four algorithms, our most reliable\\nmodel (tf-idf with elm) is able to accurately label new course\\nquestions for the undergraduate electrical and electronic\\nengineering course with 0.86 reliability in terms of f1 measure.\\nany novice instructor who takes over this course in the future or\\nteaching assistants tasked with refreshing the course assignments\\nwould be able to extract new questions from any external source\\nand pass them to the algorithm to automatically label the questions\\nas the original course coordinator would. this allows members of\\nthe course design team without a strong background in learning to\\nmake curriculum decisions regarding the alignment of the course’s\\nlearning outcomes.\\nas discussed earlier, outcome-based learning environments\\nfacilitate transforming the model of instruction from instructorcentric and lecture-based to being more learner focused filled with\\na variety of activities and learning pathways. however, in learnercentered environments, assessment is still the key driver, and often\\nthe key inhibitor of learning [3]. if the assessments require shallow\\nunderstanding, then learners calibrate their efforts to achieve this\\nlow bar. when assessments require deep understanding or great\\nproficiency, learners are likely to put in more effortful practice.\\nin line with this assessment philosophy, our tf-idf with elm\\nmodel is theoretically capable of matching any learning activity to\\nany set of learning outcomes as long as the course designers or\\nsubject matter experts provide enough examples that are explicitly\\n\\naligned to the intended learning outcomes when training the model.\\nfor the convenience of the subject matter expert in our context, we\\nused a reduced version of bloom’s taxonomy in this study.\\nhowever, the final algorithm is capable of using the full bloom’s\\nmodel, a different model, or a custom set of learning outcomes as\\nits labeling framework.\\nhence, with the high reliability of the prediction algorithm\\npresented in our work, our process for calibrating the algorithm can\\nbe used in any academic or industrial setting to provide the right set\\nof formative assessment opportunities to students (enhancing\\nsubject knowledge) or employees (professional development).\\nonce the learning outcomes of activities are labeled reliably, it is\\nthen easier to think about how to engage learners in deliberate\\npractice to reach those outcomes and develop their expertise. once\\nopportunities for deliberate practice that align to the course learning\\noutcomes are implemented into a course, it becomes easier to think\\nabout how to align the feedback regarding those opportunities to\\nsupport the development of domain expertise.\\nthis work provides a first step at being able to regularly introduce\\nlearning activities that promote the development of adaptive\\nexpertise into a course by matching external sources of activities\\nwith the course’s learning outcomes. deliberate practice requires\\nrepetition that varies in ways that highlight the structural elements\\nof a domain. having a way to incorporate new sources of questions\\nand problems into a course that align with the course’s goals\\nprovides learners more opportunities for internalizing when to\\napply their domain specific skills and knowledge. finally, our\\nalgorithm is potentially useful for designing courses to reach noncontent-based learning outcomes, making policies that support\\nconstructive alignment, and evaluating course assessment of\\nlearning plans.\\n\\n6. future work\\nbuilding off of our machine learning labeling work, we would like\\nto explore constructing a new version of lda that can be tailormade to label questions. there are situations in which weightages\\ngiven to words are the same, with different words representing\\nthose weightages. similarly, the same words can have different\\nweightages. we are keen to continue working on features based on\\nword arrangement, word context and word order that affect\\nweightage assignments. in addition, elm can be enhanced by\\nusing kernels.\\nfrom the learning aspect, we would like to extend our question\\nlabel categories to all six outcomes described in bloom’s\\ntaxonomy and expand the model to label outcomes based on the\\ntypes of sentences used in forum conversations and other\\ncollaborative learning activities. eventually, we aim to determine\\nthe proficiency level of learners so we can put learning supports in\\nplace to guide their learning journeys. ultimately, we wish to\\nprovide learners with learning activities and opportunities for\\ndeliberate practice embedded with actionable feedback to develop\\ntheir adaptive expertise.\\n\\n7. acknowledgments\\nthis work was conducted within the delta-ntu corporate lab for\\ncyber-physical systems with funding support from delta\\nelectronics inc and the national research foundation (nrf)\\nsingapore under the corp lab@university scheme.\\n\\n8. references\\n[1] krathwohl, d.r. 2002. a revision of bloom's taxonomy:\\nan overview. theory into practice. 41, 4 (2002), 212-218.\\ndoi= http://dx.doi.org/10.1207/s15430421tip4104_262\\n\\n\\x0c[2] biggs, j. 1996. enhancing teaching through constructive\\nalignment. higher education. 32, 3 (1996), 347-364. doi=\\nhttp://dx.doi.org/10.1007/bf00138871\\n[3] boud, d. 2010. sustainable assessment: rethinking\\nassessment for the learning society. studies in continuing\\neducation. 22, 2 (2010), 151-167. doi=\\nhttp://dx.doi.org/10.1080/713695728\\n[4] boud, d. and falchikov, n. 2006. aligning assessment with\\nlong-term learning. assessment & evaluation in higher\\neducation. 31, 4 (2006), 399-413. doi=\\nhttp://dx.doi.org/10.1080/02602930600679050\\n[5] miller, g. e. 1990. the assessment of clinical\\nskills/competence/performance. academic medicine. 65, 9\\n(1990), s63-s67. doi=\\nhttp://dx.doi.org/10.1097/00001888-199009000-00045\\n[6] wass, v. et al. 2001. assessment of clinical competence.\\nthe lancet. 357, 9260 (2001), 945-949. doi=\\nhttp://dx.doi.org/10.1016/s0140-6736(00)04221-5\\n[7] hmelo-silver, c.e. 2004. problem-based learning: what\\nand how do students learn? educational psychology\\nreview. 16, 3 (2004). 235-266. doi=\\nhttp://dx.doi.org/10.1023/b:edpr.0000034022.16470.f3\\n[8] biggs, j. b. and collis, k.f. 2014. evaluating the quality of\\nlearning: the solo taxonomy (structure of the observed\\nlearning outcomes). academic press.\\n[9] crowe, a. et al. 2008. biology in bloom: implementing\\nbloom's taxonomy to enhance student learning in biology.\\ncbe-life sciences education. 7, 4 (2008), 368-381. doi=\\nhttp://dx.doi.org/10.1187/cbe.08-05-0024\\n[10] ericsson, k.a. et al. 1993. the role of deliberate practice\\nin the acquisition of expert performance. psychological\\nreview. 100, 3 (1993), 363-406. doi=\\nhttp://dx.doi.org/10.1037/0033-295x.100.3.363\\n[11] duckworth, a. l. et al. 2007. grit: perseverance and\\npassion for long-term goals. journal of personality and\\nsocial psychology. 92, 6 (2007), 1087. doi=\\nhttp://dx.doi.org/10.1037/0022-3514.92.6.1087\\n[12] schwartz d. l. et al. 2005. efficiency and innovation in\\ntransfer. transfer of learning from a modern\\nmultidisciplinary perspective. information age publishing.\\n1-51.\\n[13] chi, m. t. 2006. two approaches to the study of experts'\\ncharacteristics. the cambridge handbook of expertise and\\nexpert performance. cambridge university press. 21-30.\\n[14] black, p. and william, d. 1998. assessment and classroom\\nlearning. assessment in education principles policy and\\npractice. 5, 1 (1998), 7-74. doi=\\nhttp://dx.doi.org/10.1080/0969595980050102\\n[15] ritter, s. et al. 2007. cognitive tutor: applied research in\\nmathematics education. psychonomic bulletin & review. 14,\\n2 (2007), 249-255. doi=\\nhttp://dx.doi.org/10.3758/bf03194060\\n[16] fei, t. et al. 2003. question classification for e-learning by\\nartificial neural network. in proceedings of the 2003 joint\\nfourth international conference on information,\\ncommunications and signal processing and the fourth\\npacific rim conference on multimedia (singapore, 2003),\\n1-5. doi= http://dx.doi.org/10.1109/icics.2003.1292768\\n\\n[17] cheng, s. c. et al. 2005. automatic leveling system for elearning examination pool using entropy-based decision\\ntree. in advances in web-based learning – icwl 2005\\n(hong kong, 2005), 273-278. doi=\\nhttp://dx.doi.org/10.1007/11528043_27\\n[18] jayakodi, k. et al. 2015. an automatic classifier for exam\\nquestions in engineering: a process for bloom's\\ntaxonomy. in 2015 ieee international conference on\\nteaching, assessment, and learning for engineering\\n(tale) (zhuhai, china, 2015). doi=\\nhttps://dx.doi.org/10.1109/tale.2015.7386043\\n[19] haris, s. s. and omar, n. 2015. bloom's taxonomy question\\ncategorization using rules and n-gram approach. journal of\\ntheoretical and applied information technology. 76, 3\\n(2015), 401-407.\\n[20] yahya, a. a. et al. 2013. analyzing the cognitive level of\\nclassroom questions using machine learning techniques. in\\nthe 9th international conference on cognitive science\\n(kuching, sarawak, malaysia, 2013). 587-595. doi=\\nhttp://dx.doi.org/10.1016/j.sbspro.2013.10.277\\n[21] abduljabbar, d. a. and omar, n. 2015. exam questions\\nclassification based on bloom's taxonomy cognitive level\\nusing classifiers combination. journal of theoretical and\\napplied information technology. 78, 3 (2015), 447-455.\\n[22] sangodiah, a. et al. 2014. a review in feature extraction\\napproach in question classification using support vector\\nmachine. in 2014 ieee international conference on\\ncontrol system, computing and engineering (penang,\\nmalaysia, 2014), 536-541. doi=\\nhttp://dx.doi.org/10.1109/iccsce.2014.7072776\\n[23] huang, g. b. et al. 2006. extreme learning machine:\\ntheory and applications. neurocomputing. 70, 1-3 (2006),\\n489-501. doi=\\nhttp://dx.doi.org/10.1016/j.neucom.2005.12.126\\n[24] trinity university course assessment and outcomes: 2016\\nhttps://inside.trinity.edu/collaborative/collaborativegrants/course-redesign-stipends/course-assessment-andoutcomes. accessed: 2017-02-24.\\n[25] bernardi, r. term frequency and inverted document\\nfrequency. university of trento, trentino.\\n[26] blei, d. m. et al. 2003. latent dirichlet allocation. journal\\nof machine learning research. 3 (2003), 993-1022.\\n[27] huang, g. b. 2015. what are extreme learning machines?\\nfilling the gap between frank rosenblatt’s dream and\\njohn von neumann’s puzzle. cognitive computation. 7, 3\\n(2015), 263-278. doi= http://dx.doi.org/10.1007/s12559015-9333-0\\n[28] weston, j. support vector machine (and statistical\\nlearning theory). nec labs america, princeton.\\n[29] chang, c. c. and lin, c. j. 2011. libsvm: a library for\\nsupport vector machines. acm transactions on\\nintelligent systems and technology (tist). 2, 3 (2011), 139. doi= http://dx.doi.org/10.1145/1961189.1961199\\n[30] santra, a. k. and christy, c. j. 2012. genetic algorithm\\nand confusion matrix for document clustering. ijcsi\\ninternational journal of computer science issues. 9, 1\\n(2012), 322-328.\\n[31] hu, d. j. 2009. latent dirichlet allocation for text,\\nimages, and music.\",\n",
       " '63\\n\\n\\x0cbehavior-based latent variable model\\nfor learner engagement\\nandrew s. lan1 , christopher g. brinton2 , tsung-yen yang3 , mung chiang1\\n1\\n\\nprinceton university, 2 zoomi inc., 3 national chiao tung university\\n\\nandrew.lan@princeton.edu, christopher.brinton@zoomiinc.com, tsungyenyang.eecs02@nctu.edu.tw, chiangm@princeton.edu\\n\\nabstract\\nwe propose a new model for learning that relates videowatching behavior and engagement to quiz performance. in\\nour model, a learner’s knowledge gain from watching a lecture\\nvideo is treated as proportional to their latent engagement\\nlevel, and the learner’s engagement is in turn dictated by a set\\nof behavioral features we propose that quantify the learner’s\\ninteraction with the lecture video. a learner’s latent concept\\nknowledge is assumed to dictate their observed performance\\non in-video quiz questions. one of the advantages of our\\nmethod for determining engagement is that it can be done\\nentirely within standard online learning platforms, serving\\nas a more universal and less invasive alternative to existing\\nmeasures of engagement that require the use of external\\ndevices. we evaluate our method on a real-world massive\\nopen online course (mooc) dataset, from which we find that\\nit achieves high quality in terms of predicting unobserved\\nfirst-attempt quiz responses, outperforming two state-of-theart baseline algorithms on all metrics and dataset partitions\\ntested. we also find that our model enables the identification\\nof key behavioral features (e.g., larger numbers of pauses\\nand rewinds, and smaller numbers of fast forwards) that are\\ncorrelated with higher learner engagement.\\n\\nkeywords\\nbehavioral data, engagement, latent variable model, learning\\nanalytics, mooc, performance prediction\\n\\n1.\\n\\nintroduction\\n\\nthe recent and rapid development of online learning platforms, coupled with advancements in machine learning, has\\ncreated an opportunity to revamp the traditional “one-sizefits-all” approach to education. this opportunity is facilitated\\nby the ability of many learning platforms, such as massive\\nopen online course (mooc) platforms, to collect several\\ndifferent types of data on learners, including their assessment\\nresponses as well as their learning behavior [9]. the focus\\nof this work is on using different forms of data to model\\nthe learning process, which can lead to effective learning\\nanalytics and potentially improve learning efficacy.\\n\\n1.1\\n\\nbehavior-based learning analytics\\n\\ncurrent approaches to learning analytics are focused mainly\\non providing feedback to learners about their knowledge\\nstates – or the level to which they have mastered given concepts/topics/knowledge components – through analysis of\\ntheir responses to assessment questions [10, 24]. there are\\nother cognitive (e.g., engagement [17, 31], confusion [37], and\\n\\nemotion [11]) as well as non-cognitive (e.g., fatigue, motivation, and level of financial support [14]) factors beyond\\nassessment performance that are crucial to the learning process as well. accounting for them thus has the potential to\\nyield more effective learning analytics and feedback.\\nto date, it has been difficult to measure these factors of the\\nlearning process. contemporary online learning platforms,\\nhowever, have the capability to collect behavioral data that\\ncan provide some indicators of them. this data commonly\\nincludes learners’ usage patterns of different types of learning\\nresources [12, 15], their interactions with others via social\\nlearning networks [7, 28], their clickstream and keystroke activity logs [2, 8, 30], and sometimes other metadata including\\nfacial expressions [35] and gaze location [6].\\nrecent research has attempted to use behavioral data to\\naugment learning analytics. [5] proposed a latent response\\nmodel to classify whether a learner is gaming an intelligent\\ntutoring system, for example. several of these works have\\nsought to demonstrate the relationship between behavior and\\nperformance of learners in different scenarios. in the context\\nof moocs, [22] concluded that working on more assignments\\nlead to better knowledge transfer than only watching videos,\\n[12] extracted probabilistic use cases of different types of\\nlearning resources and showed they are predictive of certification, [32] used discussion forum activity and topic analysis to\\npredict test performance, and [26] discovered that submission\\nactivities can be used to predict final exam scores. in other\\neducational domains, [2] discovered that learner keystroke\\nactivity in essay-writing sessions is indicative of essay quality, [29] identified behavior as one of the factors predicting\\nmath test achievement, and [25] found that behavior is predictive of whether learners can provide elegant solutions to\\nmathematical questions.\\nin this work, we are interested in how behavioral data can\\nbe used to model a learner’s engagement.\\n\\n1.2\\n\\nlearner engagement\\n\\nmonitoring and fostering engagement is crucial to education,\\nyet defining it concretely remains elusive. research has\\nsought to identify factors in online learning that may drive\\nengagement; for example, [17] showed that certain production\\nstyles of lecture videos promote it. [20] defined disengagement\\nas dropping out in the middle of a video and studied the\\nrelationship between disengagement and video content, while\\n[31] considered the relationship between engagement and the64\\n\\n\\x0csemantic features of mathematical questions that learners\\nrespond to. [33] studied the relationship between learners’\\nself-reported engagement levels in a learning session and their\\nfacial expressions immediately following in-session quizzes,\\nand [34] considered how engagement is related to linguistic\\nfeatures of discussion forum posts.\\nthere are many types of engagement [3], with the type of\\ninterest depending on the specific learning scenario. several\\napproaches have been proposed for measuring and quantifying different types. these approaches can be roughly\\ndivided into two categories: device-based and activity-based.\\ndevice-based approaches measure learner engagement using\\ndevices external to the learning platform, such as cameras to\\nrecord facial expressions [35], eye-tracking devices to detect\\nmind wandering while reading text documents [6], and pupil\\ndilation measurements, which are claimed to be highly correlated with engagement [16]. activity-based approaches, on\\nthe other hand, measure engagement using heuristic features\\nconstructed from learners’ activity logs; prior work includes\\nusing replies/upvote counts and topic analysis of discussions\\n[28], and manually defining different engagement levels based\\non activity types found in moocs [4, 21].\\nboth of these types have their drawbacks. device-based\\napproaches are far from universal in standard learning platforms because they require integration with external devices.\\nthey are also naturally invasive and carry potential privacy\\nrisks. activity-based approaches, on the other hand, are\\nnot built on the same granularity of data, and tend to be\\ndefined from heuristics that have no guarantee of correlating\\nwith learning outcomes. it is therefore desirable to develop a\\nstatistically principled, activity-based approach to inferring\\na learner’s engagement.\\n\\n1.3\\n\\nour approach and contributions\\n\\nin this paper, we propose a probabilistic model for inferring a\\nlearner’s engagement level by treating it as a latent variable\\nthat drives the learner’s performance and is in turn driven\\nby the learner’s behavior. we apply our framework to a\\nreal-world mooc dataset consisting of clickstream actions\\ngenerated as learners watch lecture videos, and question\\nresponses from learners answering in-video quiz questions.\\nwe first formalize a method for quantifying a learner’s behavior while watching a video as a set of nine behavioral features\\nthat summarize the clickstream data generated (section 2).\\nthese features are intuitive quantities such as the fraction\\nof video played, the number of pauses made, and the average playback rate, some of which have been associated with\\nperformance previously [8]. then, we present our statistical\\nmodel of learning (section 3) as two main components: a\\nlearning model and a response model. the learning model\\ntreats a learner’s gain in concept knowledge as proportional\\nto their latent engagement level while watching a lecture\\nvideo. concept knowledge is treated as multidimensional, on\\na set of latent concepts underlying the course, and videos\\nare associated with varying levels to different concepts. the\\nresponse model treats a learner’s performance on in-video\\nquiz questions, in turn, as proportional to their knowledge\\non the concepts that this particular question relates to.\\nby defining engagement to correlate directly with perfor-\\n\\nmance, we are able to learn which behavioral features lead to\\nhigh engagement through a single model. this differs from\\nprior works that first define heuristic notions of engagement\\nand subsequently correlate engagement with performance,\\nin separate procedures. moreover, our formulation of latent\\nengagement can be made from entirely within standard learning platforms, serving as a more universally applicable and\\nless invasive alternative to device-based approaches.\\nfinally, we evaluate two different aspects of our model (section 4): its ability to predict unobserved, first-attempt quiz\\nquestion responses, and its ability to provide meaningful\\nanalytics on engagement. we find that our model predicts\\nwith high quality, achieving aucs of up to 0.76, and outperforming two state-of-the-art baselines on all metrics and\\ndataset partitions tested. one of the partitions tested corresponds to the beginning of the course, underscoring the\\nability of our model to provide early detection of struggling\\nor advanced students. in terms of analytics, we find that\\nour model enables us to identify behavioral features (e.g.,\\nlarge numbers of pauses and rewinds, and small numbers of\\nfast forwards) that indicate high learner engagement, and to\\ntrack learners’ engagement patterns throughout the course.\\nmore generally, these findings can enable an online learning platform to detect learner disengagement and perform\\nappropriate interventions in a fully automated manner.\\n\\n2.\\n\\nbehavioral data\\n\\nin this section, we start by detailing the setup of lecture\\nvideos and quizzes in moocs. we then specify videowatching clickstream data and our method for summarizing\\nit into behavioral features.\\n\\n2.1\\n\\ncourse setup and data capture\\n\\nwe are interested in modeling learner engagement while\\nwatching lecture videos to predict their performance on invideo quiz questions. for this purpose, we can view an\\ninstructor’s course delivery as the sequence of videos that\\nlearners will watch interspersed with the quiz questions they\\nwill answer. let q = (q1 , q2 , . . .) be the sequence of questions\\nasked through the course. a video could have any number\\nof questions generally, including none; to enforce a 1:1 correspondence between video content and questions, we will\\nconsider the “video” for question qn to be all video content\\nthat appears between qn−1 and qn . based on this, we will\\nexplain the formats of video-watching and quiz response data\\nwe work with in this section.\\nour dataset. the dataset we will use is from the fall 2012\\noffering of the course networks: friends, money, and bytes\\n(fmb) on coursera [1]. this course has 92 videos distributed\\namong 20 lectures, and exactly one question per video.\\n\\n2.1.1\\n\\nvideo-watching clickstreams\\n\\nwhen a learner watches a video on a mooc, their behavior\\nis typically recorded as a sequence of clickstream actions.\\nin particular, each time a learner makes an action – play,\\npause, seek, ratechange, open, or close – on the video\\nplayer, a clickstream event is generated. formally, the ith\\nevent created for the course will be in the format\\nei =< ui , vi , ei , p0i , pi , xi , si , ri >65\\n\\n\\x0chere, ui and vi are the ids of the specific learner (user) and\\nvideo, respectively, and ei is the type of action that ui made\\non vi . pi is the position of the video player (in seconds)\\nimmediately after ei is made, p0i is the position immediately\\nbefore,1 xi is the unix timestamp (in seconds) at which ei\\nwas fired, si is the binary state of the video player – either\\nplaying or paused – once this action is made, and ri is the\\nplayback rate of the video player once this action is made.\\nour fmb dataset has 314,632 learner-generated clickstreams\\nfrom 3,976 learners.2\\nthe set eu,v = {ei |ui = u, vi = v} of clickstreams for learner\\nu recorded on video v can be used to reconstruct the behavior\\nu exhibits on v. in section 2.2 we will explain the features\\ncomputed from eu,v to summarize this behavior.\\n\\nfigure 1: distribution of the number of videos that\\neach each learner completed in fmb. more than\\n85% of learners completed less than 20 videos.\\n\\n2.1.2 quiz responses\\nwhen a learner submits a response to an in-video quiz question, an event is generated in the format\\nam =< um , vm , xm , am , ym >\\nagain, um and vm are the learner and video ids (i.e., the\\nquiz corresponding to the video). xm is the unix timestamp\\nof the submission, am is the specific response, and ym is the\\nnumber of points awarded for the response. the questions\\nin our dataset are multiple choice with a single response, so\\nym is binary-valued.\\nin this work, we are interested in whether quiz responses\\nwere correct on first attempt (cfa) or not. as a result,\\nwith au,v = {am |um = u, vm = v}, we consider the event\\na0u,v in this set with the earliest timestamp x0u,v . we also\\n0\\nonly consider the set of clickstreams eu,v\\n⊆ eu,v that occur\\nbefore x0u,v , as the ones after would be anti-causal to cfa.\\n\\n2.2\\n\\nbehavioral features and cfa score\\n\\n0\\nwith the data eu,v\\nand a0u,v , we construct two sets of information for each learner u on each video v, i.e., each\\nlearner-video pair. first is a set of nine behavioral features\\nthat summarize u’s video-watching behavior on v [8]:\\n\\n(3) fraction played. the fraction of the video that the\\nlearner played relative to the length. formally, it is calculated\\nas gu,v /lv , where\\ngu,v =\\n\\nx\\ni∈s\\n\\nis the total length of video that was played (while in the\\nplaying state). here, s = {i ∈ a0u,v : ai+1 6= open ∧ si =\\nplaying}.\\n(4) fraction paused. the fraction of time the learner\\nstayed paused on the video relative to the length. it is\\ncalculated as hu,v /lv , where\\nhu,v =\\n\\nx\\ni∈s\\n\\neu,v =\\n\\nx\\ni∈s\\n\\nmin(xi+1 − xi , lv )\\n\\nis the elapsed time on v obtained by finding the total unix\\ntime for u on v, and lv is the length of the video (in seconds).\\nhere, s = {i ∈ a0u,v : ai+1 6= open}. lv is included as an\\nupper bound for excessively long intervals of time.\\n(2) fraction completed. the fraction of the video that the\\nlearner completed, between 0 (none) and 1 (all). formally,\\nit is cu,v /lv , where cu,v is the number of unique 1 second\\nsegments of the video that the learner visited.\\n\\n(5) number of pauses. the number of times the learner\\npaused the video, or\\n\\nx\\n\\npi and p0i will only differ when i is a skip event.\\nthis number excludes invalid stall, null, and error events,\\nas well as open and close events which are generated automatically.\\n\\n1{ai = pause}\\n\\nwhere 1{} is the indicator function.\\n(6) number of rewinds. the number of times the learner\\nskipped backwards in the video, or\\n\\nx\\n\\ni∈a0u,v\\n\\n1{ai = skip ∧ p0i < pi }\\n\\n(7) number of fast forwards. the number of times the\\nlearner skipped forward in the video, i.e., with p0i > pi in the\\nprevious equation.\\n(8) average playback rate. the time-average of the\\nlearner’s playback rate on the video. formally, it is calculated\\nas\\n\\n1\\n\\n2\\n\\nmin(ti+1 − ti , lv )\\n\\nis the total time the learner stayed in the paused state on this\\nvideo. here, s = {i ∈ a0u,v : ai+1 6= open ∧ si = paused}.\\n\\ni∈a0u,v\\n\\n(1) fraction spent. the fraction of time the learner spent\\non the video, relative to the playback length of the video.\\nformally, this quantity is eu,v /lv , where\\n\\nmin(p0i+1 − pi , lv )\\n\\nr̄u,v =\\n\\np\\ni∈s ri · min(xi+1 − xi , lv )\\np\\ni∈s\\n\\nwhere s = {i ∈\\n\\na0u,vmin(xi+1 − xi , lv )\\n\\n: ai+1 6= open ∧ si = playing}.\\n\\n66\\n\\n\\x0c(9) standard deviation of playback rate. the standard\\ndeviation of the learner’s playback rate. it is calculated as\\n\\n\\xa0p\\n\\ni∈s (ri\\n\\n− r̄u,v )2 · min(xi+1 − xi , lv )\\ni∈s min(xi+1 − xi , lv )\\n\\np\\n\\nwith the same s as the average playback rate.\\nthe second piece of information for each learner-video pair\\nis u’s cfa score yu,v ∈ {0, 1} on the quiz question for v.\\n\\n2.3\\n\\ndataset subsets\\n\\nwe will consider different groups of learner-video pairs when\\nevaluating our model in section 4. our motivation for doing\\nso is the heterogeneity of learner motivation and high dropoff\\nrates in moocs [9]: many will quit the course after watching\\njust a few lectures. modeling in a small subset of data,\\nparticularly those at the beginning of the course, is desirable\\nbecause it can lead to “early detection” of those who may\\ndrop out [8].\\nfigure 1 shows the dropoff for our dataset in terms of the\\nnumber of videos each learner completed: more than 85%\\nof learners completed just 20% of the course. “completed”\\nis defined here as having watched some of the video and\\nresponded to the corresponding question. let tu be the\\nnumber of videos learner u completed and γ(v) be the index\\nof video v in the course, we define ωu0 ,v0 = {(u, v) : tu ≥\\nu0 ∧ γ(v) ≤ v0 } to be the subset of learner-video pairs\\nsuch that u completed at least u0 videos and v is within the\\nfirst v0 videos. the full dataset is ω1,92 , and we will also\\nconsider ω20,92 as the subset of 346 active learners over the\\nfull course and ω1,20 as the subset of all learners over the\\nfirst two weeks3 in our evaluation.\\n\\n3.\\n\\nstatistical model of learning\\nwith latent engagement\\n\\nin this section, we propose our statistical model. let u\\ndenote the number of learners (indexed by u) and v the\\nnumber of videos (indexed by v). further, we use tu to\\ndenote the number of time instances registered by learner\\nu (indexed by t); we take a time instance to be a learner\\ncompleting a video, i.e., watching a video and answering the\\ncorresponding quiz question. for simplicity, we use a discrete\\nnotion of time, i.e., each learner-video pair will correspond\\nto one time instance for one learner.\\nour model considers learners’ responses to quiz questions\\nas measurements of their underlying knowledge on a set of\\nconcepts; let k denote the number of such concepts. further,\\nour model considers the action of watching lecture videos\\nas part of learning that changes learners’ latent knowledge\\nstates over time. these different aspects of the model are\\nvisualized in figure 2: there are two main components, a\\nresponse model and a learning model.\\n\\n3.1\\n\\nresponse model\\n\\nour statistical model of learner responses is given by\\nt\\n(t)\\np(yu(t) = 1|c(t)\\nu ) = σ(wv(u,t) cu − µv(u,t) + au ),\\n3\\n\\n(1)\\n\\nin fmb, the first two weeks of lectures is the first 20 videos.\\n\\nfigure 2: our proposed statistical model of learning\\nconsists of two main parts, a response model and a\\nlearning model.\\nwhere v(u, t) : ω ⊆ {1, . . . , u } × {1, . . . , maxu tu } →\\n{1, . . . , v } denotes a mapping from a learner index-time\\nindex pair to the index of the video v that u was watching at\\n(t)\\nt. yu ∈ {0, 1} is the binary-valued cfa score of learner u\\non the quiz question corresponding to the video they watch\\nat time t, with 1 denoting a correct response (cfa) and 0\\ndenoting an incorrect response (non-cfa).\\nthe variable wv ∈ rk\\n+ denotes the non-negative, kdimensional quiz question–concept association vector that\\ncharacterizes how the quiz question corresponding to video v\\ntests learners’ knowledge on each concept, and the variable\\nµv is a scalar characterizing the intrinsic difficulty of the quiz\\n(t)\\nquestion. cu is the k-dimensional concept knowledge vector\\nof learner u at time t, characterizing the knowledge level of\\nthe learner on each concept at the time, and au denotes the\\nstatic, intrinsic ability of learner u. finally, σ(x) = 1+e1−x is\\nthe sigmoid function.\\nwe restrict the question–concept association vector wv to be\\nnon-negative in order to make the parameters interpretable\\n[24]. under this restriction, the values of concept knowledge\\n(t)\\nvector cu can be understood as follows: large, positive values\\nlead to higher chances of answering a question correctly, thus\\ncorresponding to high knowledge, while small, negative values\\nlead to lower chances of answering a question correctly, thus\\ncorresponding to low knowledge.\\n\\n3.2\\n\\nlearning model\\n\\nour model of learning considers transitions in learners’ knowledge states as induced by watching lecture videos. it is given\\nby\\n(t−1)\\nc(t)\\n+ e(t)\\nu = cu\\nu dv(u,t) ,\\n\\nt = 1, . . . , tu ,\\n\\n(2)\\n\\nwhere the variable dv ∈ rk\\n+ denotes the non-negative, kdimensional learning gain vector for video v; each entry\\ncharacterizes the degree to which the video improves learners’\\nknowledge level on each concept. the assumption of nonnegativity on dv implies that videos will not negatively affect\\n(0)\\nlearners’ knowledge, as in [23]. cu is the initial knowledge\\nstate of learner u at time t = 0, i.e., before starting the67\\n\\n\\x0cω20,92\\n\\nω1,20\\n\\nω1,92\\n\\nacc\\n\\nauc\\n\\nacc\\n\\nauc\\n\\nacc\\n\\nauc\\n\\nproposed model\\n\\n0.7293±0.0070\\n\\n0.7608±0.0094\\n\\n0.7096±0.0057\\n\\n0.7045±0.0066\\n\\n0.7058±0.0054\\n\\n0.7216±0.0054\\n\\nsparfa\\n\\n0.7209±0.0070\\n\\n0.7532±0.0098\\n\\n0.7061±0.0069\\n\\n0.7020±0.0070\\n\\n0.6975±0.0048\\n\\n0.7124±0.0050\\n\\nbkt\\n\\n0.7038±0.0084\\n\\n0.7218±0.0126\\n\\n0.6825±0.0058\\n\\n0.6662±0.0065\\n\\n0.6803±0.0055\\n\\n0.6830±0.0059\\n\\ntable 1: quality comparison of the different algorithms on predicting unobserved quiz question responses.\\nthe obtained acc and auc metrics on different subsets of the fmb dataset are given. our proposed model\\nobtains higher quality than the sparfa and bkt baselines in each case.\\ncourse and watching any video.\\n(t)\\n\\nthe scalar latent variable eu ∈ [0, 1] in (2) characterizes\\nthe engagement level that learner u exhibits when watching\\nvideo v(u, t) at time t. this is in turn modeled as\\nt (t)\\ne(t)\\nu = σ(β fu ),\\n\\n(3)\\n\\n(t)\\n\\nwhere fu is a 9-dimensional vector of the behavioral features\\ndefined in section 2.2, summarizing learner u’s behavior while\\nthe video at time t. β is the unknown, 9-dimensional parameter vector that characterizes how engagement associates\\nwith each behavioral feature.\\ntaken together, (2) and (3) state that the knowledge gain a\\nlearner will experience on a particular concept while watching\\na particular video is given by\\n(i) the video’s intrinsic association with the concept, modulated by\\n(ii) the learner’s engagement while watching the video, as\\nmanifested by their clickstream behavior.\\nfrom (2), a learner’s (latent) engagement level dictates the\\nfraction of the video’s available learning gain they acquire\\nto improve their knowledge on each concept. the response\\nmodel (1) in turn holds that performance is dictated by a\\nlearner’s concept knowledge states. in this way, engagement\\nis directly correlated with performance through the concept\\nknowledge states. note that in this paper, we treat the en(t)\\ngagement variable eu as a scalar; the extension of modeling\\nit as a vector and thus separating engagement by concept is\\npart of our ongoing work.\\nit is worth mentioning the similarity between our characterization of engagement as a latent variable in the learning\\nmodel and the input gate variables in long-short term memory (lstm) neural networks [18]. in lstm, the change\\nin the latent memory state (loosely corresponding to the\\n(t)\\nlatent concept knowledge state vector cu ) is given by the\\ninput vector (loosely corresponding to the video learning\\ngain vector dv ) modulated by a set of input gate variables\\n(t)\\n(corresponding to the engagement variable eu ).\\nparameter inference. our statistical model of learning\\nand response can be seen as a particular type of recurrent neural network (rnn). therefore, for parameter inference, we\\nimplement a stochastic gradient descent algorithm with standard backpropagation. given the graded learner responses\\n(t)\\n(t)\\nyu and behavioral features fu , our parameter inference\\n\\nalgorithm estimates the quiz question–concept association\\nvectors wv , the quiz question intrinsic difficulties µv , the the\\nvideo learning gain vectors dv , the learner initial knowledge\\n(0)\\nvectors cu , the learner abilities au , and the engagement–\\nbehavioral feature association vector β. we omit the details\\nof the algorithm for simplicity of exposition.\\n\\n4.\\n\\nexperiments\\n\\nin this section, we evaluate the proposed latent engagement\\nmodel on the fmb dataset. we first demonstrate the gain\\nin predictive quality of the proposed model over two baseline\\nalgorithms (section 4.1), and then show how our model can\\nbe used to study engagement (section 4.2).\\n\\n4.1\\n\\npredicting unobserved responses\\n\\nwe evaluate our proposed model’s quality by testing its\\nability to predict unobserved quiz question responses.\\nbaselines. we compare our model against two well-known,\\nstate-of-the-art response prediction algorithms that do not\\nuse behavioral data. first is the sparse factor analysis\\n(sparfa) algorithm [24], which factors the learner-question\\nmatrix to extract latent concept knowledge, but does not use\\na time-varying model of learners’ knowledge states. second is\\na version of the bayesian knowledge tracing (bkt) algorithm\\nthat tracks learners’ time-varying knowledge states, which\\nincorporates a set of guessing and slipping probability parameters for each question, a learning probability parameter\\nfor each video, and an initial knowledge level parameter for\\neach learner [13, 27].\\n\\n4.1.1\\n\\nexperimental setup and metrics\\n\\nregularization. in order to prevent overfitting, we add\\n`2 -norm regularization terms to the overall optimization\\nobjective function for every set of variables in both the\\nproposed model and in sparfa. we use a parameter λ to\\ncontrol the amount of regularization on each variable.\\ncross validation. we perform 5-fold cross validation on\\nthe full dataset (ω1,92 ), and on each subset of the dataset\\nintroduced in section 2.3 (ω20,92 and ω1,20 ). to do so, we\\nrandomly partition each learner’s quiz question responses\\ninto 5 data folds. leaving out one fold as the test set, we use\\nthe remaining four folds as training and validation sets to\\nselect the values of the tuning parameters for each algorithm,\\ni.e., by training on three of the folds and validating on the\\nother. we then train every algorithm on all four observed\\nfolds using the tuned values of the parameters, and evaluate\\nthem on the holdout set. all experiments are repeated for\\n20 random partitions of the training and test sets.\\nfor the proposed model and for sparfa, we tune both the68\\n\\n\\x0cfeature\\n\\ncoefficient\\n\\nfraction spent\\n\\n0.1941\\n\\nfraction completed\\n\\n0.1443\\n\\nfraction played\\n\\n0.2024\\n\\nfraction paused\\n\\n0.0955\\n\\nnumber of pauses\\n\\n0.2233\\n\\nnumber of rewinds\\nnumber of fast forwards\\naverage playback rate\\nstandard deviation of playback rate\\n\\n0.4338\\n−0.1551\\n0.2797\\n\\n0.0314\\n\\ntable 2: regression coefficient vector β learned over\\nthe full dataset, associating each clickstream feature\\nto engagement. all but one of the features (number\\nof fast forwards) is positively correlated with engagement.\\nnumber of concepts k ∈ {2, 4, 6, 8, 10} and the regularization parameter λ ∈ {0.5, 1.0, . . . , 10.0}. note that for the\\nproposed model, when a question response is left out as part\\nof the test set, only the response is left out of the training\\nset: the algorithm still uses the clickstream data for the\\ncorresponding learner-video pair to model engagement.\\nmetrics. to evaluate the quality of the algorithms, we\\nemploy two commonly used binary classification metrics:\\nprediction accuracy (acc) and area under the receiver operating characteristic curve (auc) [19]. the acc metric is\\nsimply the fraction of predictions that are made correctly,\\nwhile the auc measures the tradeoff between the true and\\nfalse positive rates of the classifier. both metrics take values\\nin [0, 1], with larger values indicating higher quality.\\n\\n4.1.2 results and discussion\\ntable 1 gives the evaluation results for the three algorithms.\\nthe average and standard deviation over the 20 random data\\npartitions are reported for each dataset group and metric.\\nfirst of all, the results show that our proposed model consistently achieves higher quality than both baseline algorithms\\non both metrics. it significantly outperforms bkt in particular (sparfa also outperforms bkt). this shows the\\npotential of our model to push the envelope on achievable\\nquality in performance prediction research.\\nnotice that our model achieves its biggest quality improvement on the full dataset, with a 1.3% gain in auc over\\nsparfa and a 5.7% gain over bkt. this observation suggests that as more clickstream data is captured and available\\nfor modeling – especially as we observe more video-watching\\nbehavioral data from learners over a longer period of time\\n(the full dataset ω1,92 contains clickstream data for up to\\n12 weeks, while the ω1,20 subset only contains data for the\\nfirst 2 weeks) – the proposed model achieves more significant\\nquality enhancements over the baseline algorithms. this\\nis somewhat surprising, since prior work on behavior-based\\nperformance prediction [8] has found the largest gains in the\\npresence of fewer learner-video pairs, i.e., before there are\\nmany question responses for other algorithms to model on.\\nbut our algorithm also benefits from additional question re-\\n\\n(t)\\n\\nfigure 3: plot of the latent engagement level ej\\nover time for one third of the learners in fmb, showing a diverse set of behaviors across learners.\\n\\nsponses, to update its learned relationship between behavior\\nand concept knowledge.\\nthe first two weeks of data (ω1,20 ) is sparse in that the\\nmajority of learners answer at most a few questions during\\nthis time, many of whom will drop out (see figure 1). in\\nthis case, our model obtains a modest improvement over\\nsparfa, which is static and uses fewer parameters. the\\ngain over bkt is particularly pronounced, at 5.7%. this,\\ncombined with the findings for active learners over the full\\ncourse (ω20,92 ), shows that observing video-watching behavior of learners who drop out of the course in its early states\\n(these learners are excluded from ω20,92 ) leads to a slight\\nincrease in the performance gain of the proposed model over\\nthe baseline algorithms. importantly, this shows that our\\nalgorithm provides benefit for early detection, with the ability\\nto predict performance of learners who will end up dropping\\nout [8].\\n\\n4.2\\n\\nanalyzing engagement\\n\\ngiven predictive quality, one benefit of our model is that it\\ncan be used to analyze engagement. the two parameters to\\nconsider for this are the regression coefficient vector β and\\n(t)\\nthe engagement scalar eu itself.\\nbehavior and engagement. table 2 gives each of the\\nestimated feature coefficients in β for the full dataset ω1,92 ,\\nwith regularization parameters chosen via cross validation.\\nall of the features except for the number of fast forwards are\\npositively correlated with the latent engagement level. this\\nis to be expected since many of the features are associated\\nwith processing more video content, e.g., spending more\\ntime, playing more, or pausing longer to reflect, while fast\\nforwarding involves skipping over the content.\\nthe features that contribute most to high latent engagement\\nlevels are the number of pauses, the number of rewinds, and\\nthe average playback rate. the first two of these are likely\\nindicators of actual engagement as well, since they indicate\\nwhether the learner was thinking while pausing the video\\nor re-visiting earlier content which contains knowledge that\\nthey need to recall or revise. the strong, positive correlation\\nof average playback rate is somewhat surprising though:\\nwe may expect that a higher playback rate would have a69\\n\\n\\x0c(a) learners that consistently exhibit\\nhigh engagement and finish the course.\\n\\n(b) learners that exhibit high engagement but drop out early.\\n(t)\\n\\nfigure 4: plot of the latent engagement level ej\\n\\nover time for selected learners in three different groups.\\n\\nnegative impact on engagement, like fast forwarding does, as\\nit involves speeding through content. on the other hand, it\\nmay be an indication that learners are more focused on the\\nmaterial and trying to keep their interest higher.\\nengagement over time. figure 3 visualizes the evolution\\n(t)\\nof eu over time for 1/3 of the learners (randomly selected).\\npatterns in engagement differs substantially across learners;\\nthose who finish the course mostly exhibit high engagement\\nlevels throughout, while those who drop out early vary greatly\\nin their engagement, some high and others low.\\nfigure 4 breaks down the learners into three different types\\naccording to their engagement patterns, and plots their engagement levels over time separately. the first type of learner\\n(a) finishes the course and consistently exhibits high engagement levels throughout the duration. the second type (b)\\nalso consistently exhibits high engagement levels, but drops\\nout of the course after up to three weeks. the third type of\\nlearner (c) exhibits inconsistent engagement levels before an\\nearly dropout. equipped with temporal plots like these, an\\ninstructor could determine which learners may be in need\\nof intervention, and could design different interventions for\\ndifferent engagement clusters [8, 36].\\n\\n5.\\n\\n(c) learners that exhibit inconsistent\\nengagement and drop out.\\n\\nconclusions and future work\\n\\nin this paper, we proposed a new statistical model for learning, based on learner behavior while watching lecture videos\\nand their performance on in-video quiz questions. our model\\nhas two main parts: (i) a response model, which relates a\\nlearner’s performance to latent concept knowledge, and (ii)\\na learning model, which relates the learner’s concept knowledge in turn to their latent engagement level while watching\\nvideos. through evaluation on a real-world mooc dataset,\\nwe showed that our model can predict unobserved question\\nresponses with superior quality to two state-of-the-art baselines, and also that it can lead to engagement analytics: it\\nidentifies key behavioral features driving high engagement,\\nand shows how each learner’s engagement evolves over time.\\nour proposed model enables the measurement of engagement\\nsolely from data that is logged within online learning platforms: clickstream data and quiz responses. in this way, it\\nserves as a less invasive alternative to current approaches\\nfor measuring engagement that require external devices, e.g.,\\ncameras and eye-trackers [6, 16, 35]. one avenue of future\\nwork is to conduct an experiment that will correlate our\\ndefinition of latent engagement with these methods.\\n\\nadditionally, one could test other, more sophisticated characterizations of the latent engagement variable. one such\\napproach could seek to characterize engagement as a function of learners’ previous knowledge level. an alternative or\\naddition to this would be a generative modeling approach of\\nengagement to enable the prediction of future engagement\\ngiven each learner’s learning history.\\none of the long-term, end-all goals of this work is the design\\nof a method for useful, real-time analytics to instructors. the\\ntrue test of this ability comes from incorporating the method\\ninto a learning system, providing its outputs – namely, performance prediction forecasts and engagement evolution – to\\nan instructor through the user interface, and measuring the\\nresulting improvement in learning outcomes.\\n\\nacknowledgments\\nthanks to debshila basu mallick for discussions on the\\ndifferent types of engagement.\\n\\n6.\\n\\nreferences\\n\\n[1] networks: friends, money, and bytes. https:\\n//www.coursera.org/course/friendsmoneybytes.\\n[2] l. allen, m. jacovina, m. dascalu, r. roscoe, k. kent,\\na. likens, and d. mcnamara. {enter}ing the time\\nseries {space}: uncovering the writing process\\nthrough keystroke analyses. in proc. intl. conf. educ.\\ndata min., pages 22–29, june 2016.\\n[3] a. anderson, s. christenson, m. sinclair, and c. lehr.\\ncheck & connect: the importance of relationships for\\npromoting engagement with school. j. school psychol.,\\n42(2):95–113, mar. 2004.\\n[4] a. anderson, d. huttenlocher, j. kleinberg, and\\nj. leskovec. engaging with massive online courses. in\\nproc. intl. conf. world wide web, pages 687–698, apr.\\n2014.\\n[5] r. baker, a. corbett, and k. koedinger. detecting\\nstudent misuse of intelligent tutoring systems. in proc.\\nintl. conf. intell. tutoring syst., pages 531–540, aug.\\n2004.\\n[6] r. bixler and s. d’mello. automatic gaze-based\\nuser-independent detection of mind wandering during\\ncomputerized reading. user model. user-adapt.\\ninteract., 26(1):33–68, mar. 2016.\\n[7] c. brinton, s. buccapatnam, f. wong, m. chiang, and\\nh. poor. social learning networks: efficiency\\noptimization for mooc forums. in proc. ieee conf.70\\n\\n\\x0ccomput. commun., pages 1–9, apr. 2016.\\n[8] c. brinton and m. chiang. mooc performance\\nprediction via clickstream data and social learning\\nnetworks. in proc. ieee conf. comput. commun.,\\npages 2299–2307, april 2015.\\n[9] c. brinton, r. rill, s. ha, m. chiang, r. smith, and\\nw. ju. individualization for education at scale: miic\\ndesign and preliminary evaluation. ieee trans. learn.\\ntechnol., 8(1):136–148, jan. 2015.\\n[10] h. cen, k. koedinger, and b. junker. learning factors\\nanalysis – a general method for cognitive model\\nevaluation and improvement. in proc. intl. conf. intell.\\ntutoring syst., pages 164–175, june 2006.\\n[11] l. chen, x. li, z. xia, z. song, l. morency, and\\na. dubrawski. riding an emotional roller-coaster: a\\nmultimodal study of young child’s math problem\\nsolving activities. in proc. intl. conf. educ. data min.,\\npages 38–45, june 2016.\\n[12] c. coleman, d. seaton, and i. chuang. probabilistic\\nuse cases: discovering behavioral patterns for\\npredicting certification. in proc. acm conf. learn at\\nscale, pages 141–148, mar. 2015.\\n[13] a. corbett and j. anderson. knowledge tracing:\\nmodeling the acquisition of procedural knowledge. user\\nmodel. user-adapt. interact., 4(4):253–278, dec. 1994.\\n[14] c. farrington, m. roderick, e. allensworth,\\nj. nagaoka, t. keyes, d. johnson, and n. beechum.\\nteaching adolescents to become learners: the role of\\nnoncognitive factors in shaping school performance–a\\ncritical literature review. consortium on chicago\\nschool research, 2012.\\n[15] b. gelman, m. revelle, c. domeniconi, a. johri, and\\nk. veeramachaneni. acting the same differently: a\\ncross-course comparison of user behavior in moocs. in\\nproc. intl. conf. educ. data min., pages 376–381, june\\n2016.\\n[16] m. gilzenrat, j. cohen, j. rajkowski, and\\ng. aston-jones. pupil dynamics predict changes in task\\nengagement mediated by locus coeruleus. in proc. soc.\\nneurosci. abs., page 19, nov. 2003.\\n[17] p. guo, j. kim, and r. rubin. how video production\\naffects student engagement: an empirical study of\\nmooc videos. in proc. acm conf. learn at scale,\\npages 41–50, mar. 2014.\\n[18] s. hochreiter and j. schmidhuber. long short-term\\nmemory. neural comput., 9(8):1735–1780, nov. 1997.\\n[19] h. jin and c. ling. using auc and accuracy in\\nevaluating learning algorithms. ieee trans. knowl.\\ndata eng., 17(3):299–310, mar. 2005.\\n[20] j. kim, p. guo, d. seaton, p. mitros, k. gajos, and\\nr. miller. understanding in-video dropouts and\\ninteraction peaks in online lecture videos. in proc.\\nacm conf. learn at scale, pages 31–40, mar. 2014.\\n[21] r. kizilcec, c. piech, and e. schneider. deconstructing\\ndisengagement: analyzing learner subpopulations in\\nmassive open online courses. in proc. intl. conf. learn.\\nanalyt. knowl., pages 170–179, apr. 2013.\\n[22] k. koedinger, j. kim, j. jia, e. mclaughlin, and\\nn. bier. learning is not a spectator sport: doing is\\nbetter than watching for learning from a mooc. in\\nproc. acm conf. learn at scale, pages 111–120, mar.\\n2015.\\n\\n[23] a. lan, c. studer, and r. baraniuk. time-varying\\nlearning and content analytics via sparse factor\\nanalysis. in proc. acm sigkdd intl. conf. knowl.\\ndiscov. data min., pages 452–461, aug. 2014.\\n[24] a. lan, a. waters, c. studer, and r. baraniuk. sparse\\nfactor analysis for learning and content analytics. j.\\nmach. learn. res., 15:1959–2008, june 2014.\\n[25] l. malkiewich, r. baker, v. shute, s. kai, and\\nl. paquette. classifying behavior to elucidate elegant\\nproblem solving in an educational game. in proc. intl.\\nconf. educ. data min., pages 448–453, june 2016.\\n[26] j. mcbroom, b. jeffries, i. koprinska, and k. yacef.\\nmining behaviours of students in autograding\\nsubmission system logs. in proc. intl. conf. educ. data\\nmin., pages 159–166, june 2016.\\n[27] z. pardos and n. heffernan. modeling individualization\\nin a bayesian networks implementation of knowledge\\ntracing. in proc. intl. conf. user model. adapt.\\npersonalization, pages 255–266, june 2010.\\n[28] j. reich, b. stewart, k. mavon, and d. tingley. the\\ncivic mission of moocs: measuring engagement across\\npolitical differences in forums. in proc. acm conf.\\nlearn at scale, pages 1–10, apr. 2016.\\n[29] m. san pedro, e. snow, r. baker, d. mcnamara, and\\nn. heffernan. exploring dynamical assessments of\\naffect, behavior, and cognition and math state test\\nachievement. in proc. intl. conf. educ. data min.,\\npages 85–92, june 2015.\\n[30] c. shi, s. fu, q. chen, and h. qu. vismooc:\\nvisualizing video clickstream data from massive open\\nonline courses. in ieee pacific visual. symp., pages\\n159–166, apr. 2015.\\n[31] s. slater, r. baker, j. ocumpaugh, p. inventado,\\np. scupelli, and n. heffernan. semantic features of\\nmath problems: relationships to student learning and\\nengagement. in proc. intl. conf. educ. data min.,\\npages 223–230, june 2016.\\n[32] s. tomkins, a. ramesh, and l. getoor. predicting\\npost-test performance from online student behavior: a\\nhigh school mooc case study. in proc. intl. conf.\\neduc. data min., pages 239–246, june 2016.\\n[33] a. vail, j. wiggins, j. grafsgaard, k. boyer, e. wiebe,\\nand j. lester. the affective impact of tutor questions:\\npredicting frustration and engagement. in proc. intl.\\nconf. educ. data min., pages 247–254, june 2016.\\n[34] x. wang, d. yang, m. wen, k. koedinger, and c. rosé.\\ninvestigating how student’s cognitive behavior in\\nmooc discussion forums affect learning gains. in proc.\\nintl. conf. educ. data min., pages 226–233, june 2015.\\n[35] j. whitehill, z. serpell, y. lin, a. foster, and\\nj. movellan. the faces of engagement: automatic\\nrecognition of student engagement from facial\\nexpressions. ieee trans. affect. comput., 5(1):86–98,\\njan. 2014.\\n[36] j. whitehill, j. williams, g. lopez, c. coleman, and\\nj. reich. beyond prediction: towards automatic\\nintervention in mooc student stop-out. in proc. intl.\\nconf. educ. data min., pages 171–178, june 2015.\\n[37] d. yang, r. kraut, and c. rosé. exploring the effect of\\nstudent confusion in massive open online courses. j.\\neduc. data min., 8(1):52–83, 2016.',\n",
       " '87\\n\\n\\x0cgeneralizability of face-based mind wandering detection\\nacross task contexts\\nangela stewart\\n\\nnigel bosch\\n\\nsidney k. d’mello\\n\\nuniversity of notre dame\\n384 fitzpatrick hall\\nnotre dame, in, 46556, usa\\n\\nuniversity of illinois at urbanachampaign\\n1205 west clark street\\nurbana, il, 61801, usa\\n\\nuniversity of notre dame\\n118 haggar hall\\nnotre dame, in, 46556\\n\\nastewa12@nd.edu\\n\\nsdmello@nd.edu\\n\\npnb@illinois.edu\\nabstract\\nwe investigate generalizability of face-based detectors of mind\\nwandering across task contexts. we leveraged data from two lab\\nstudies: one where 152 college students read a scientific text and\\nanother where 109 college students watched a narrative film. we\\nautomatically extracted facial expressions and body motion\\nfeatures, which were used to train supervised machine learning\\nmodels on each dataset, as well as a concatenated dataset. we\\napplied models from each task context (scientific text or narrative\\nfilm) to the alternate context to study generalizability. we found\\nthat models trained on the narrative film dataset generalized to the\\nscientific text dataset with no modifications, but the predicted mind\\nwandering rate needed to be adjusted before models trained on the\\nscientific text dataset would generalize to the narrative film dataset.\\nadditionally, we analyzed generalizability of individual features\\nand found that the lip tightener and jaw drop action units had the\\ngreatest potential to generalize across task contexts. we discuss\\nfindings and applications of our work to attention-aware learning\\ntechnologies.\\n\\nkeywords\\nmind wandering, mental states, attention aware interfaces,\\ncross-corpus training.\\n\\n1. introduction\\nconsider a typical day when you were an undergraduate college\\nstudent. your first class is your favorite, so you are engaged in the\\nlecture content and processing new information. in your next class,\\nyou watch a documentary about a subject that does not interest you,\\ncausing your attention to focus on unrelated thoughts of your social\\nlife, rather than processing the information in the video. later, you\\nwork on a homework assignment that you find frustrating, leading\\nto waning motivation. towards the end of your day, you attend a\\nchemistry lab, where you interact with a new educational game that\\nteaches you the basics of chemical bonds. at some points you are\\nenjoying the game, and thus engaged in deeply learning the content.\\nhowever, you later become bored during a long period of repetitive\\ngameplay, causing you to become distracted and miss important\\ninformation. throughout the day, your mental states (engagement,\\nfrustration, boredom) influenced your learning. your learning\\n\\nexperience could have been augmented with technology that\\nresponded to your changing mental state, thus assisting you in\\nachieving the most effective learning experience.\\neducational interfaces that detect and respond to student mental\\nstates are driven by work on cognitive and affective state modeling,\\nwhich has been investigated for many years. for example, attention\\nand affect has been modeled in educational tasks such as reading\\ncomprehension [6, 16, 28] and computerized tutoring [3, 19],\\namong others. in general, there has been a plethora of work that has\\nmodeled a variety of mental states within specific educational tasks\\n(e.g., [2, 15, 19]) to better understand these states and use that\\nknowledge to facilitate student learning.\\nhowever, prior research has overwhelmingly investigated single\\ntask contexts, and has overlooked generalizability to different\\ncontexts. for example, models that track attention during reading\\nmight not generalize to lecture viewing, educational gaming, and\\nso on. this makes it difficult to decouple task-specific effects from\\nmore fundamental patterns. in contrast, models that successfully\\ngeneralize across multiple contexts should reveal observable\\nsignals (i.e. eye gaze, facial features, and physiology data) that are\\ngeneral, rather than task-specific. models using such indicators will\\nbe key to developing adaptive technologies that are sensitive to\\nstudent mental states and that can operate across a range of\\neducational activities.\\nwe report results on modeling mental states in a generalized way\\nusing mind wandering (mw) as a case study. mw is a ubiquitous\\nphenomenon where thoughts shift from task-related processing to\\ntask-unrelated thoughts [15]. mw is estimated to occur anywhere\\nfrom 20% - 50% of the time, depending on the person, task, and\\nenvironmental context [23]. it is has also been associated with\\nlower performance on a variety of educational tasks, such as\\nreading comprehension [16] and retention of lecture content [29],\\nthus impacting student learning.\\nas with work on other mental states, research on mw has largely\\nfailed to address models that generalize across contexts [6, 15].\\nmw detection has been investigated in reading comprehension [6,\\n16], narrative and instructional film comprehension [25, 26], and\\nstudent interaction with an intelligent tutoring system (its) [19].\\nto our knowledge, no work has investigated mw detection with\\nthe goal of generalizability across task contexts.\\nwe specifically investigate the generalizability of mw models\\nacross two task contexts - reading a scientific text and viewing a\\nnarrative film. these contexts were chosen because of their broad\\napplicability to education in the classroom and online. for example,\\na documentary film could be shown in a sociology course or\\ndistance learning students could read instructional texts prior to\\nengaging in an online discussion.88\\n\\n\\x0c1.1 related work\\ncross corpus training has been researched in a variety of\\nclassification problems, such as sentiment analysis [31] and\\nacoustic-based emotion recognition [35]. cross corpus training\\nseeks to improve robustness of machine-learned models by\\nleveraging multiple datasets in classifier training and testing. for\\nexample, webb and ferguson [32] applied cross corpus training\\ntechniques to characterize the function of segments of dialogue\\nusing automatically extracted lexical and syntactic features called\\ncue phrases. each extracted cue phrase was used to classify a\\nsegment of dialogue. they trained separate classifiers on two\\ndifferent datasets, and applied the classifier to the dataset on which\\nit was not trained. they found the cross-training results were\\ncomparable to the results of training and testing on the same dataset\\n(e.g. the best cross-trained classifier achieved and accuracy of 71%,\\ncompared to an accuracy of 81% when trained and tested on the\\nsame dataset). additionally, they examined generalizability of the\\ncue phrases across datasets by reducing the feature set to contain\\nonly cues present in both datasets. they found that reducing the\\nfeature set yielded slight improvements, and demonstrated the\\ndiscriminative nature of a small number of features.\\nzhang et. al. [35] similarly explored the use of multiple datasets for\\ncreating context-generalizable models. they built classifiers for\\nvalence and arousal on highly varied emotional speech datasets\\nusing a leave-one-corpora-out cross-validation technique.\\nadditionally, they explored methods for data normalization (within\\neach dataset and between datasets) and agglomeration of both\\nlabeled and unlabeled data. they found that, of their six emotional\\nspeech corpora, training on some subsets yielded higher accuracy\\nthan others. their work suggested that careful selection of corpora\\nbest suited for training might yield better emotional speech\\nrecognition performance than an all-or-nothing approach to crosscorpus training.\\nour work approaches cross-corpus modeling through detection of\\nmw. a variety of studies have investigated mw detection during\\neducational tasks, such a reading [15], interacting with an\\nintelligent tutoring system (its) [19], or watching an educational\\nvideo [26]. no work has focused on mw from a cross-corpus\\nmodeling perspective, to our knowledge, so we review the\\nindividual studies below.\\ndetection of mw from eye gaze features while reading has been\\namply investigated. for example, bixler and d’mello [4] built\\nmodels to detect mw while students read texts about scientific\\nresearch methods. this work made use of probe-caught reports\\n(students respond yes or no to auditory thought probes of whether\\nthey were mw), instead of self-caught reports (students report\\nwhenever they catch themselves mw). their analysis of eye gaze\\nfeatures showed that certain types of fixations were longer during\\nmw. specifically, they found that longer gaze fixations\\n(consecutive fixations on a single word), first-pass fixations\\n(fixations on a word during the first pass through a text), and single\\nfixations (fixations on a word only fixated on once) were predictive\\nof mw. in other work, bixler and d’mello [5] similarly used eye\\ngaze features, but used self-caught reports of mw. they found that\\na greater number of fixations, longer saccade length, and line cross\\nsaccades were indicative of mw. across studies on mw detection\\nduring reading, longer fixations were found to be indicative of mw\\n[4, 15, 28], suggesting these features might generalize well.\\n\\nmonitoring fingertip blood flow, using the back camera of a\\nsmartphone (i.e., photoplethysmography). their models achieved a\\n22% improvement over chance. although their method for\\ndetecting mw could be implemented across a variety of tasks, the\\nquestion of whether heart rate is indicative of mw across task\\ncontexts has not yet been investigated.\\nhutt et. al. provided limited evidence of generalizability of mw\\ndetection across different learning tasks during student interaction\\nwith an its [19]. they employed a genetic algorithm to train a\\nneural network using context-independent eye-gaze features and\\ncontext-dependent interaction features (e.g., current progress\\nwithin the its). they achieved an f1 value of .490 (chance = .190).\\nthis work provided some evidence of generalizability because the\\nvisual stimuli and interaction patterns varied throughout. for\\nexample, students interacted with an animated pedagogical agent in\\na scaffolded dialogue phase and completed concept maps without\\nthe tutoring agent in another interaction phase. however, it is still\\nunclear if their model would generalize to a broader range of tasks,\\nparticularly less interactive ones like reading or film viewing.\\nfurthermore, their best-performing models used context-dependent\\nfeatures, which could prevent the detector from generalizing to a\\ntask where those features could not be used.\\n\\n1.2 novelty\\nour contribution is novel in a variety of ways. first, we demonstrate\\nthe feasibility of building cross-context detectors of mental states,\\nspecifically mw. further, previous work on mw detection has\\nsometimes made use of context-specific features (e.g., reading\\ntimes) that are not expected to generalize to other contexts [19, 25].\\nin contrast, our work detects mw using only facial features and\\nupper body movement, recorded using commercial-off-the-shelf\\n(cots) webcams that are expected to generalize more broadly.\\nadditionally, the use of cots webcams support a broader\\nimplementation of mw detectors as webcams are ubiquitous in\\nmodern technology. this is in contrast to prior research that has\\nused specialized equipment, like eye trackers [15, 19, 25] or\\nphysiology sensors [7], which students would likely not have\\naccess to.\\n\\n2. datasets\\nthis study makes use of narrative film [23] and scientific reading\\ncomprehension [22] datasets collected as part of a larger project.\\nhere, we include details pertaining to video-based detection of\\nmw.\\n\\n2.1 narrative film comprehension\\nparticipants were 68 undergraduate students from a medium-sized\\nprivate midwestern university and 41 undergraduate students from\\na large public university in the southern united states. of the 109\\nstudents, 66% were female and their average age was 20.1 years.\\nstudents were compensated with course credit. data from four\\nstudents were discarded due to equipment failure.\\nstudents viewed the narrative film the red balloon (1956), a 32.5minute french-language film with english subtitles (figure 1). the\\nfilm has a musical score but only sparse dialogue. this short fantasy\\nfilm depicts the story of a young parisian boy who finds a red\\nhelium balloon and quickly discovers it has a mind of its own as it\\nfollows him wherever he goes. this film was selected because of\\nthe low likelihood that participants have previously seen it and\\nbecause it has been used in other film comprehension studies [34].\\n\\npham and wang [26] similarly used consumer-grade equipment to\\ndetect mw while students watched videos from massively open\\nonline courses (moocs). they made use of heart rate, detected by89\\n\\n\\x0cfigure 1. a screenshot of the narrative film (left) and scientific text (right) are shown.\\nstudents’ faces and upper bodies were recorded with a low-cost\\n($30) consumer-grade webcam (logitech c270).\\nstudents were instructed to report mw throughout the film by\\npressing labeled keys on the keyboard. specifically, students were\\nasked to report a task-unrelated thought if they were “thinking\\nabout anything else besides the movie” and a task-related\\ninterference if they were “thinking about the task itself but not the\\nactual content of the movie.” a small beep sounded to register their\\nreport, but film play was not paused. after viewing the film,\\nstudents took a short test about the content and completed\\nadditional measures not discussed further.\\nwe recorded a total of 1,368 mw reports from the 105 participants\\nwith valid video recordings. in this work, we do not distinguish\\nbetween the two types of mw, instead merging the task-unrelated\\nthoughts and the task-related interferences, both of which represent\\nthoughts independent of the content of the film.\\n\\n2.2 scientific reading comprehension\\nparticipants were 104 undergraduate students from a medium-sized\\nprivate midwestern university and 48 undergraduate students from\\na large public university in the southern united states. of the 152\\nparticipants, 61% were female and their average age was 20.1\\nyears. participants were compensated with course credit. data from\\neight participants were discarded due to equipment failure.\\nstudents read an excerpt from soap-bubbles and the forces which\\nmould them [8]. like the red balloon (figure 1), we chose this\\ntext because its content would likely be unfamiliar to a majority of\\nreaders. the text contained around 6,500 words from the first\\nchapter of the book. in all, 57 pages (screens of text) with an\\naverage of 115 words each were displayed on a computer screen in\\n36-pt courier new typeface. the only modification to the text was\\nthe removal of images and references to them after verifying that\\nthese were not needed for comprehension.\\nstudents who read the scientific text were instructed to report mw\\nin the same way as those who watched the narrative film. they were\\ninstructed to report a task-unrelated thought if they were “thinking\\nabout anything else besides the task” and a task-related interference\\nif they were “thinking about the task itself but not the actual content\\nof the text.” participants completed a comprehension assessment\\nafter reading the text. we recorded a total of 3,168 mw reports\\nfrom the 144 students with valid video recordings.\\n\\n2.3 self reports of mw\\nmw was measured via self-reports in both studies, so it is prudent\\nto discuss the validity of self-reports. we used self-reports because\\n\\nthis is currently the most common approach to measure an\\ninherently internal (but conscious) phenomenon [5, 15]. selfreported mw has been linked to predictable patterns in physiology\\n[30], pupillometry [17], eye-gaze [28] and task performance [27],\\nproviding evidence for the convergent and predictive validity for\\nthis approach. to improve the quality of self-reports, we\\nencouraged students to report honestly and assured them that\\nreporting mw would not in any way effect the credit they received\\nfor participation.\\nthe alternative to using self-caught reports is using probe-caught\\nreports, which require a student response to a thought-probe (e.g.,\\na beep). we chose self-caught reports over the probe-caught\\nbecause the probe-caught method can potentially interrupt the\\ncomprehension process (i.e., when participants report “no” to the\\nprobes). interruptions are particularly problematic in the film\\ncomprehension task, as participants did not have control over the\\nmedia presentation (i.e., no pausing or rewinding of the film).\\nfurthermore, it is also unclear if a probe-caught report takes place\\nat the beginning or end of mw, or somewhere in between.\\nconversely, self-caught reports are likely to occur at the end of a\\nmw episode when the student became aware that they were not\\nattending to the task at hand.\\n\\n3. machine learning\\nwe explored a variety of machine learning techniques for crosscontext mw detection using the same approach to segmenting\\ninstances and constructing features for both datasets.\\n\\n3.1 segmenting instances\\nreports of mw were distributed throughout the course of the film\\nviewing or text reading session. we created instances that\\ncorresponded to reports of mw by first adding a 4-second offset\\nprior to the report. this was done to ensure that we captured\\nparticipants’ faces while mw vs. in the act of reporting mw itself\\n(i.e., the preparation and execution of the key press). this 4-second\\noffset was chosen based on four raters judgements of whether or\\nnot movement related to the key-press could be seen within offsets\\nranging from 0 to 6 seconds. data was then extracted from the 20\\nseconds prior to the mw report. a window size of 20 seconds was\\nchosen based on prior experimentation that sought to balance\\ncreating as many instances as possible (shorter window sizes) and\\nhaving sufficient data in each window (longer window sizes) to\\ndetect mw.\\nwe extracted “not mw” instances from windows of data between\\nmw reports. the entire session (reading or video watching) was\\ndivided into 24-second segments (20 second windows of data and\\na 4 second offset as with the mw segments). any segments90\\n\\n\\x0coverlapping the 30 seconds prior to a mw report were discarded.\\nwe do not know precisely when mw starts, so we chose to discard\\ninstances overlapping the 30 seconds prior to mw reports, to\\nseparate students when they were actually mw from when they\\nwere not. we also discarded any segments overlapping a page turn\\n(discussed in section 3.2). all remaining segments were labeled\\nnot mw. our approach to segmenting instances is shown in figure\\n2.\\n\\ntable 1. an accounting of instance selection process\\n\\nbase\\n\\nreading\\n(% mw)\\n7,267 (30%)\\n\\nfilm\\n(% mw)\\n7,313 (14%)\\n\\nface detected\\n\\n7,266 (30%)\\n\\n7,238 (14%)\\n\\npage boundary\\n\\n1,400 (36%)\\n\\nn/a\\n\\nparticipant matching\\n\\n1,273 (35%)\\n\\nn/a\\n\\ndownsampling\\n\\n1,100 (25%)\\n\\n1,100 (25%)\\n\\n3.3 feature extraction and selection\\n\\nfigure 2. illustration of the instance extraction method.\\n\\n3.2 instance selection\\na full accounting of the instance selection process is shown in\\ntable 1. our goal was to make the two data sets as similar as\\npossible so that task-specific effects could be studied without\\nadditional confounds.\\nwe first discarded any instances where there was less than one\\nsecond of usable data in that time window. data was not usable\\nwhen the student’s face was occluded due to extreme head pose or\\nposition, hand-to-face gestures, and rapid movements.\\nadditionally, for the scientific reading dataset, we discarded\\ninstances that overlapped with page turn events. in prior\\nexperimentation, we trained a model to detect mw using only a\\nbinary feature of whether or not that instance overlapped a page\\nturn boundary. mw was detected at rates above chance in this\\nexperimental model. therefore, we concluded that including\\ninstances that overlapped page turn boundaries would inflate\\nperformance as the detector could simply be picking up on the act\\nof pressing the key to advance to the next page.\\n\\nwe used commercial software, the emotient sdk [36] to extract\\nfacial features. the emotient sdk, a version of the cert\\ncomputer vision software [24] (figure 3) provides likelihood\\nestimates of the presence of 20 facial action units (aus; specifically\\n1, 2, 4, 5, 6 ,7, 9, 10, 12, 14, 15, 17, 18, 20, 23, 24, 25, 26, 28, and\\n43 [14]) as well as head pose (orientation), face position (horizontal\\nand vertical within the frame), and face size (a proxy for distance\\nto camera). additionally, we used a validated motion estimation\\nalgorithm to compute gross body movements [33]. body movement\\nwas calculated by measuring the proportion of pixels in each video\\nframe that differed by a threshold from a continuously updated\\nestimate of the background image generated from the four previous\\nframes.\\n\\nfigure 3. interface demonstrating au estimates detected from\\na face video.\\n\\nafter discarding instances using the method above, we matched the\\nscientific reading and narrative film datasets on school (mediumsized midwestern private university or large southern public\\nuniversity), reported ethnicity, and reported gender. the scientific\\nreading dataset was randomly downsampled to contain\\napproximately the same number of students in each gender, race, or\\nschool category, as the film dataset. this participant-level matching\\non school, ethnicity, and gender was done to eliminate external\\nsources of variance that could influence mw detection, potentially\\nobfuscating task effects from population effects.\\n\\nfeatures were created by aggregating emotient estimates in a\\nwindow of time leading up to each mw or not mw instance using\\nminimum, maximum, median, mean, range, and standard deviation\\nfor aggregation. in all, there were 162 facial features (6 aggregation\\nfunctions × [20 aus + 3 head pose orientation axes + 2 face\\nposition coordinates + face size + motion]). outliers (values greater\\nthan three standard deviations from the mean) were replaced by the\\nclosest non-outlier value in a process called winsorization [11].\\n\\nfinally, the datasets were downsampled to contain equal numbers\\nof instances because the size of the training set is known to bias\\nclassifier performance [13]. we also downsampled the data to\\nachieve a 25% mw rate in order to be consistent with research that\\nsuggests that mw occurs between 20% and 30% of the time during\\nreading and film comprehension [6, 23]. further, the mw rates of\\n30% and 14% obtained in these data are more artefacts of the\\ninstance segmentation approach rather than the objective rate, so\\nresampling ensures a dataset that is more reflective of expected\\nmw rates.\\n\\nwe used tolerance analysis to eliminate features with high\\nmulticollinearity (variance inflation factor > 5) [1], after which, 37\\nfeatures remained. this was followed by relief-f [21] feature\\nselection (on the training data only) to rank features. we retained a\\nproportion of the highest ranked features for use in the models\\n(proportions ranging from .05 to 1.0 were tested). feature selection\\nwas performed using nested cross-validation on training data only.\\nwe ran 5 iterations of feature selection within each cross-validation\\nfold (discussed below), using data from a randomly chosen 67% of\\nstudents within the training set in each iteration.\\n\\n3.4 supervised classification and validation\\ninformed by preliminary experiments, we selected seven classifiers\\nfor more extensive tests (naïve bayes, simple logistic regression,\\nlogitboost, random forest, c4.5, stochastic gradient descent,\\nand classification via regression) using the weka data mining91\\n\\n\\x0ctoolkit [18]. for each classifier, we applied smote [9] to the\\ntraining set only. smote, a common machine learning technique\\nfor dealing with data imbalance, creates synthetic interpolated\\ninstances of the minority class to increase classification\\nperformance.\\nwe evaluated the performance of our classifiers using leave-oneparticipant-out cross-validation. this process runs multiple\\niterations of each classifier in which, for each fold, the instances\\npertaining to a single participant are added to the test set and the\\ntraining set is comprised of the instances for the other participants.\\nfeature selection was performed on a subset of participants in the\\ntraining set. the leave-one-out process was repeated for each\\nparticipant, and the classifications of all folds were weighted\\nequally to produce the overall result. this cross-validation\\napproach ensured that in each fold, data from the same participant\\nwas in the training set or testing set but never both, thereby\\nimproving generalization to new participants.\\naccuracy (recognition rate) is a common measure to evaluate\\nperformance in machine learning tasks. however, any classifier\\nthat defaults to predicting the majority class label of an imbalanced\\ndataset can appear to have high accuracy despite incorrect\\npredictions of all instances of the minority class label [20]. this is\\nparticularly detrimental in applications where detecting the\\nminority class is of upmost importance. in our task, we prioritized\\nthe detection of mw despite the large imbalance in our dataset.\\ntherefore, we considered the f1 score for the mw label as our key\\nmeasure of detection accuracy since f1 attempts to strike a balance\\nbetween precision and recall.\\n\\n4. results\\n4.1 cross-dataset training and testing\\nwe trained three classifiers: one on the scientific text dataset, one\\non the narrative film dataset, and one on a concatenated dataset\\ncomprised of the first two. for each of the three training sets, the\\nclassifier that yielded the highest mw f1 is shown in table 2. we\\nused leave-one-student-out cross validation for within-dataset\\nevaluations. conversely, to measure generalizability of the models\\nacross contexts we applied the classifier trained on scientific text\\ndata to the narrative film data, and vice versa. we compared our\\nmodel to a chance model that classified a random 25% (mw prior\\nproportion) of the instances as mw. this chance-level method\\nyielded a precision and recall of .250 (equal to the mw base rate).\\ntable 2. results for the models with highest mw f1 for the\\nwithin-data set validation (cross-training results in\\nparentheses).\\ntraining set\\n\\nclassifier mw f1\\n\\nprecision\\n\\nrecall\\n\\nscientific text logitboost .441 (.267) .376 (.252)\\n\\n.553 (.284)\\n\\nnarrative film c4.5\\n\\n.436 (.407) .303 (.278)\\n\\n.775 (.760)\\n\\nboth\\n\\n.424\\n\\n.655\\n\\nlogistic\\n\\n.314\\n\\nwe calculated improvement over chance as (actual performance –\\nchance)/(perfect performance – chance). all three models showed\\nimprovement over chance (25% for scientific text, 25% for\\nnarrative film, and 23% for the concatenated dataset) when trained\\nand tested on the same dataset. when tested on the alternative\\ndataset, the narrative film classifier generalized well to the\\nscientific text dataset (21% improvement over chance). however,\\nthe scientific text model showed chance-level performance on the\\nnarrative film corpus (2% improvement over chance). the mw f1\\n\\nof the concatenated dataset model was simply an average of the\\nmw f1 score of the individual datasets when the instance\\npredictions of the individual datasets are separated (.413 for the\\nscientific reading dataset and .436 on the narrative film dataset).\\nthese results showed that the concatenated classifier does not skew\\ntowards predicting one dataset better than the other, but rather\\npredicts both models with comparable accuracy.\\ntable 2 also shows precision and recall for each of the models.\\nacross all models, recall was higher than precision, indicating a lot\\nfalse positives. it is important to note the near chance-level recall\\nand precision of the model trained on scientific reading data when\\napplied to the narrative film data. the lack of improvement over\\nchance for both recall and precision demonstrated the need to\\nimprove generalizability in both dimensions. conversely, the crosstrained narrative film model had lower precision, but good recall,\\nresulting in an improved mw f1 score.\\n\\n4.2 classifier generalizability\\nto address the negligible improvement over chance of the scientific\\ntext model when tested on the narrative film dataset, we repeated\\nthe training and testing using c4.5 as the classifier. the c4.5\\nclassifier was chosen because it generalized better when trained on\\nthe narrative film dataset than the logitboost classifier generalized\\nwhen trained on the scientific text dataset. the results are shown in\\ntable 3, where we note no notable improvement over the previous\\nlogitboost classifier in table 2 (change from .267 to .287 when\\ntested on the narrative film dataset). therefore, the lack of evidence\\nfor generalizability for the scientific text model could be due to\\noverfitting to the training set, rather than classifier selection.\\ntable 3. results (mw f1) for the c4.5 classifier for withinand cross- validation.\\ntraining set\\n\\nwithin\\n\\ncross\\n\\nscientific text\\n\\n0.425\\n\\n0.287\\n\\nnarrative film\\n\\n0.436\\n\\n0.407\\n\\nboth\\n\\n0.415\\n\\nn/a\\n\\n4.3 prediction threshold adjustment\\nwe further investigated the lack of generalizability of the scientific\\ntext model by considering the mw prediction rate. we compared\\nthe performance of both models on the narrative film dataset. recall\\ndropped considerably more than precision (table 2; recall dropped\\nfrom .775 to .284; precision decreased from .303 to .252). we\\nhypothesized that recall decreased because of a difference in\\npredicted mw rates (table 4). in fact, the predicted mw rate in the\\nnarrative film data dropped from 64% to 28% when applying the\\nscientific text model to the same data. this supported our\\nhypothesis that the low recall was linked to lower predicted mw\\nrates. furthermore, 39% of the correctly classified instances (true\\npositives and true negatives) were mw when applying the narrative\\nfilm model to the narrative film data compared to 12% for the\\nscientific text model applied to the same data. this demonstrated\\nthat the scientific text model was much more prone to missing mw\\ninstances, further supporting our hypothesis.\\nto address this, we adjusted the predicted mw rate of the scientific\\ntext model when applied to the narrative film dataset. the classifier\\noutputs a likelihood of mw and we previously considered instances\\nwith likelihoods greater than .5 as mw. we adjusted that prediction\\nthreshold from .1 to 1 in increments of .1 (figure 4) to investigate\\nhow changes in predicted mw rate (higher for lower thresholds)\\neffected recall, and thus mw f1.92\\n\\n\\x0cto rank the subsets of features on generalizability, we examined\\nmw f1 scores when testing on the alternative dataset only. for\\nexample, using the au9 (nose wrinkle) subset, we investigated\\nmw f1 value of scientific text model applied to the narrative film\\ndataset and the narrative film model applied to the scientific text\\ndataset. table 4 shows these results only for features that achieved\\na mw f1 of greater than .250 (chance) on all dimensions (within\\ndataset validation and cross-training). we selected features for\\nfurther analysis if their mw f1 was greater than .300 for both crosstraining results. this value of .300 was used to filter out features\\nthat performed well on the within-dataset validation, but fell short\\non cross training. it also ensured that a feature performed better\\nthan chance on both cross-trained results (i.e., train on narrative\\nfilm and test on scientific text, and vice versa), rather than only\\ngeneralizing to one dataset. using this criterion, only au23 and\\nau26 showed notable improvement over chance.\\n\\ntable 4. predicted mw rates.\\ntraining set\\n\\nwithin\\n\\ncross\\n\\nscientific text\\n\\n38%\\n\\n28%\\n\\nnarrative film\\n\\n64%\\n\\n68%\\n\\nboth\\n\\n52%\\n\\nn/a\\n\\nfigure 4. mw precision, recall, and f1 as the prediction\\nthreshold varies for the scientific text model applied to the\\nnarrative film dataset.\\nwe note that mw f1 score degrades at a threshold of .5. we\\nadjusted the threshold to .3 and yielded the results shown in table\\n5. after adjusting the mw prediction threshold, both precision and\\nrecall of the narrative film data applied to the scientific text model\\nshowed comparable performance to the cross-trained narrative film\\nmodel. it is important to note that the adjusted mw prediction\\nthreshold yielded a predicted mw rate of 76%, much higher than\\nthe mw rate of the dataset (25%). as with the generalized narrative\\nfilm model, this reduced precision because the high predicted mw\\nrate produced a large number of false positives.\\ntable 5. results for models with highest mw f1 (crosstraining results in parentheses). cross-training results for the\\nscientific text model reflect a mw prediction threshold of .3.\\ntraining set\\n\\nclassifier mw f1\\n\\nprecision\\n\\n.553 (.836)\\n\\nnarrative film c4.5\\n\\n.436 (.407) .303 (.278)\\n\\n.775 (.760)\\n\\nboth\\n\\n.424\\n\\n.655\\n\\n.314\\n\\ntable 6. mw f1 score for within-data set validation with\\ncross-data set scores (in parentheses).\\nfacial feature\\nau4 (brow lowerer)\\nau6 (cheek raiser)\\nau9 (nose wrinkler)\\nau14 (dimpler)\\nau23 (lip tightener)\\nau26 (jaw drop)\\nface height (size)\\nface x (position)\\n\\ntraining set\\nscientific text narrative film\\n.378 (.278)\\n.398 (.395)\\n.369 (.259)\\n.361 (.321)\\n.300 (.268)\\n.392 (.303)\\n.303 (.267)\\n.383 (.376)\\n.334 (.333)\\n.363 (.317)\\n.414 (.321)\\n.365 (.357)\\n.322 (.256)\\n.339 (.289)\\n.404 (.316)\\n.382 (.282)\\n\\nrecall\\n\\nscientific text logitboost .441 (.416) .376 (.276)\\n\\nlogistic\\n\\nwe used the c4.5 classifier to generate the same models in table 2\\n(train/test scientific text, train scientific text/test narrative film, etc.)\\nusing only the features from au23 and au26 (table 7). none of\\nthese models (scientific text, narrative film, or concatenated)\\nachieved a mw f1 as high as those in table 2, which used a\\ncombination of tolerance analysis and relief-f to select features.\\nthis suggested that, while au23 and au26 might individually\\npredict mw, when used together, their prediction power might be\\nlimited, compared to other feature selection techniques.\\n\\n4.4 feature analysis\\nwe analyzed the facial features to further study generalizability by\\npredicting mw with different subsets of the entire feature set. the\\nc4.5 classifier was chosen for this feature analysis because of its\\nconsistency on both the scientific text model and concatenated\\ndataset. each subset consisted of the features (e.g., median,\\nstandard deviation) from one au, or from face position, size,\\norientation, or motion. since tolerance analysis was not used here,\\nwe only considered the minimum, maximum, median, and standard\\ndeviation aggregated features to prevent redundancy (e.g., between\\nmedian and mean). for example, we used the minimum, maximum,\\nmedian, and standard deviation feature values for au5 (upper lid\\nraiser) to predict mw. this approach was applied to the 20 au\\nsubsets, as well as face position, size, orientation, and motion\\nsubsets. we generated the same cross-training configurations of in\\nsection 4.1 (i.e., train on scientific text, test on narrative film, etc.).\\n\\ntable 7. results for models when only using the c4.5 classifier\\non au23 and au26.\\ntraining set\\n\\nclassifier mw f1\\n\\nprecision\\n\\nrecall\\n\\nscientific text c4.5\\n\\n.383 (.272) .255 (.206)\\n\\n.764 (.404)\\n\\nnarrative film c4.5\\n\\n.397 (.257) .333 (.235)\\n\\n.491 (.284)\\n\\nboth\\n\\n.368\\n\\n.575\\n\\nc4.5\\n\\n.271\\n\\n5. analysis\\nwe developed automated detectors of mw using video-based\\nfeatures in the contexts of narrative film viewing and scientific\\nreading. the generalizability of these models was dependent on\\ncorpora on which the model was trained and the rate at which the\\nmodel predicts mw. in this section, we discuss our main findings\\nand applications of this work. we also discuss limitations and\\nfuture work.\\n\\n5.1 main findings\\nwe expanded on previous mw detection work through crosscontext modeling. we trained three models on three datasets93\\n\\n\\x0c(scientific text, narrative film, and a dataset concatenated from the\\ntwo). we found each of these models (trained and tested on the\\nsame corpus) performed at a notable 23% to 25% improvement\\nover chance. this demonstrated the feasibility of detecting mw on\\nindividual corpora. however, recall was greater than precision,\\nindicating prediction of false positives. this should be considered\\nwhen implementing mw detectors in educational environments\\nwhere excessive prediction of student mw could be demotivating.\\n\\nlearning (both in the classroom and online). for example, films can\\ngive historical background on a time period being discussed in\\nliterature classes and instructional texts can supplement lecture\\ncontent through textbooks or technical articles. due to the\\nrelationship between mw and low task performance, user\\ninterfaces that detect and respond to mw in contexts where\\nattention is key (i.e. education) would help students remain focused\\non their learning.\\n\\nwe investigated generalizability of the single-dataset models (i.e.\\nscientific text or narrative film) by applying the model to the dataset\\non which it was not trained. the model trained on the narrative film\\ndataset maintained performance when applied to the scientific text\\ndataset (table 2), providing some evidence for generalizability, but\\nthis performance was boosted by high recall (and comparatively\\nlow precision). precision and recall (and thus mw f1) were near\\nchance-level when the model trained on the scientific text dataset\\nwas applied to the narrative film dataset, suggesting that the model\\nmight overfit to the scientific text training set.\\n\\nthese findings are particularly promising for implementation in\\nmassively open online courses (moocs). our method for detecting\\nmw exclusively uses cots webcams. these webcams are\\nubiquitous in today’s computers and mobile devices; thus our work\\nwould integrate into a variety of learning environments without\\nextra cost. such a video-based detector of mw could feasibly\\nrespond to student mw through suggesting a student revisit text or\\nvideo content, asking a reengaging question, or advising the student\\nto take a break.\\n\\nwe attempted to address this problem by applying the c4.5\\nclassifier, as it comparatively generalized well when trained on the\\nnarrative film dataset. mw f1 score for the scientific text classifier\\napplied to the narrative film data again negligibly increased. this\\nsuggested that the training data (only scientific text) used was not\\nappropriate for model generalization. this idea is supported by the\\nperformance of the narrative film model on the scientific text data\\n(although detection of false positives is a limitation) and the notable\\nimprovement over chance (22% to 23%) for the concatenated\\ndataset. the performance of both models suggested that there were\\ndiscernable similarities between mw instances across the two\\ndatasets, which can be detected using our techniques.\\nin addition to training data, we also found that predicted mw rate\\neffected model generalizability. we adjusted mw predictions\\naccording to a sliding threshold for the narrative film predictions\\nobtained from the scientific text model. we found that relaxing the\\ncriteria for classifying an instance as mw (i.e. adjusting the\\nlikelihood prediction threshold from .5 to .3) yielded results\\ncomparable to the cross-trained narrative film model. however, this\\napproach to increasing recall should be used with caution as it leads\\nto increased likelihood of false positives. perhaps in a real-time\\nmw intervention scenario, a more balanced approach could be\\ntaken where the mw likelihood prediction is used to determine if a\\nmw intervention is triggered (e.g., if the detector determines there\\nis a 40% likelihood the student is mw, then there is a 40% chance\\na mw intervention is triggered).\\n\\n5.3 limitations and future work\\nwhile we demonstrated techniques for modeling generalizability\\nacross task contexts, our work has a few limitations. first, precision\\nis moderate, even on our best models. high predicted mw rates\\nlead to high recall, but also more false positives. in this work, we\\nchose to accept this tradeoff, with the goal of generalizability in\\nmind. however, raising precision, while maintaining recall is key\\nto task-generalizable mw detectors being successful in educational\\nenvironments. since mw is the minority class (25% of all\\ninstances), investigating skew-insensitive classifiers, such as\\nhellinger distance decision trees [10], could improve precision.\\nadditionally, this work focuses exclusively on generalizability\\nfrom the perspective of task context (viewing a narrative film vs.\\nreading a scientific text). claims of generalizability could be\\nstrengthened through mw detection across environments. both the\\nnarrative film and scientific reading datasets were collected in a\\ncontrolled lab setting. mw detection in the field, such as computerenabled classrooms or the personal workstations of mooc users,\\nshould be considered prior to implementation in such\\nenvironments. furthermore, student generalizability should be\\nfurther examined. in this work, we detect mw in a studentindependent way. however, participants were all of similar age and\\nenrolled in college. future work could examine the generalizability\\nof our method for detecting mw in non-college-aged students, such\\nas elementary students in a computer-enabled classroom or nontraditional students enrolled in distance learning courses.\\n\\nwe detected mw using individual feature subsets to ascertain\\nwhether certain face-based features (i.e. aus, head orientation,\\nposition, size, and motion) generalize. we found two feature\\nsubsets (au23 – lip tightener and au26 – jaw drop) that showed a\\nmw f1 of at least .300 on both cross-trained models. it is notable\\nthat when looking at the generalizability of these features, they did\\nnot individually achieve mw f1 scores as high as the best\\nperforming models in table 2. this demonstrated the need for\\nmultiple features to work together to detect mw, rather than relying\\non a single feature. furthermore, this showed that our method of\\nfeature selection (tolerance analysis and selecting a proportion of\\nfeatures using relieff) was important to model performance.\\n\\n5.4 concluding remarks\\n\\n5.2 applications\\n\\nthis research was supported by the national science foundation\\n(nsf) (drl 1235958 and iis 1523091). any opinions, findings\\nand conclusions, or recommendations expressed in this paper are\\nthose of the authors and do not necessarily reflect the views of the\\nnsf.\\n\\nthe present findings are applicable to educational user interfaces\\nthat involve reading or film comprehension. monitoring and\\nresponding to mw could greatly improve student performance on\\nthese tasks. films and instructional texts play a major role in\\n\\nin this work, we showed evidence that generalizable detectors of\\nmw can be created using video-based features. the corpora used\\nto train models of mw and predicted mw rates both play a role in\\nthe model’s ability to generalize and should be considered as work\\non cross-context mw generalization advances. this work advances\\nthe field of attention-aware interfaces [12] by demonstrating the\\nfeasibility of modeling mw across the educational contexts of\\nreading a scientific text and viewing a narrative film. our approach\\nto detecting mw is the first step towards building interfaces that\\ndetect mw across multiple educational activities.\\n\\n6. acknowledgments94\\n\\n\\x0c7. references\\n[1]\\n[2]\\n\\n[3]\\n\\n[4]\\n\\n[5]\\n\\n[6]\\n\\n[7]\\n\\n[8]\\n[9]\\n\\n[10]\\n\\n[11]\\n\\n[12]\\n\\n[13]\\n\\n[14]\\n[15]\\n\\n[16]\\n\\n[17]\\n\\n[18]\\n\\n[19]\\n\\nallison, p.d. 1999. multiple regression: a primer. pine\\nforge press.\\nbaker, r.s. et al. 2012. towards automatically detecting\\nwhether student learning is shallow. international\\nconference on intelligent tutoring systems (chania, crete,\\ngreece, 2012), 444–453.\\nbaker, r.s. et al. 2012. towards sensor-free affect detection\\nin a cognitive tutor for algebra. educational data mining\\n(chania, crete, greece, 2012).\\nbixler, r. and d’mello, s. 2016. automatic gaze-based\\nuser-independent detection of mind wandering during\\ncomputerized reading. user modeling and user-adapted\\ninteraction. 26, 1 (2016), 33–68.\\nbixler, r. and d’mello, s.k. 2015. automatic gaze-based\\ndetection of mind wandering with metacognitive\\nawareness.\\nuser\\nmodeling,\\nadaptation\\nand\\npersonalization: 23rd international conference (dublin,\\nireland, 2015), 31–43.\\nbixler, r. and d’mello, s.k. 2014. toward fully automated\\nperson-independent detection of mind wandering.\\nproceedings of the 22nd international conference on user\\nmodeling, adaptation, and personalization (switzerland,\\n2014), 37–48.\\nblanchard, n. et al. 2014. automated physiological-based\\ndetection of mind wandering during learning. intelligent\\ntutoring systems (honolulu, hawaii, usa, 2014), 55–60.\\nboys, c.v. and others 1890. soap-bubbles, and the forces\\nwhich mould them. cornell university library.\\nchawla, n.v. et al. 2002. smote: synthetic minority oversampling technique. journal of artificial intelligence\\nresearch. (2002), 321–357.\\ncieslak, d.a. et al. 2012. hellinger distance decision trees\\nare robust and skew-insensitive. data mining and\\nknowledge discovery. 24, 1 (2012), 136–158.\\ndixon, w.j. and yuen, k.k. 1974. trimming and\\nwinsorization: a review. statistische hefte. 15, 2–3 (1974),\\n157–170.\\nd’mello, s.k. 2016. giving eyesight to the blind: towards\\nattention-aware aied. international journal of artificial\\nintelligence in education. 26, (2016), 645–659.\\ndomingos, p. 2012. a few useful things to know about\\nmachine learning. communications of the acm. 55, 10\\n(2012), 78–87.\\nekman, p. and friesen, w.v. 1977. facial action coding\\nsystem.\\nfaber, m. et al. 2017. an automated behavioral measure of\\nmind wandering during computerized reading. behavior\\nresearch methods. (2017), 1–17.\\nfranklin, m.s. et al. 2011. catching the mind in flight:\\nusing behavioral indices to detect mindless reading in real\\ntime. psychonomic bulletin & review. 18, 5 (2011), 992–\\n997.\\nfranklin, m.s. et al. 2013. window to the wandering mind:\\npupillometry of spontaneous thought while reading. the\\nquarterly journal of experimental psychology. 66, 12\\n(2013), 2289–2294.\\nholmes, g. et al. 1994. weka: a machine learning\\nworkbench. proceedings of the 1994 second australian and\\nnew zealand conference on intelligent information systems\\n(1994), 357–361.\\nhutt, s. et al. 2016. the eyes have it: gaze-based detection\\nof mind wandering during learning with an intelligent\\n\\n[20]\\n\\n[21]\\n\\n[22]\\n\\n[23]\\n\\n[24]\\n\\n[25]\\n\\n[26]\\n\\n[27]\\n\\n[28]\\n[29]\\n\\n[30]\\n\\n[31]\\n\\n[32]\\n\\n[33]\\n\\n[34]\\n\\n[35]\\n\\n[36]\\n\\ntutoring system. proceedings of the 9th international\\nconference on educational data mining, international\\neducational data mining society (2016), 86–93.\\njeni, l.a. et al. 2013. facing imbalanced data–\\nrecommendations for the use of performance metrics.\\naffective computing and intelligent interaction (acii),\\n2013 humaine association conference on (2013), 245–251.\\nkononenko, i. 1994. estimating attributes: analysis and\\nextensions of relief. machine learning: ecml-94\\n(1994), 171–182.\\nkopp, k. et al. 2015. influencing the occurrence of mind\\nwandering while reading. consciousness and cognition. 34,\\n(2015), 52–62.\\nkopp, k. et al. 2015. mind wandering during film\\ncomprehension: the role of prior knowledge and situational\\ninterest. psychonomic bulletin & review. 23, 3 (2015), 842–\\n848.\\nlittlewort, g. et al. 2011. the computer expression\\nrecognition toolbox (cert). 2011 ieee international\\nconference on automatic face & gesture recognition and\\nworkshops (fg 2011) (2011), 298–305.\\nmills, c. et al. 2016. automatic gaze-based detection of\\nmind wandering during film viewing. proceedings of the\\n9th international conference on educational data mining\\n(raleigh, nc, usa, jun. 2016).\\npham, p. and wang, j. 2015. attentivelearner: improving\\nmobile mooc learning via implicit heart rate tracking.\\nartificial intelligence in education. c. conati et al., eds.\\nspringer international publishing. 367–376.\\nrandall, j.g. et al. 2014. mind-wandering, cognition, and\\nperformance: a theory-driven meta-analysis of attention\\nregulation. psychological bulletin. 140, 6 (2014), 1411.\\nreichle, e.d. et al. 2010. eye movements during mindless\\nreading. psychological science. 21, 9 (2010), 1300–1310.\\nrisko, e.f. et al. 2013. everyday attention: mind wandering\\nand computer use during lectures. computers & education.\\n68, (2013), 275–283.\\nsmallwood, j. et al. 2004. subjective experience and the\\nattentional lapse: task engagement and disengagement\\nduring sustained attention. consciousness and cognition. 13,\\n4 (2004), 657–690.\\nwan, x. 2009. co-training for cross-lingual sentiment\\nclassification. proceedings of the joint conference of the\\n47th annual meeting of the acl and the 4th international\\njoint conference on natural language processing of the\\nafnlp (stroudsburg, pa, usa, 2009), 235–243.\\nwebb, n. and ferguson, m. 2010. automatic extraction of\\ncue phrases for cross-corpus dialogue act classification.\\nproceedings of the 23rd international conference on\\ncomputational linguistics: posters (stroudsburg, pa,\\nusa, 2010), 1310–1317.\\nwestlund, j.k. et al. 2015. motion tracker: camera-based\\nmonitoring of bodily movements using motion silhouettes.\\nplos one. 10, 6 (2015).\\nzacks, j.m. et al. 2010. the brain’s cutting-room floor:\\nsegmentation of narrative cinema. frontiers in human\\nneuroscience. 4, 168 (2010), 1–15.\\nzhang, z. et al. 2011. unsupervised learning in cross-corpus\\nacoustic emotion recognition. 2011 ieee workshop on\\nautomatic speech recognition understanding (dec. 2011),\\n523–528.\\n2016. emotient module: facial expression emotion\\nanalysis.',\n",
       " '119\\n\\n\\x0con the influence on learning of student compliance with\\nprompts fostering self-regulated learning\\nsébastien lallé\\n\\ncristina conati\\n\\nroger azevedo\\n\\nuniversity of british columbia\\n2366 main mall\\nvancouver, bc v6t1z4, canada\\n\\nuniversity of british columbia\\n2366 main mall\\nvancouver, bc v6t1z4, canada\\n\\nnorth carolina state university\\n106 caldwell hall\\nraleigh, nc 27695-8101, usa\\n\\nlalles@cs.ubc.ca\\n\\nconati@cs.ubc.ca\\n\\nrazeved@ncsu.edu\\n\\nnicholas mudrick\\n\\nmichelle taub\\n\\nnorth carolina state university\\n106 caldwell hall\\nraleigh, nc 27695-8101, usa\\n\\nnorth carolina state university\\n106 caldwell hall\\nraleigh, nc 27695-8101, usa\\n\\nnvmudric@ncsu.edu\\n\\nmtaub@ncsu.edu\\n\\nabstract\\nin this paper, we investigate the relationship between students’\\nlearning gains and their compliance with prompts fostering selfregulated learning (srl) during interaction with metatutor, a\\nhypermedia-based intelligent tutoring systems (its). when possible, we evaluate compliance from student explicit answers on\\nwhether they want to follow the prompts, when such answers are\\nnot available, we mine several student behaviors related to prompt\\ncompliance. these behaviors are derived from students’ eyetracking and interaction data (e.g., time spent on a learning page,\\nnumber of gaze fixations on that page). our results reveal that\\ncompliance with some, but not all srl prompts provided by\\nmetatutor do influence learning. these results contribute to gain\\na better understanding of how students benefit from srl prompts,\\nand provides insights on how to further improve their effectiveness. for instance, prompts that do improve learning when followed could be the focus of adaptation designed to foster compliance for those students who would disregard them otherwise.\\nconversely, prompts that do not improve learning when followed\\ncould be improved based on further investigations to understand\\nthe reason for their lack of effectiveness\\n\\nkeywords\\nintelligent tutoring systems; self-regulated learning; scaffolding;\\ncompliance with prompts; learning gains; eye tracking; linear\\nregression; hypermedia\\n\\n1. introduction\\nthere is extensive evidence that the effectiveness of intelligent\\ntutoring systems (its) is influenced by how well students can\\nregulate their learning, e.g., [13, 22]. current research has shown\\nthat scaffolding self-regulated learning (srl) strategies such as\\nsetting learning goals or assessing progress through the learning\\ncontent can improve learning outcomes with an its, e.g., [1, 10,\\n22]. in particular, one of the most common approaches to scaffold\\nsrl is to deliver prompts designed to guide students in applying\\nspecific srl strategies as needed [22]. previous work has focused\\non assessing the general effectiveness of such srl prompts, for\\ninstance by comparing learning outcomes of students working\\nwith versions of the same its with and without the prompts. (e.g.,\\n[1, 19, 21]). other work has investigated the extent to which\\nstudents comply with the overall set of prompts generated by an\\nits [16, 21]. however, there has been no reported study on the\\n\\nrelationship between compliance with specific srl prompts and\\nlearning outcomes. in this paper, we aim to fill this gap. specifically, we explore the impact of student compliance with srl\\nprompts on learning gains with metatutor, an its designed to\\nscaffold student srl processes while learning about topics of the\\nhuman circulatory system [1].\\nour results show that student learning is influenced by compliance with some, but not all, of the srl prompts delivered by\\nmetatutor. overall, we found a positive impact on learning for\\ncompliance with prompts fostering learning strategies (revising a\\nsummary, reviewing notes), or planning processes (setting new\\nlearning goals). on the other hand, we found no impact on learning with prompts related to metacognitive monitoring processes\\n(e.g., prompts to stay on or move away from the current page\\ndepending on student performance on a quiz on that page). having information on the efficacy of each specific prompt in a its is\\nimportant to guide further research on how to improve prompts\\nthat do not seem to improve learning when students follow them.\\nfurthermore, prompts that foster learning when followed can\\nbecome the focus of adaptive interventions designed to improve\\ncompliance for those students who would disregard these prompts\\nif left to their own device.\\nthe paper also provides initial insights into prompts design issues\\nthat affect how easy it is to evaluate compliance. in metatutor,\\nsome prompts explicitly asked students whether they wanted to\\nfollow the prompt, and then provided suitable affordance to accommodate a positive reply. compliance with these prompts is\\neasy to assess, but the additional interactions that they require\\nmight not always be possible, or might even be intrusive for some\\nstudents. other prompts did not require any specific response\\nfrom the students. thus, such prompts are in less danger of being\\nintrusive, and provide for a more open-ended interaction. on the\\nother hand, assessing compliance with these prompts is not trivial,\\nbecause there is no clear definition of what compliance means.\\nfor example, one of the metatutor prompts asks students to reread the current metatutor content page, but there is no obvious\\nway to map this rather generic suggestion to a specific desired\\nbehavior (e.g., spend a specific amount of time on the page, read a\\nspecific number of words). we addressed this problem by running\\nlinear models to correlate a variety of student behaviors related to\\nprompt compliance with learning. the behaviours we mined are\\nbased on both action and eye-tracking data (e.g., time spent on\\nthat page, gaze fixations on the content of the page), and our120\\n\\n\\x0cfigure. 1. screenshot of metatutor.\\nresults provide initial evidence that combining these two data\\nsources can help to evaluate compliance. thus, our findings represent a step toward research on how to evaluate compliance with\\nprompts, both for the type of off line analysis presented in this\\npaper, as well as for the real-time detection of compliance necessary if we want to have itss that adaptively help students follow\\nprompts as needed.\\nthe remainder of the paper starts with an overview of related\\nwork, followed by a description of metatutor and the study that\\ngenerated the dataset we used for this research. next, we illustrate\\nhow we mined data to evaluate compliance with metatutor’s\\nprompts, the statistical analysis we conducted, and our results.\\n\\n2. related work\\nthere has been extensive work on assessing the effectiveness of\\nscaffolding designed to support learning with itss. scaffolding\\ncan include prompts or hints (i.e., interventions that guide the\\nstudent in the right direction), feedback (evaluation of students\\nanswers, behavior or strategies), or demonstration (e.g., worked\\nexamples showing expert behavior) [22, 23]. such scaffolding can\\nbe domain-specific to support the acquisition of domain-specific\\nknowledge, or targeting domain-independent, meta-cognitive\\nlearning processes such as processes for self-regulated learning\\n(srl). there is extensive evidence that both domain-specific\\nscaffolding (e.g., [3, 12, 18, 20]) and meta-cognitive scaffolding\\n(e.g., [2, 10, 11, 21]) can improve the effectiveness of its. for\\nexample, domain-specific hints that explain how to solve the\\ncurrent problem step have been shown to improve skill acquisition in a variety of domains such as mathematics [20] and reading\\n[3, 12]. at the meta-cognitive level, roll et al. [21] tracked\\nsuboptimal help-seeking patterns (e.g., overuse of help) to deliver\\nprompts and feedback on how to effectively use help. prompts\\nand feedback designed to help construct self-explanations during\\nreading [10] or solving scientific problems [11] have been found\\n\\nto positively influence learning. azevedo et al. [2] showed that\\nsrl prompts and feedback effectively foster efficient use of srl\\nstrategies while learning about biology.\\nresearch has also examined student compliance with srl\\nprompts in its [5, 16]. kardan and conati [16] examined the\\nbenefit of providing a variety of prompts designed to help students progress within an interactive learning simulation. overall\\nthey found that students largely complied with the prompts and\\nthat providing these prompts improved learning gains. however,\\nthey did not explore whether and how compliance with specific\\nprompts influence learning outcomes, and which prompts are the\\nmost effective. bouchet et al. [5] adapted the frequency of prompt\\ndelivery in metatutor based on whether students previously complied with prompts of the same type. however, their analysis\\nuncovered no influence of such adaptive prompting strategy on\\nlearning gains. we extend the aforementioned work on prompt\\ncompliance by showing how learning gains are impacted by compliance with some, but not all srl prompts in metatutor. furthermore, whereas previous solely used interaction data to evaluate compliance, we also leverage eye-tracking data when compliance cannot be inferred directly from students’ answers or actions\\n(e.g., compliance with the prompts of reading a text further).\\neye-tracking has been used in its to model a variety of students\\ntraits and behavior, e.g., emotions [14], learning outcomes [15],\\nmetacognitive behavior [7], or mind wandering [4]. eye tracking\\nhas also been used to capture students attention to prompts [6, 8]\\nand to pedagogical agents [17]. conati et al. [6] leveraged gaze\\ndata to detect whether students processed domain-specific textual\\nprompts in an educational game for math, and found that reading\\nthe prompts more extensively improved game performance. lallé\\net al. [17] used gaze data to capture student visual attention to\\npedagogical agents in metatutor, and found that student learning\\ngains are significantly influenced by specific metrics for visual\\nattention (fixation rate, longest fixation). eye-tracking has also121\\n\\n\\x0cbeen used to add real-time adaptive prompts to guru, an agentbased its for learning biology [9]. in that work, audible prompts\\ndesigned to reorient student attention towards the screen were\\ntriggered if a student had not looked at the screen for more than 5s\\nwhile guru was providing scaffolding. this research showed that\\nthis gaze-reactive feedback can improve learning with guru. in\\nour work, we mine eye-tracking data to evaluate compliance with\\nspecific srl prompts, and examine whether and how compliance\\nwith such srl prompts influences learning gains.\\n\\n3. metatutor\\nmetatutor [1] is a hypermedia-based its containing multiple\\npages of content about the circulatory system, as well as mechanisms to help students self-regulating their learning with the assistance of multiple speaking pedagogical agents (pas). when working with metatutor, students are given the overall goal of learning\\nas much as they can about the human circulatory system. the\\nmain interface of metatutor (see fig. 1) includes a table of contents (fig. 1a), the text of the current content page (fig.1b), a\\nminiature image allowing the student to display a diagram along\\nwith the text (fig. 1c), the current goals and subgoals to learn\\nabout (fig. 1e), a timer indicating how much time remains in the\\nlearning session (fig. 1f), and an srl palette (fig. 1d). this\\npalette is designed to scaffold students self-regulatory processes\\nby providing buttons they can select to initiate specific srl activities (e.g., making a summary, taking a quiz, setting subgoals).\\nfurther srl scaffolding is provided by three pas in the form of\\nfeedback on student performance on these srl activities (e.g.,\\nperformance on quiz or on the quality of their summaries), as well\\nas prompts designed to guide these activities as needed. the pas\\ndeliver these prompts based on student behavior (e.g., time spent\\non page, number of pages visited).\\nspecifically, pam the planner prompts planning processes primarily at the beginning of the learning session by suggesting to\\nadd a new subgoal and, if needed, which one to choose (e.g., path\\nof blood flow, heart components). mary the monitor scaffolds\\nstudents’ metacognitive monitoring processes by making them\\ntake quizzes on the target material when they appear to be ready\\nfor them. based on quiz outcomes, mary prompts students to\\nevaluate the relevance of the current content and subgoal to their\\nknowledge, and suggests how to move through the available material and sub goals accordingly. sam the strategizer prompts students to apply the learning strategies consisting of summarizing\\nthe content studied so far or reviewing notes they have taken on\\nthe content1.\\nall pas provide audible assistance through the use of a text-tospeech engine (nuance). the pas are visually rendered using\\nhaptek virtual characters, which generate idle movements when\\nthe pas are not speaking (subtle, gradual head and eye movements), as well as lip movements during speech.\\n\\n4. user study\\nthe data used for the analysis presented in this paper were collected via a user study designed to gain a general understanding of\\nhow students learn with metatutor [1]. the study included the\\ncollection of a variety of multi-channel trace data (e.g., eye track-\\n\\n1\\n\\nmore details about the design of the agents can be found in [1].\\n\\ning, log files, physiological sensors). in this paper, we focus on\\nusing interaction and eye-tracking data to track compliance with\\nthe srl prompts provided by metatutor, and study the relationship among compliance with the prompts and learning gains.\\ntwenty-eight college students participated in the study, which\\nconsisted of two sessions conducted on separate days. during the\\nfirst session, lasting approximately 30-60 minutes, students were\\nadministered several questionnaires, including a 30-item pretest to\\nassess their knowledge of the circulatory system. during the second session lasting approximately three hours, students first underwent a calibration phase with the eye tracker (smi red 250)\\nas well as a training session on metatutor. each student was then\\ngiven 90 minutes to interact with the system. finally, students\\ncompleted a posttest analogous to the pretest, followed by a series\\nof questionnaires about their experience with metatutor.\\n\\n5. data analysis\\n5.1 evaluating compliance with prompts\\nin our analysis we categorize prompts into two types based on\\nhow compliance can be evaluated. the first type includes prompts\\nfor which compliance can be explicitly assessed from students\\nsubsequent responses (explicit compliance prompts); the second\\ntype includes prompts for which compliance needs to be inferred\\nby mining a variety of behaviors (inferred compliance prompts).\\nexplicit compliance prompts are those that:\\n\\uf0b7 require students to answer “yes” or “no” (using a dialogue\\npanel that becomes active at the bottom of the display). if students answers yes, the only action they can perform in the\\nmetatutor interface is the one they agreed upon (e.g., adding a\\nspecific subgoal suggested by the agent, making or revising a\\nsummary, moving to a previously added subgoal or staying on\\nthe current one)2.\\n\\uf0b7 require students to take a specific action within a specific time\\nframe (i.e., open the diagram while they are on the current page,\\nand review notes by the end of the learning session).\\ntable 1 lists the explicit compliance prompts considered in this\\nanalysis.\\ninferred compliance prompts are those for which the pas do not\\nforce students to provide an explicit answer. specifically, after the\\nagent utters one of these prompts, the student simply clicks on\\n“continue” in the same dialogue panel, and can either ignore the\\nprompted action, or comply at some point. these prompts (listed\\nin table 2) include all prompts related to staying on or moving\\naway from the current page, as well as initiating the action of\\nadding a new subgoal.\\n\\n5.2 statistical analysis\\nour analysis aims to investigate if and how compliance with\\nmetatutor’s srl prompts influence learning. the variable we\\n\\n2\\n\\nfor the “stay on current subgoal” prompt, students are not forced\\nto comply after answering “yes”, but we have listed it in this\\ncategory because student are still required to explicitly answer\\n“yes” or “no” to the pas as for whether they want to follow the\\nprompt or not.122\\n\\n\\x0ctable 1. list of explicit compliance prompts provided in metatutor (grouped by type of prompted srl processes).\\nprompt label\\n\\ndescription\\n\\nprompts for\\n\\nsuggest subgoal recommend possible subgoals to learn about while the students is adding new subgoal.\\n\\nplanning processes\\n\\nmoving to next\\nsubgoal\\n\\nrecommend moving on to another subgoal when the student did well on a quiz related to\\nthe current subgoal.\\n\\nstay on subgoal\\n\\nrecommend to learn more about the current subgoal when the student did not do well\\nenough on a quiz related to that subgoal.\\n\\nopen diagram\\n\\nrecommend opening the diagram when it is relevant to the current subgoal.\\n\\nsummarize\\n\\nrecommend making a summary of the current page when the student has spent enough\\ntime on that page.\\n\\nrevise summary\\n\\nrecommend revising the summary submitted by the student when there are issues with the\\nlearning strategies\\nsummary (e.g., the summary is too long or too short).\\n\\nreview notes\\n\\nrecommend reviewing notes taken on the learning content when approaching from the\\nend of the session.\\n\\nmetacognitive monitoring processes\\n\\ntable 2. list of inferred compliance prompts provided in metatutor (grouped by type of prompted srl processes).\\nprompt label\\n\\ndescription\\n\\nprompts for\\n\\nadd subgoal\\n\\nrecommend adding a new subgoal to learn about when a student has no active subgoal.\\n\\nmove to next\\npage\\n\\nrecommend moving on to another page when the student did well on a quiz related to the\\ncurrent page.\\nmetacognitive monitorrecommend staying on the current page when the student did not well enough on a quiz ing processes\\nrelated to that page.\\n\\nstay on page\\n\\nadopted to measure learning in our analysis is proportional learning gain, defined as:\\n\\nplanning processes\\n\\ntable 4 shows the compliance rate averaged across students for\\neach of the seven explicit compliance prompts in metatutor, and\\nthe number of prompts delivered.\\ntable 4. descriptive statistics of the number of explicit compliance prompts delivered, as well as on compliance rate.\\n\\ntable 3 reports statistics for pre- and post-test scores, as well as\\nfor the corresponding learning gains.3\\ntable 3. descriptive statistics for pretest, posttest, and\\nlearning gain.\\nmeasures of learning\\npretest\\nposttest\\nproportional learning gain\\n\\nm\\n18.6\\n21.4\\n15.3\\n\\nsd\\n4.2\\n4\\n50.2\\n\\nmedian\\n19\\n21\\n20\\n\\nwe conducted two separate analyses for explicit and inferred\\ncompliance prompts, described next.\\nexplicit compliance prompts. since compliance is directly\\nobserved in the data for explicit compliance prompts (listed in\\ntable 2), we computed a compliance rate for each of these\\nprompts as follow:\\n\\n3\\n\\nthe increase from pretest to post-test is statistically significant\\nindicating that metatutor is overall effective at fostering learning, as further discussed in [1].\\n\\nprompt\\nsuggest subgoal\\nmove next subgoal\\nstay on subgoal\\nopen diagram\\nsummarize\\nrevise summary\\nreview notes\\n\\ntotal number of\\nprompts delivered\\n60\\n25\\n44\\n77\\n105\\n59\\n28\\n\\ncompliance rate\\nmean (sd)\\n.90 (.25)\\n.85 (.34)\\n.27 (.37)\\n.21 (.32)\\n.32 (.41)\\n.76 (.37)\\n.46 (.51)\\n\\nto investigate the impact of compliance with explicit compliance\\nprompts on learning, we ran a multiple linear regression model\\nwith proportional learning gain as the dependent variable, as\\nwell as the compliance rate for each of the seven explicit compliance prompts, and the total number of prompts received as the\\nfactors. for post-hoc analysis we ran pairwise t-test comparisons,\\nand p-values were adjusted with the holm-bonferroni approach to\\naccount for multiple comparisons.\\ninferred compliance prompts. as stated above, for inferred\\ncompliance prompts (listed in table 5), students are not forced to\\nexplicitly accept or ignore the prompt. this means that compliance with those prompts has to be assessed from student behaviors following the prompts. one approach we considered was to\\nmake this assessment binary, as we did for explicit compliance\\nprompts, by establishing thresholds for relevant behaviors. for\\ninstance, compliance with the prompt to re-read the current page\\ncould be assessed to be true if the student stays on the page for a\\nfixed number of seconds after receiving this prompts. however, it123\\n\\n\\x0cis difficult to fix these thresholds in an informed manner, as they\\nmay depend on the student (e.g., on a student’s readings speed,\\nexisting understanding of the page, etc.), and on the object of the\\nprompt (e.g., on the length or difficulty of the page to be re-read).\\nit is also difficult to decide which specific behaviors should be\\nconsidered for compliance, as several might be relevant (e.g., time\\nspent on a page, specific attention patterns on a page).\\nthus, for the subsequent analysis, we avoided committing to\\nspecific thresholds and behaviors, and we opted instead for performing regression analyses to try to relate multiple relevant compliance behaviors to learning.\\nwe started by building data windows that capture student data\\nfrom the delivery of each inferred compliance prompt in table 2,\\nto the following actions:\\n\\uf0b7 “moving to another page” for the move to next page and stay\\non page prompts;\\n\\uf0b7 “adding a new subgoal” for the add new subgoal prompt.\\nwe used these data windows to derive three behavioral measures\\nrelated to compliance:\\n\\uf0b7 window length, capturing how long students spent before moving on to another page or adding a new subgoal;\\n\\uf0b7 number of fixations4 made on metatutor’s learning content\\n(text and diagram), as captured by eye tracking. we use this\\nmeasure to understand whether students read the page and/or\\nprocessed the diagram;\\n\\uf0b7 number of srl strategies initiated by the student by pressing\\nthe corresponding buttons in the srl palette (see fig. 1 d).\\nhigher values of these measures (i.e., long windows, high number\\nof fixations on the page and high number of srl strategies used)\\nare possible indicators that the student is processing the current\\npage, e.g., the student is thinking about or reading the content (as\\ncaptured by the length of the data window and number of fixations on the page), or using srl strategies on the current page.\\nthus, we hypothesized that higher values of these measures could\\nreveal compliance with stay on page prompts, whereas lower\\nvalues could reveal compliance with prompts instructing students\\nto move on. similarly, because prompts to add a subgoal requires\\nmoving on from the learning content to actually add a subgoal, we\\nexpected a short window, a small number of fixations on the page,\\nand a small number of srl strategies to indicate compliance.\\nit should be noted that we could have generated other eyetracking measures, such as fixation duration on the text or the\\nnumber of transitions from the text to other components of the\\nmetatutor’s interface. however, because valid eye-tracking data\\nwere collected for only 16 students out of the 28 who participated\\nin the study, resulting in a rather small dataset, we focused on the\\nmost promising behavioral measures that could be related to compliance, as a proof of concept. table 5 shows the amount of inferred compliance prompts delivered to those 16 students.\\n\\ntable 5. number of inferred prompts delivered.\\nprompt\\nadd a subgoal\\nstay on page\\nmove to next page\\n\\nwe leveraged the three aforementioned measures of student behavior to investigate if complying with inferred compliance\\nprompts influences learning, and if so, how. specifically, for each\\nof the three inferred compliance prompts, we ran a multiple linear\\nregression model with proportional learning gain as the dependent variable, as well as the window length, number of srl strategies performed, and number of fixations on the learning content\\nas the factors. as done for explicit compliance prompts, we used\\npairwise t-test comparisons for post-hoc analysis, and all p-values\\nwere adjusted with the holm-bonferroni approach.\\n\\n6. results\\n\\nwe describe below the significant5 effects found in our analysis,\\nfirst for explicit compliance prompts, and second for inferred\\ncompliance prompts.\\n\\n6.1 effects for explicit compliance prompts\\nour statistical analysis uncovered significant main effects of compliance rate for three explicit compliance prompts:\\n\\uf0b7 revise summary (f1,20 = 6.17, p=.02, ηp2 =.15), shown fig. 2a.\\n\\uf0b7 review notes (f1,20 = 7.43, p=.013, ηp2 =.16), shown fig. 2b.\\n\\uf0b7 suggest subgoal (f1,20 = 11.4, p=.003, ηp2=.27), shown fig. 2c.\\nthese three main effects and related pairwise comparisons all\\nreveal that students learned more when they complied more with\\nthese prompts than when complying less.\\nthese results for revise summary and review notes are consistent\\nwith previous findings showing these learning strategies can be\\nbeneficial for learning [17, 22, 24], and extend them by showing\\nthat prompting these strategies is effective when students comply\\nwith the prompts. notably, we found a significant effect for\\nprompts to revise summary, but not for prompts to summarize.\\nthis indicates that solely prompting to summarize is not enough\\nto improve learning, and that guiding the students through the\\nprocess of making a good summary is necessary. results for suggest subgoal indicate that recommending a particular learning\\nsubgoal is useful, possibly because it is difficult for students to\\nchoose good subgoals by themselves.\\nthese results suggest to examine ways to improve compliance\\nwith prompts to revise summary, review notes and suggest subgoal, since our analysis reveals that not complying with them\\nhinders learning. for instance, metatutor could foster compliance\\nwith these prompts by explaining how they can help the students,\\nor conversely force the students to follow these prompts.\\n\\n5\\n4\\n\\nfixation is defined as gaze maintained at one point on the screen\\nfor at least 80ms.\\n\\ntotal number of\\nprompts delivered\\n34\\n117\\n326\\n\\nwe report statistical significance at the 0.05 level throughout\\nthis paper, and effect sizes as small for ηp2 ≥ 0.02, medium for\\nηp2 ≥ 0.13, and large for ηp2 ≥ 0.26.124\\n\\n\\x0ca. main effect of compliance rate with “revise summary”.\\n\\nb. main effect of compliance rate with “review notes”.\\n\\nc. main effect of compliance rate with “suggest subgoal”.\\n\\nd. main effect of fixation on page after reception of “add\\nsubgoal”.\\n\\nfigure 2. main effects found in this analysis, for explicit compliance prompts (charts a, b, c) and inferred conpliance prompts\\n(chart d). error bars show 95% confidence interval.\\nwe found no significant effects and small effect sizes (see appendix a) for the four remaining prompts, namely summarize, stay\\non subgoal or move to next subgoal, and open the diagram.\\nthese results indicate it is important to study the effectiveness of\\nsrl prompts individually, to identify those for which compliance\\ndoes not improve learning. based on these findings, it is justified\\nto further investigate why complying with these prompts is not\\nbeneficial for learning in metatutor, and revise the prompts accordingly. for example, it might be due to the nature of the\\nprompts, their timing, their frequency, their wording, and so forth.\\n\\n6.2 effects for inferred compliance prompts\\nwe found a main effect of fixation on learning content for the\\n“add subgoal” prompts (f1,3 = 13, p = .03, ηp2 = .29), shown in\\nfig. 2d. this effect and related pairwise comparisons reveal that\\nstudents learned more when they fixate more on the current page\\nthan when fixating less. since students were instructed to add a\\nnew subgoal rather than process the current page, this finding\\nsuggests that complying with this prompt might not be effective\\nfor learning with metatutor, possibly because of the timing of\\nthis prompt, its frequency or its wording. although only seven\\nstudents with valid gaze data received this prompt, the effect size\\nis large, suggesting it is worth conducting further analysis to ascertain whether and why complying with this prompt is not beneficial for learning.\\n\\nwe found no effects and small effect sizes (see appendix b) for\\nthe other inferred compliance prompts, namely stay on page and\\nmove to next page, two prompts related to metacognitive monitoring processes. we cannot make final conclusions on the pedagogical effectiveness on these prompts based on these results, because\\nthe dataset is not large and for this reason we did not include in\\nthe analysis other features that could indicate compliance (for\\nexample other eye-tracking measures such as fixation duration on\\ntext or gaze transitions from the text to other components of\\nmetatutor). however, it should be noted that we also found no\\neffect for the explicit compliance prompts that foster metacognitive monitoring processes (stay on subgoal, move to next subgoal,\\nand open the diagram, see previous section). this lack of effect\\nfor all prompts fostering metacognitive monitoring, even when\\ncompliance is explicitly assessed, suggests that these prompts are\\nnot beneficial for learning with metatutor. this could be due to\\nthe way these prompts are currently implemented in metatutor\\n(e.g., their wording, timing delivery or frequency), or to the nature\\nor the prompts itself. our results nonetheless justify to run further\\nanalysis to ascertain whether (and why) prompts fostering metacognitive monitoring are not effective, and revise them as needed.\\n\\n7. conclusion\\nin this research we investigated the relationship between compliance with prompts designed to support the use of self-regulated\\nlearning (srl) processes and learning gains while learning about125\\n\\n\\x0cthe human circulatory system with metatutor. we identified two\\napproaches to evaluate compliance to metatutor’s prompts:\\n(i) assess compliance from students’ subsequent response to the\\nprompts when students are forced to express compliance (e.g., by\\nanswering “yes” or “no” to a prompt);\\n(ii) run linear models to examine the influence on learning of a\\nvariety of student behaviors related to prompt compliance, when\\ncompliance is not elicited by metatutor. the behaviors we mined\\nare based on both interface and eye-tracking data (e.g., time spent\\non that page, gaze fixations on the content of the page).\\nour results revealed that student learning gains are influenced by\\ncompliance with some, but not all srl prompts provided by\\nmetatutor. specifically, we found a positive influence on learning\\nfor prompts that foster learning strategies (revise a summary and\\nreview notes) as well as prompts that recommend setting a specific learning subgoal. based on these findings, it is worth exploring\\nways to improve compliance with these prompts. in particular, in\\nfuture research we plan to examine whether forcing students to\\ncomply with these prompts or providing detailed explanations on\\nhow the prompted srl strategies can be useful can improve\\nlearning.\\nwe found that compliance with the other metatutor’s prompts\\nstudied in this analysis does not improve learning. this finding\\nreveals that assessing compliance to srl prompts individually is\\nuseful to identify prompts that may not be effective at supporting\\nlearning. in particular, we found no results for all prompts related\\nto metacognitive monitoring processes (e.g., staying on/moving\\naway from the current page), suggesting to examine further why\\ncomplying with these prompts do not influence learning with\\nmetatutor. for example, it could be due to their timing and frequency, their wording, their nature, and so forth.\\nin this paper we also addressed the challenge of evaluating compliance with rather open-ended prompts for which there is no\\nclear definition of compliance. specifically we ran a linear regression analysis to relate relevant compliance behaviors to learning.\\nsuch behaviors were derived from a combination of student interaction and eye-tracking data after receipt of a prompt (e.g., time\\nspent and amount of gaze fixations on a page can reveal compliance with prompt to read that page). preliminary results show that\\nsuch interaction-based and eye-tracking-based measures can help\\nevaluate compliance. in future research, we plan to investigate\\nfurther behavioral measures relevant to assessing compliance,\\nsuch as tracking eye gaze patterns on the different components of\\nmetatutor as well as transitions between those components.\\nlastly, we plan to investigate the possibility of detecting in real\\ntime compliance with srl prompts for which we found a positive\\neffect on learning, using eye-tracking and interaction data. such\\nreal-time detection could inform the design of adaptive prompts to\\nfoster compliance for those students who might otherwise disregard these prompts. for instance, adaptive prompts could force\\nstudents to follow them or explain how the prompted srl processes can improve learning. evaluating such adaptive prompts\\nfostering srl processes would provide further insights on how\\nstudents comply with and benefit from srl prompts.\\n\\n8. acknowledgments\\nthis publication is based upon work supported by the national\\nscience foundation under grant no. drl-1431552 and the social sciences and humanities research council of canada. any\\n\\nopinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the national science foundation or the\\nsocial sciences and humanities research council of canada.\\n\\n9. references\\n[1] azevedo, r., harley, j., trevors, g., duffy, m., feyzibehnagh, r., bouchet, f. and landis, r. 2013. using trace\\ndata to examine the complex roles of cognitive, metacognitive, and emotional self-regulatory processes during learning\\nwith multi-agent systems. international handbook of metacognition and learning technologies. springer, 427–449.\\n[2] azevedo, r., martin, s.a., taub, m., mudrick, n.v., millar,\\ng.c. and grafsgaard, j.f. 2016. are pedagogical agents’\\nexternal regulation effective in fostering learning with intelligent tutoring systems? proceedings of the 13th international conference on intelligent tutoring systems (zagreb,\\ncroatia, 2016). springer, 197–207.\\n[3] beck, j., chang, k., mostow, j. and corbett, a. 2008. does\\nhelp help? introducing the bayesian evaluation and assessment methodology. proceedings on the 9th international\\nconference on intelligent tutoring systems (montréal, qc,\\ncanada, 2008). springer, 383–394.\\n[4] bixler, r. and d’mello, s. 2015. automatic gaze-based\\ndetection of mind wandering with metacognitive awareness. proceedings of the 23rd international conference on\\nuser modeling, adaptation and personalization (dublin,\\nireland, 2015). springer, 31–43.\\n[5] bouchet, f., harley, j.m. and azevedo, r. 2016. can adaptive pedagogical agents’ prompting strategies improve students’ learning and self-regulation? proceedings of the\\n13th international conference on intelligent tutoring systems (zagreb, croatia, 2016). springer, 368–374.\\n[6] conati, c., jaques, n. and muir, m. 2013. understanding\\nattention to adaptive hints in educational games: an eyetracking study. international journal of artificial intelligence in education. 23, 1–4 (2013), 136–161.\\n[7] conati, c. and merten, c. 2007. eye-tracking for user modeling in exploratory learning environments: an empirical\\nevaluation. know.-based syst. 20, 6 (2007), 557–574.\\n[8] d’mello, s., olney, a., williams, c. and hays, p. 2012.\\ngaze tutor: a gaze-reactive intelligent tutoring system. international journal of human-computer studies. 70, 5\\n(2012), 377–398.\\n[9] d’mello, s., olney, a., williams, c. and hays, p. 2012.\\ngaze tutor: a gaze-reactive intelligent tutoring system. international journal of human-computer studies. 70, 5\\n(2012), 377–398.\\n[10] graesser, a. and mcnamara, d. 2010. self-regulated learning in learning environments with pedagogical agents that interact in natural language. educational psychologist. 45, 4\\n(2010), 234–244.\\n[11] hausmann, r.g. and vanlehn, k. 2007. explaining selfexplaining: a contrast between content and generation. proceedings of the 13th international conference on artificial\\nintelligence in education (los angeles, ca, usa, 2007).\\nspringer, 417–424.126\\n\\n\\x0c[12] heiner, c., beck, j. and mostow, j. 2004. improving the help\\nselection policy in a reading tutor that listens. proceedings\\nof the instil/icall symposium on nlp and speech technologies in advanced language learning systems (venice,\\nitaly, 2004), 195–198.\\n[13] jacobson, m.j. 2008. a design framework for educational\\nhypermedia systems: theory, research, and learning emerging scientific conceptual perspectives. educational technology research and development. 56, 1 (2008), 5–28.\\n[14] jaques, n., conati, c., harley, j.m. and azevedo, r. 2014.\\npredicting affect from gaze data during interaction with an\\nintelligent tutoring system. proceedings of the 12th international conference on intelligent tutoring systems (honolulu, hi, usa, 2014). springer, 29–38.\\n[15] kardan, s. and conati, c. 2012. exploring gaze data for\\ndetermining user learning with an interactive simulation.\\nproceedings of the 20th international conference on user\\nmodeling, adaptation, and personalization (montréal, qc,\\ncanada, 2012). springer, 126–138.\\n[16] kardan, s. and conati, c. 2015. providing adaptive support\\nin an interactive simulation for learning: an experimental\\nevaluation. proceedings of the 33rd annual acm conference on human factors in computing systems (seoul,\\nsouth korea, 2015). acm, 3671–3680.\\n[17] lallé, s., taub, m., mudrick, n.v., conati, c. and azevedo,\\nr. 2017. the impact of student individual differences and\\nvisual attention to pedagogical agents during learning with\\nmetatutor. proceedings of the 18th international conference on artificial intelligence in education (wuhan, china,\\n2017). springer (to appear).\\n[18] mcnamara, d.s., boonthum, c., levinstein, i.b. and millis,\\nk. 2007. evaluating self-explanations in istart: comparing word-based and lsa algorithms. handbook of latent\\nsemantic analysis. psychology press. 227–241.\\n[19] najar, a.s., mitrovic, a. and mclaren, b.m. 2014. adaptive\\nsupport versus alternating worked examples and tutored\\nproblems: which leads to better learning? proceedings of\\nthe 22nd international conference on user modeling, adaptation, and personalization (aalborg, denmark, 2014).\\nspringer, 171–182.\\n\\n[24] shute, v.j. 2008. focus on formative feedback. review of\\neducational research. 78, 1 (2008), 153–189.\\n[25] trevors, g., duffy, m. and azevedo, r. 2014. note-taking\\nwithin metatutor: interactions between an intelligent tutoring system and prior knowledge on note-taking and learning.\\neducational technology research and development. 62, 5\\n(2014), 507–528.\\n\\nappendix a\\nall statistical results for explicit compliance prompts (discussed in\\nsection 6.1). bold indicates a significant effect.\\nprompt\\nsuggest subgoal\\nreview notes\\nrevise summary\\nsummarizing\\nmove on subgoal\\nstay on subgoal\\nopen diagram\\n\\nf value\\nf1,20 = 11.4\\nf1,20 = 7.43\\nf1,20 = 6.17\\nf1,20 = 1.76\\nf1,20 = 0.92\\nf1,20 = 1.47\\nf1,20 = 0.71\\n\\np-value\\np=.003\\np=.013\\np=.02\\np=.20\\np=.35\\np=.24\\np=.41\\n\\neffect size\\nηp2=.27\\nηp2 =.16\\nηp2 =.15\\nηp2 =.06\\nηp2 =.02\\nηp2 =.01\\nηp2 =.08\\n\\nappendix b\\nall statistical results for explicit compliance prompts (discussed in\\nsection 6.2). bold indicates a significant effect.\\nprompt\\nadd subgoal\\nmove on\\npage\\nstay on\\npage\\n\\nmeasure\\nwindow length\\n#fixations on\\npage\\n#srl strategies\\nwindow length\\n#fixations on page\\n#srl strategies\\nwindow length\\n#fixations on page\\n#srl strategies\\n\\nf value\\n\\np-value\\n\\nf1,3 = .91\\n\\np = .41\\n\\neffect\\nsize\\nηp2 = .04\\n\\nf1,3 = 13\\n\\np = .03\\n\\nηp2 = .29\\n\\nf1,3 = .02\\nf1,10 = .00\\nf1,10 = .03\\nf1,10 = .40\\nf1,10 = .34\\nf1,10 = .07\\nf1,10 = .004\\n\\np = .90\\np = .98\\np = .86\\np = .54\\np = .57\\np = .79\\np = .95\\n\\nηp2 = .01\\nηp2 = .00\\nηp2 = .00\\nηp2 = .01\\nηp2 = .01\\nηp2 = .03\\nηp2 = .02\\n\\n[20] poitras, e.g. and lajoie, s.p. 2014. developing an agentbased adaptive system for scaffolding self-regulated inquiry\\nlearning in history education. educational technology research and development. 62, 3 (2014), 335–366.\\n[21] ritter, s., anderson, j.r., koedinger, k.r. and corbett, a.\\n2007. cognitive tutor: applied research in mathematics education. psychonomic bulletin & review. 14, 2 (2007), 249–\\n255.\\n[22] roll, i., aleven, v., mclaren, b.m. and koedinger, k.r.\\n2011. improving students’ help-seeking skills using metacognitive feedback in an intelligent tutoring system. learning and instruction. 21, 2 (2011), 267–280.\\n[23] roll, i., wiese, e.s., long, y., aleven, v. and koedinger,\\nk.r. 2014. tutoring self-and co-regulation with intelligent\\ntutoring systems to help students acquire better learning\\nskills. design recommendations for intelligent tutoring\\nsystems, volume 2. u.s. army research laboratory. 169–\\n182.',\n",
       " '31\\n\\n\\x0canalysis of problem-solving behavior in open-ended\\nscientific-discovery game challenges\\naaron bauer\\n\\nawb@cs.washington.edu\\n\\njeff flatten\\n\\njflat06@cs.washington.edu\\n\\nzoran popović\\n\\nzoran@cs.washington.edu\\n\\ncenter for game science, computer science and engineering\\nuniversity of washington\\nseattle, wa 98195, usa\\n\\nabstract\\n\\nproblem-solving skills in creative, open-ended domains are both\\nimportant and little understood. these domains are generally illstructured, have extremely large exploration spaces, and require\\nhigh levels of specialized skill in order to produce quality solutions.\\nwe investigate problem-solving behavior in one such domain, the\\nscientific-discovery game foldit. our goal is to discover differentiating patterns and understand what distinguishes high and low levels\\nof problem-solving skill. to address the challenges posed by the\\nscale, complexity, and ill-structuredness of foldit solver behavior\\ndata, we devise an iterative visualization-based methodology and use\\nthis methodology to design a concise, meaning-rich visualization of\\nthe problem-solving process in foldit. we use this visualization to\\nidentify key patterns in problem-solving approaches, and report how\\nthese patterns distinguish high-performing solvers in this domain.\\n\\nkeywords\\n\\nproblem solving; scientific-discovery games; visualization\\n\\n1.\\n\\nintroduction\\n\\nas efforts in scalable online education expand, interest continues\\nto increase in moving beyond small, highly constrained tasks, such\\nas multiple choice or short answer questions, and incorporating\\ncreative, open-ended activities [7, 14]. existing research supports\\nthis move, showing that problem-based learning can enhance students’ problem-solving and metacognitive skills [11]. scaling such\\nactivities poses significant challenges, however, in terms of both assessment and feedback. it will be vital to devise scalable techniques\\nnot only to assess students’ final products, but also to understand\\ntheir progress through complex and heterogeneous problem-solving\\nspaces. these techniques will apply to a broad range of education\\nsettings, from purely online programs like udacity’s nanodegrees\\nto more traditional settings where new standards like the common\\ncore emphasize strategic problem solving.\\na growing body of work has found that educational and serious\\ngames are fertile ground for assessing students’ capabilities and\\nproblem-solving skills [6, 10]. our work continues this general\\nline of inquiry by examining creative, problem-solving behavior\\namong players in the scientific-discovery game foldit. by modeling\\nthe functions of proteins, the workhorses of living cells, foldit\\nchallenges players, hereafter referred to as solvers, to resolve the\\nshape of proteins as a 3d puzzle. these puzzles are completely\\nopen and often under-specified, making it a highly suitable setting\\nin which to gain insight into student progress through complex\\nsolution spaces. in the foldit scientific-discovery community, the\\nfocus is on developing people from novices to experts that are\\neventually capable of solving protein structure problems that are\\n\\ncurrently unsolved by the scientific community. in fact, solutions\\nproduced in foldit have led to three results published in nature [3,\\n5, 16]. foldit is an attractive learning space domain because its\\nsolvers are capable of contributing to state-of-the-art biochemistry\\nresults, and the vast majority of best performing solvers had no\\nexposure to biochemistry prior to joining foldit community. hence,\\nsolver behavior in foldit represents development of highly effective\\nproblem-solving in an open-ended domain over long time horizons.\\nin this work, we identify six strategic patterns employed by foldit\\nsolvers and show how these patterns differentiate between successful\\nand less successful solvers. these patterns cover instances where\\nsolvers investigate multiple hypotheses, explore more greedily or\\nmore inquisitively, try to escape local optima, and make structured\\nuse of the manual or automated tools available in foldit.\\nthe aspects of the foldit environment that make it an attractive\\nsetting in which to study problem solving also present significant\\nchallenges. problems in foldit share many of the properties jonassen\\nattributes to design problems, which they describe as “among the\\nmost complex and ill-structured kinds of problems that are encountered in practice” [13]. these properties include a vague goal with\\nfew constraints (in foldit, the goal is often entirely open-ended:\\nfind a good configuration of the protein), answers that are neither\\nright or wrong, only better or worse, and limited feedback (in foldit,\\nreal-time feedback and solution evaluation are limited to a single\\nnumerical score corresponding to the protein’s current energy state,\\nand solvers frequently must progress through many low-scoring\\nstates to reach a good configuration; more nuanced feedback from\\nbiochemists is sometimes available, but on a timescale of weeks).\\nthe ill-structured nature of problems posed in foldit necessarily\\ndeprives us of the structures, such as clear goal states and straightforward relationships between intermediate states and goal states,\\nthat typically form the basis of existing detailed and quantitative\\nanalyses of problem-solving behavior.\\nthe size and complexity of foldit’s problem space presents another\\nmajor challenge. even though the logs of solver interactions consist\\nonly of regular snapshots of a solver’s current solution (along with\\nattendant metadata), the record of a single solver’s performance on\\na given problem frequently consists of thousands of such snapshots\\n(which in turn are just a sparse sampling of the actual solving process). furthermore, the nature of the solution state, the configuration\\nof hundreds of components in continuous three-dimensional space,\\nrenders collapsing the state space by directly comparing solution\\nstates impractical. compounding the size of the problem space is\\nthe complexity of the actions available to foldit solvers. in addition\\nto manual manipulation of the protein configuration, solvers can\\ninvoke various low-level automated optimization routines (some32\\n\\n\\x0cof which run until the solver terminates them) and place different\\nkinds of constraints on the protein configuration (rubber bands in\\nfoldit parlance) that restrict its modification in a variety of ways.\\nsolvers can also deploy many of these tools programmatically via\\nlua scripts called recipes. taken together these challenges of illstructuredness, size, and complexity threaten to make analysis of\\nhigh-level problem-solving behavior in foldit intractable.\\nto overcome these obstacles, we devise a visualization-based methodology capable of producing tractable representations of foldit solvers’\\nproblem-solving behavior while maintaining the key encodings necessary for analysis of high-level strategic behavior. a process of\\niterative summarization forms the core of this methodology, and\\nensures that the transformations applied to the raw data do not\\nelide structures potentially relevant to understanding solvers’ unique\\nstrategic behavior. using this methodology, we examine solver activity logs from 11 foldit puzzles, representing 970 distinct solvers and\\nnearly 3 million solution snapshots. leveraging metadata present\\nin the solution snapshots, we represent solving behavior as a tree,\\nand apply our methodology to visualize a summarized tree showing\\nwhere they branched off to investigate multiple hypotheses, how\\nthey employed some of the automated tools available to them, and\\nother salient problem-solving behavior. we use these depictions to\\ndetermine key distinguishing features of this exploration process.\\nwe subsequently use these features to better understand the patterns\\nof expert-level problem solving.\\nour work focuses on the following research questions: (1) how\\ncan we visually represent an open-ended exploration towards a\\nhigh-quality solution in a large, ill-structured problem space? (2)\\nwhat are the key patterns of problem-solving behavior exhibited\\nby individuals?, and (3) what are the key differences along these\\npatterns between high-performing and lower-performing solvers in\\nan open-ended domain like foldit? in addressing these questions we\\nfind that high-performing solvers explore the solution space more\\nbroadly. in particular, they pursue more hypotheses and actively\\navoid getting stuck in local minima. we also found that both highand lower-performing solvers have similar proportion of manual and\\nautomated tool actions, indicating that better performance on openended challenges stems from the quality of the action intermixing\\nrather than aggregate quantity.\\n\\n2.\\n\\nrelated work\\n\\nwhile automated grading has mostly been explored for well-specified\\ntasks where the correct answer has a straightforward and concise\\ndescription, some previous work has developed techniques for more\\ncomplex activities. some achieve scalability through a crowdsourcing framework such as udacity’s system for hiring external\\nexperts as project reviewers [14]. other work has demonstrated\\nautomated approaches that leverage machine learning to enable scalable grading of more complex assignments. for example, geigle et\\nal. describe an application of online active learning to minimize the\\ntraining set a human grader must produce [7] when automatically\\ngrading an assignment where students must analyze medical cases.\\nour work does not focus on grading problem-solving behavior, but\\ninstead approaches the issue of scalability at a more fundamental\\nlevel: understanding fine-grained problem-solving strategies and\\nhow they contribute to success in an open-ended domain.\\na robust body of prior work has addressed the challenge of both\\nvisualizing and gleaning insight from player activity in educational\\nand serious games. andersen et al. developed playtracer, a general method for visualizing players’ progress through a game’s\\n\\nstate space when a spatial relationship between the player and the\\nvirtual environment is not available [1]. wallner and kriglstein provide a thorough review of visualization-based analysis of gameplay\\ndata [21]. prior work has analyzed gameplay data without visualization as well. falakmasir et al. propose a data analysis pipeline\\nfor modeling player behavior in educational games. this system\\ncan produce a simple, interpretable model of in-game actions that\\ncan predict learning outcomes [6]. our work differs in its aims from\\nthis prior work. we do not seek to develop a general visualization\\ntechnique, but instead to design and leverage a domain-specific\\nvisualization to analyze problem-solving behavior. we are also\\nnot predicting player behavior, nor modeling players in terms of\\nlow-level actions, but rather identifying higher-level strategy use.\\nthe work most similar to ours is that which focuses on problemsolving behavior, including both the long-running efforts in educational psychology to develop general theories and more recent\\nwork data-driven on understanding the problem-solving process.\\nour formulation of solving behavior in foldit as a search through\\na problem space follows from classic information-processing theories of problem solving (e.g., [9, 19]). gick reviews research on\\nboth problem-solving strategies and the differences in strategy use\\nbetween experts and novices [8]. our work complements the existing literature by focusing on understanding problem solving in\\nthe little-studied domain of scientific-discovery games, and on the\\nill-structured problems present in foldit. our findings on the differences in strategy use between high- and lower-performing solvers in\\nfoldit are consistent with the consensus in the literature that expert’s\\nknowledge allows them to effectively use strategies that are poorly\\nor infrequently used by less-skilled solvers. we also contribute a\\ngranular understanding of the specific strategies and differences at\\nwork in the foldit domain.\\nsignificant recent work has investigated problem-solving behavior\\nin educational games and intelligent tutoring systems using a variety\\nof techniques. tóth et al. used clustering to characterize problemsolving behavior on tasks related to understanding a system of linear\\nstructural equations. the clusters distinguished between students\\nthat used a vary-one-thing-at-a-time strategy (both more and less\\nefficiently) and those that used other strategies [20]. through a\\ncombination of automated detectors, path analysis, and classroom\\nstudies, rowe et al. investigated the relationship between a set\\nof six strategic moves in a newtonian physics simulation game\\nand performance on pre- and post-assessments. they found that\\nthe use of some moves mediated the relationship between prior\\nachievement and post scores [18]. eagle et al. discuss several applications of using interaction networks to visualize and categorize\\nproblem-solving behavior in education games and intelligent tutoring systems. these networks offer insight for hint generation\\nand a flexible method for visualizing student work in rule-using\\nproblem solving environments [4] . using decision trees to build\\nseparate models for optimal and non-optimal student performance,\\nmalkiewich et al. gained insight into how learning environments\\ncan encourage elegant problem solving [17]. our primary contribution is to extend analysis of problem-solving behavior to a more\\ncomplex and open-ended domain that those studied in similar previous work. the size and complexity of foldit’s problem space,\\nthe volume of data necessary to capture exploration in this space,\\nand the ill-structured nature of the foldit problems all pose unique\\nchallenges. we devise a visualization-based methodology focused\\non iterative summarization, and successfully apply it to identify key\\nproblem-solving patterns exhibited by foldit solvers.33\\n\\n\\x0c3.\\n\\nfoldit\\n\\nfoldit is a scientific-discovery game that crowdsources protein folding. it presents solvers with a 3d representation of a protein and\\ntasks them with manipulating it into the lowest energy configuration. each protein posed to the solvers is called a puzzle. solvers’\\nsolutions to each puzzle are scored according to their energy configuration, and solvers compete to produce the highest scoring results.\\n\\nfigure 1: the foldit interface. foldit solvers use a variety of\\ntools to interactively reshape proteins. in this figure, a solver\\nuses rubber bands to pull together two sheets, long flat regions\\nof the protein.\\nsolvers have many tools at their disposal when solving foldit puzzles. they can manipulate and constrain the structure in various\\nways, employ low-level automated optimization (e.g., a wiggle tool\\nmakes small, rapid, local adjustments to try and improve the score),\\nand trigger solver-created automated scripts called recipes that can\\nprogrammatically use the other tools. there is, however, a subset of\\nthe basic actions that cannot be used by recipes. we will call these\\nmanual-only actions. previous work analyzing solver behavior in\\nfoldit has focused primarily on recipe use and dissemination [2] and\\nrecipe authoring [15].\\nfoldit has several different types of puzzles for solvers to solve. in\\nthis work, we focus on the most common type of puzzle, prediction\\npuzzles. these are puzzles in which biochemists know the amino\\nacids that compose the protein in question, but do not know how\\nthe particular protein folds up in 3d space. this is in contrast to\\ndesign puzzles in which solvers insert and delete which amino acids\\ncompose the protein to satisfy a variety of scientific goals, including\\ndesigning new materials and targeting problematic molecules in\\ndiseases. we focus on prediction puzzles in this work to simplify\\nour analysis by having a consistent objective (i.e., maximize score)\\nacross the problem-solving behavior we analyze.\\n\\n4.\\n\\nmethodology\\n\\nprior work has demonstrated the power of visualization to support\\nunderstanding of problem-solving behavior (e.g., [12]). hence, we\\ndevise a methodology capable of producing concise, meaning-rich\\nvisualizations of the problem-solving process in foldit, and then\\nleverage these visualizations to identify key patterns of solver behavior. we are specifically interested in how solvers navigate from\\na puzzle’s start state to a high-quality solution, what states they\\npass through in between, and what other avenues they explored.\\n\\nsince solving a foldit puzzle can be represented as a directed search\\nthrough a problem space, the clear encoding of parent-child relationships between nodes offered by a tree make it well-suited for\\nvisualizing these aspects of the solving process.\\nthe scale of the foldit data necessitates significant transformation\\nof the raw data in order to render concise visualizations. without\\nany transformation, meaningful patterns are overwhelmed by sparse,\\nrepetitive data and would be far more challenging to identify. while\\nthere are many existing techniques for large-scale tree visualization,\\nwe find clear benefits to developing a visualization tailored to the\\nfoldit domain. specifically, preserving the semantics of our visual\\nencoding is crucial for allowing us to connect patterns in the visualization to concrete strategic behavior in foldit. to accomplish this,\\nthe process by which concise visualization are constructed must\\nbe carefully designed to maintain these links. hence, we devise a\\ndesign methodology focused on iterative summarization.\\nthis process begins by visualizing the raw data. this is followed\\nby iteratively building and refining a set of transformations to summarize the raw data while preserving meaning. the design of these\\ntransformations should be guided by frequently occurring structures.\\nthat is, those structures that the transformations can condense without eliding structures corresponding to unique strategic behavior.\\nin parallel to this iterative design, a set of visual encodings are developed to represent the solving process as richly as possible. key\\nto this entire process is frequent consultation with domain experts,\\nin our case experts on foldit and its community. by applying this\\niterative methodology for several cycles, we designed a domainspecific visualization that we use to identify patterns of strategic\\nbehavior among foldit solvers. we follow up on these patterns with\\ncomputational investigation, and quantify their application by highand lower-performing solvers.\\n\\n4.1\\n\\ndata\\n\\nfor our analysis, we selected 11 prediction puzzles spanning the\\nrange of time for which the necessary data is available. though\\nfoldit has been in continuous use since 2010, the data necessary to\\ntrack a solver’s progress through the problem space has only been\\ncollected since mid-2015. our chosen dataset represents 970 unique\\nsolvers and nearly 3 million solution snapshots. these 11 puzzles are\\njust a small subset of the available foldit data. we chose a subset of\\nsimilar puzzles (i.e., a subtype of relatively less complex prediction\\npuzzles) in order to make common solving-behavior patterns easier\\nto identify. the size of the subset was also guided by practical\\nconstraints, as each puzzle constitutes a large amount of data (20-60\\ngb for the data from all players on a single puzzle).\\nthe data logged by foldit primarily consists of snapshots of solver\\nsolutions as they play, stored as text files using the protein data\\nbank (pdb) format. these snapshots include the current protein\\npose, a timestamp, the solution’s score, the number of times the\\nsolver has invoked each action and recipe, and a record of the intermediate states that led up to the solution at the time of the snapshot.\\nthis record, or solution history, is a list of unique identifiers each\\ncorresponding to a previous solution state. this list is extended\\nevery time the solver undoes an action or reloads a previous solution.\\nhence, by comparing the histories of two snapshots from the same\\nsolver, we can answer questions about their relationship (e.g., does\\none snapshot represent the predecessor of another; where did two\\nrelated snapshots diverge). the key relationship for the purposes of\\nthis analysis is the direct parent-child relationship, which we use to\\ngenerate trees that represent a solver’s solving process.34\\n\\n\\x0c4.2\\n\\nvisualizing solution trees\\n\\nwe applied our methodology to our chosen subset of foldit data to\\ndesign a visualization of an individual’s problem-solving process\\nas a solution tree. several key principles guided this design. first,\\nsince our goal is to discover key patterns, the visualization needs\\nto highlight distinctly different strategies and approaches. these\\ndifferences cannot be buried amidst enormous structures, nor destroyed by graph transformations. second, the visualization must\\ndepict the closeness of each step to the ultimate solution in both time\\nand quality to give a sense of the solver’s progression. third, the\\nsolver’s use of automation in the form of recipes should be apparent\\nsince the use of automation is an important part of foldit.\\nthe fundamental organization of the visualization is that each node\\ncorresponds to a solution state encountered while solving. using the\\nsolution history present in the logged snapshots of solver solutions,\\nwe establish parent-child relationships between solutions. if solution\\nβ is a child of solution α, it indicates that β was generated when\\nthe solver performed actions on α. one crucial limitation, however,\\nis that a snapshot of the solver’s current solution is captured far less\\noften (only once every two minutes) than the solver takes actions.\\nthis means that our data is sparsely distributed along a solution’s\\nhistory going back to the puzzle’s starting state. hence, when naively\\nconstructing the tree from the logged solution histories, it ends up\\ndominated by vast quantities of nodes with no associated data.\\nwe address this issue by performing summarization on the solution\\ntrees, condensing them into concise representations amenable to\\nanalysis for important features. this summarization takes place\\nin two stages. the first stage trims out nodes that (1) do not have\\ncorresponding data and (2) have zero children. this eliminates\\nlarge numbers of leaf nodes that we are unable to reason about\\ngiven that we lack the corresponding data. this stage also combines\\nsequences of nodes each with only one child into a single node. for\\nthe median tree, this stage reduced the number of nodes by an order\\nof magnitude from over 12,000 nodes to about 1,600.\\nthe second stage consists of four phases, each informed by our\\nobservations of common patterns in trees produced by the first stage\\nthat would benefit from summarization. the first phase, called\\nprune, focuses on simplifying uninteresting branches. we observed\\nmany of the branches preserved by the first stage were small, with\\nat most three children, and only continued the tree from one of\\nthose children. prune removes the leaf children of these branches\\nfrom the tree. collapse, the second phase, transforms each of the\\nsequences of single-child nodes left behind after prune into single\\nnodes. the third phase, condense, targets another common pattern\\nwhere a sequence of branches feed into each other, with a child of\\neach branch the parent of the next branch. these sequences are\\nsummarized into a single node labeled cascade along with the\\ndepth (number of branches) and width (average branching factor)\\nof the summarized branches. see figure 2 for an example of the\\nfeatures summarized by these three phases. the final phase, clean,\\ntargets the ubiquitous empty nodes (i.e., nodes for which we lack\\nassociated data) shown in black in figure 2. we eliminate them by\\nmerging them with their parent node, doing so repeatedly until they\\nall have been merged into nodes that contain data. in addition to\\nmaking the trees more concise, this step allows us to reason more\\nfully over the trees since all nodes are guaranteed to contain data.\\nthis second stage of summarization further reduced the number of\\nnodes in the median tree by another order of magnitude to about\\n300 nodes. summarization similarly reduces the space required to\\nstore the data by two orders of magnitude.\\n\\nfigure 2: a solution tree after only the first stage of summarization. the non-black node color represents the score of the\\nsolution at that node (red is worse). the black nodes are empty\\nin that we do not have solution data corresponding to that node.\\nthis figure also shows examples of the features targeted by the\\nsecond summarization stage: prune and collapse eliminate long\\nchains like the one on the right, and condense combines sequences of branches like those going down to left in single cascade nodes.\\nchild-parent relationships are not the only part of the data we visually encoded in the solution trees. nodes are colored on a continuous\\ngradient from red to blue according to the score of the solution represented by that node (red is low-scoring, blue is high-scoring). the\\nbest-scoring node is highlighted as a yellow star. edges are colored\\non a continuous gradient from light to dark green according to the\\ntime the corresponding transition took place, and the children of\\neach node are arranged left to right in chronological order. finally,\\nuse of automation via recipes is an important aspect of problemsolving in foldit. since the logged solution snapshots contain a\\nrecord of which recipes have been used at that point, we can use this\\nto annotate nodes where a recipe was triggered. the annotations\\nconsist of the id of that recipe (a 4 to 6 digit number) and the number\\nof times it was started.\\none major weakness in the data available to us is the lack of a consistent way to determine when the execution of a recipe ended (some\\nrecipes save and restore, possibly being responsible for multiple\\nnodes in the graph beyond where they were triggered). we partially\\naddress this by further annotating a node with the label manual\\nwhenever the solver took a manual-only action at that node. this\\nindicates that no previously triggered recipe continued past that node\\nbecause no recipe could have performed the manual-only action.\\nsince nodes in the summarized trees can represent many individual\\nsteps, it is possible for them to have several of these recipe and\\nmanual action annotations.\\n\\n5.\\n\\nresults\\n\\nusing visualized solution trees for a large set of solvers across our\\nsample of 11 puzzles, we identify a set of six prominent patterns in\\nsolvers’ problem-solving behavior. these patterns do not encompass\\nall solving behavior in foldit, but instead capture key instances of\\nstrategic behavior in three categories: exploration, optimization, and\\nhuman-computer collaboration. future work is needed to generate\\na comprehensive survey of the strategic patterns in these and other\\ncategories. in this analysis, our focus is on identifying a small,\\ndiverse set of commonly occurring patterns to both provide initial35\\n\\n\\x0cinsight into problem-solving behavior, and to demonstrate the potential of our approach. in addition to identification, we also perform\\na quantitative comparison of how these patterns are employed by\\nhigh-performing and lower-performing solvers to gain an understanding of how these patterns contribute to success in an open-end\\nenvironment like foldit.\\n\\n5.1\\n\\nhas a high-scoring node with a low-scoring child, and then chooses\\nto explore from the low-scoring child. the solver was willing to\\nignore the short-term drop in score to try and reach a more beneficial\\nstate in the long-term. figure 5 gives an example of this pattern.\\n\\nproblem-solving patterns\\n\\nexploration. foldit solvers are confronted with a highly discon-\\n\\ntinuous solution space with many local optima, creating a trade-off\\nbetween narrowly focusing their efforts or taking the time to explore\\na broader range of possibilities. in our first two patterns, we examine the broader exploration side of this trade-off at two different\\nscales. taking the macro-scale first, we identify a pattern where\\nsolvers make significant progress on distinct branches of the tree\\n(see figure 3 for an example). we interpret this pattern as the solver\\ninvestigating multiple hypotheses about the puzzle solution, using\\nmultiple instances of the game client or foldit’s save and restore features to deeply explore them all. we call this the multiple hypotheses\\npattern.\\n\\nfigure 5: an example of the optima escape pattern. the solver\\ntransitions from a relatively high-scoring (i.e., blue) state in the\\nupper left to a low-scoring (i.e., red) state. what makes this\\nan example of the pattern is that exploration from the lowscoring state. in this case, the perseverance paid off as the\\nsolver reaches even higher-scoring states in the lower right.\\nin the other direction, we identify the greedy pattern in which solvers\\nexclusively explore from the best-scoring of the available options.\\nobviously, some amount of greedy exploration is necessary in order\\nto refine solutions, but in its extreme form deserves recognition\\nas a pattern with significant potential impact on problem-solving\\nsuccess. naturally, these two patterns do not cover all the ways\\nsolvers explore the problem space, but they do characterize specific\\nstrategic behavior of interest in this analysis.\\n\\nfigure 3: an example of the multiple hypotheses pattern. the\\ntwo hypotheses branch out one of the nodes at the top and continue to the left (a) and right (b).\\nat the micro-scale, solvers very frequently generate a large number\\nof possible next steps (i.e., a branch with a large number of children),\\nbut most often proceed to explore only one of them further. this is\\nnatural given the iterative refinement needed to successfully participate in foldit. hence, solvers that exhibit a pattern of much more\\nfrequently exploring multiple local possibilities demonstrate an unusual effort to explore more broadly. we call this the inquisitive\\npattern. figure 4 shows an example of this behavior.\\n\\nfigure 6: an example of the repeated recipe pattern. at three\\npoints in this solution tree snippet, the solver applies recipe\\n49233 to every child of a node.\\n\\nhuman-computer collaboration. human-computer collabo-\\n\\nfigure 4: an example of the inquisitive pattern. note how frequently multiple children of the same node are explored when\\ncompared to the tree in figure 3.\\n\\noptimization. navigating the extremely heterogeneous solution\\n\\nspace is the primary challenge in foldit, so we look closely at how\\nsolvers attempt to optimize their solutions, digging deeper into\\nsolvers’ approach to exploration than the previous two patterns.\\nwe identify two related patterns describing solvers’ fine-grained\\napproach to optimization. the solution spaces of foldit puzzles\\ncontain numerous local optima that solvers must escape, and we\\nidentify an optima escape pattern highly suggestive of a deliberate\\nattempt to escape a local optima. this pattern occurs when a solver\\n\\nration is a vital part of foldit, and managing the trade-off between\\nautomation and manual intervention is a key feature of solving\\nfoldit puzzles. we identify two patterns that each focus on one\\nside of this trade-off. the first, the manual pattern, corresponds to\\nextended sections of exclusively manual exploration. since recipe\\nuse is very common, extended manual exploration represents a significant investment in the manual intervention side of the trade-off.\\nlimitations with foldit logging data prevent us from capturing all\\nthe manual exploration (i.e., it is not always possible to determine\\nwhether an action was performed by a solver manually or triggered\\nas part of an automated recipe), but what can be captured is still an\\nimportant dimension of variance among problem-solving behavior.\\nour final pattern concerns recipe use. some solvers apply a recipe\\nto every child of a node periodically throughout their solution tree,\\nusing it as a clean-up or refinement step before continuing on (see\\nfigure 6). we call this the repeated recipe pattern. recipe use is\\nvery diverse and frequently doesn’t display any specific structure,\\nmaking this pattern interesting for its regimented way of managing\\nsome of the automation while solving.36\\n\\n\\x0cfigure 7: the number of hypotheses pursued in each solution\\ntree for high- and lower-performing solvers. high-performing\\nsolvers frequently pursue two or more hypotheses, whereas\\nlower-performing solvers most often pursue just one. red circles show the distribution of individual solvers.\\n\\n5.2\\n\\nproblem-solving patterns and\\nsolver performance\\n\\nto understand how the patterns we identify relate to skillful problemsolving in an open-ended domain like foldit, we compare their use\\namong high-performing solvers to that among lower-performing\\nsolvers. specifically, we analyze the occurrence of these patterns in\\nthe 15 best-scoring solutions from each puzzle and compare that to\\nthe occurrence in solutions from each puzzle ranked from 36th to\\n50th. though it varies somewhat between puzzles, in general the\\nsolutions ranked 36th to 50th represent a middle ground in terms\\nof quality. they fall outside the puzzle’s state-of-the-art solutions,\\nbut remain well above the least successful efforts. throughout these\\ncomparisons we use non-parametric mann-whitney u tests with\\nα = 0.008 confidence (bonferroni correction for six comparisons,\\nα = 0.05/6), as our data is not normally distributed. for each test,\\nwe report the test statistic u, the two-tailed significance p, and the\\nrank-biserial correlation measure of effect size r. in addition, since\\nsome of the metrics we compute may not apply to all solution trees\\n(e.g., the tree contains no branches where the inquisitive pattern\\ncan be evaluated), we report the number of solvers involved in the\\ncomparison n for each test (the full sample is n = 330).\\nwe find high-performing solvers explore more broadly than lowerperforming solvers. for the multiple hypotheses pattern, highperforming solvers pursued significantly more hypotheses than\\nlower-performing solvers (u = 10569, p = 0.000014, r = 0.217,\\nn = 330) (see figure 7). for the inquisitive pattern, we compute\\nthe proportion of each solver’s exploration that matches the pattern\\n(i.e., of all the branches in a solver’s solution tree, in what fraction of them did the solver explore more than one child) and find\\nhigh-performing solvers explore inquisitively more often than lowerperforming solvers (u = 9343, p = 0.000295, r = 0.231, n = 313)\\n\\nfigure 8: the proportion of all the branches in a solver’s solution tree in which the solver explored more than one child\\nfor high- and lower-performing solvers. red circles show the\\ndistribution of individual solvers.\\n(see figure 8).\\nwe also find high-performing solvers work harder to avoid local\\noptima. for the optima escape pattern, we compute the number of times this behavior occurs in each solution and find that\\nhigh-performing solvers engage in this behavior more than lowerperforming solvers (u = 11183.5, p = 0.00185, r = 0.173, n = 330)\\n(see figure 9). for the greedy pattern, we compute the proportion of each solver’s exploration that matches the pattern (i.e., of\\nall the branches in a solver’s solution tree, in what fraction of\\nthem did the solver only explore the best-scoring child). while\\nhigh-performing solvers engaged in greedy optimization less often\\nthan lower-performing solvers, the difference was not significant\\n(u = 9079, p = 0.0158, r = −0.163, n = 295) (see figure 10).\\nfinally, we find no significant difference between high- and lowerperforming solvers in the frequency they manually explore and\\nemploy recipes. for the manual pattern, we compute the number of\\nmanual exploration sections in each solution and find no significant\\ndifference between high- and lower-performing solvers (u = 13334,\\np = 0.789, r = 0.014, n = 330). for the repeated recipe pattern,\\nwe computed the median frequency of recipe use along all paths\\nin the solution (i.e., for each path from the root to a leaf, in what\\nfraction of the nodes did the solver trigger at least one recipe) and\\nthough lower-performing solvers used recipes more frequently, the\\ndifference between high- and lower-performing solvers was not\\nsignificant (u = 11342, p = 0.0140, r = −0.157, n = 329).\\n\\n6.\\n\\ndiscussion\\n\\nthe results from our analysis of our solution tree visualizations illuminate some key problem-solving patterns exhibited by individual\\nfoldit solvers. namely, how broadly an individual explores, both\\non a macro- and micro-scale, how actively an individual avoids37\\n\\n\\x0cfigure 9: the number of times in each solution a solver engages in optima escape behavior for high- and lower-performing\\nsolvers. red circles show the distribution of individual solvers.\\n\\nlocal optima by engaging in less greedy optimization and actively\\npursuing locally suboptimal lines of inquiry, and how an individual\\nmanages the interplay between automation and manual intervention.\\ncomparing high- and lower-performing solvers in their application of these patterns suggests that skillful problem-solving in an\\nopen-end domain like foldit involves broader exploration and more\\nconscious avoidance of local minima. this finding that a key feature\\nof high-skill solving behaviors is not being enamored by the current\\nbest solution and possessing strategies for avoiding myopic thinking\\nhad implications for the strategies that should be taught to develop\\nsuccessful problem solvers. further work is required on other large\\nopen-ended domains to confirm this trend.\\nthe finding that solvers of different skill use greedy exploration,\\nmanual exploration, and automation in similar amounts suggests\\nskillful deployment of non-greedy exploration, automation, and\\nmanual intervention takes place at a more fine-grained level than\\noverall quantity. though this work focuses on the presence or\\nabsence of specific solving behavior, the timing and sequencing of\\nstrategic moves are likely to be critical to success. further work is\\nneeded to investigate what differentiates effective and ineffective\\nuse of specific solving strategies.\\nthe foldit dataset itself presented significant challenges for our\\nanalysis, and we addressed these through an iterative visualizationbased methodology. this process served as a design method for\\ngenerating a visual grammar to describe a complex problem-solving\\nprocess. we do not study the generalization of this approach to\\nother datasets and domains in this work, but the prerequisites for\\nits application to other open-ended problem-solving domains can\\nbe concisely enumerated: (1) the logs of solver activity establish\\nclear temporal relationships between solution states such that those\\nstates can be visualized as a progression through the solution space,\\n\\nfigure 10: the proportion of all the branches in a solver’s solution tree in which the solver explored only the best-scoring\\nchild for high- and lower-performing solvers. the fact that the\\nmedian for both categories of solver is above 0.5 indicates that\\nthis pattern in an important part of refining solutions in foldit.\\nred circles show the distribution of individual solvers.\\n(2) the solution state or associated metadata is amenable to visual\\nencoding, so that the visualized progressions can represent finegrained details of the solving process, and (3) deep problem-solving\\ndomain expertise is available to provide the necessary context for\\ninterpreting and summarizing the visualized structures.\\nour chosen subset of foldit data represents only a small fraction\\nof the total available data. in particular, we limited our analysis\\nto a sample of similar prediction puzzles, and compared specific\\nranges of high- and lower-performing solvers. though these choices\\nare well-motivated, it is an important question for future work as\\nto whether our results hold across different datasets and groups of\\ncomparison. more broadly, foldit supports numerous variations\\non the prediction and design puzzle archetypes, which offers an\\nexciting opportunity to study problem solving across a number of\\nrelated contexts with varying goals, constraints, inputs, and tools.\\n\\n7.\\n\\nconclusion\\n\\ngaining a better understanding of key patterns in problem-solving\\nbehavior in complex, open-ended environments is important for deploying this kind of activity in an educational setting at scale. in this\\nwork, we identified six key patterns in problem-solving behavior\\namong solvers of foldit. the protein folding challenges in foldit\\npresent rich, completely open, heterogeneous solution spaces, making them a compelling domain in which to analyze these patterns.\\nto facilitate the identification of these patterns, we used an iterative\\nmethodology to design visualizations of solvers’ problem-solving\\nactivity as solution trees. the size and complexity of the foldit data\\nrequired us to develop domain-specific techniques to summarize the\\nsolution trees and render them tractable for analysis while preserving the salient problem-solving behaviors. finally, we compared the38\\n\\n\\x0coccurrence of the patterns we identified between high- and lowerperforming solvers. we found that high-performing solvers explore\\nmore broadly and more aggressively avoid local optima. we also\\nfound that both categories of solvers employ automation and manual\\nintervention in similar quantities, inviting future work to study how\\nthese tools are used at a more fine-grained level.\\nwe have only scratched the surface in our analysis of a subset of\\nfoldit data. two integral aspects of the foldit environment are\\nnot within the scope of this work: collaboration and expert feedback. we only considered solutions produced by individual solvers,\\nbut foldit solver can also take solutions produced by others and\\ntry and improve them. this collaborative framework may involve\\nspecialization and unique solving strategies, and deserves careful\\nstudy. expert feedback comes into play for design puzzles, where\\nbiochemists will select a small number of the solutions to try and\\nsynthesize in the lab. experts will also impose additional constraints\\non future design puzzles to try and guide solutions toward more\\npromising designs. the interaction of these channels for expert\\nfeedback and problem-solving behavior is an important topic for\\nfuture research. also outside the scope of this work is how individual solvers change their problem-solving behavior over time. many\\nsolvers have been participating in the foldit community for many\\nyears, and studying how their behavior evolves could yield insights\\ninto the acquisition of high-level problem-solving skills.\\nlooking more broadly at the impact of this work, our methodology\\nand analysis can serve as a first step toward discovering the scaffolding necessary to develop high-level problem-solving skills. these\\nresults could contribute to a hint generation system, where solvers\\ncould be guided toward known effective strategies, or a meta-planner\\ncomponent in foldit that could tailor the parameters of particular\\npuzzles to optimize the quality of the scientific results. in all of\\nthese cases, this work contributes to the necessary foundational\\nunderstanding of the problem-solving behavior involved.\\n\\n8.\\n\\nacknowledgements\\n\\nthis work was supported by the national institutes of health grant\\n1uh2ca203780, rosettacommons, and amazon. this material\\nis based upon work supported by the national science foundation\\nunder grant no. 1629879.\\n\\n9.\\n\\n[6]\\n\\n[7]\\n\\n[8]\\n[9]\\n[10]\\n\\n[11]\\n[12]\\n\\n[13]\\n[14]\\n[15]\\n\\n[16]\\n\\nreferences\\n\\n[1] e. andersen, y.-e. liu, e. apter, f. boucher-genesse, and\\nz. popović. gameplay analysis through state projection. in\\nproceedings of the fifth international conference on the\\nfoundations of digital games, pages 1–8. acm, 2010.\\n[2] s. cooper, f. khatib, i. makedon, h. lu, j. barbero, d. baker,\\nj. fogarty, z. popović, et al. analysis of social gameplay\\nmacros in the foldit cookbook. in proceedings of the 6th\\ninternational conference on foundations of digital games,\\npages 9–14. acm, 2011.\\n[3] s. cooper, f. khatib, a. treuille, j. barbero, j. lee,\\nm. beenen, a. leaver-fay, d. baker, z. popović, et al.\\npredicting protein structures with a multiplayer online game.\\nnature, 466(7307):756–760, 2010.\\n[4] m. eagle, d. hicks, b. peddycord iii, and t. barnes.\\nexploring networks of problem-solving interactions. in\\nproceedings of the 5th conference on learning analytics and\\nknowledge. acm, 2015.\\n[5] c. b. eiben, j. b. siegel, j. b. bale, s. cooper, f. khatib,\\n\\n[17]\\n\\n[18]\\n[19]\\n[20]\\n\\n[21]\\n\\nb. w. shen, b. l. stoddard, z. popovic, and d. baker.\\nincreased diels-alderase activity through backbone\\nremodeling guided by foldit players. nature biotechnology,\\n30(2):190–192, 2012.\\nm. h. falakmasir, j. p. gonzalez-brenes, g. j. gordon, and\\nk. e. dicerbo. a data-driven approach for inferring student\\nproficiency from game activity logs. in proceedings of the\\nthird (2016) acm conference on learning@ scale, pages\\n341–349. acm, 2016.\\nc. geigle, c. zhai, and d. c. ferguson. an exploration of\\nautomated grading of complex assignments. in proceedings of\\nthe third (2016) acm conference on learning@ scale, pages\\n351–360. acm, 2016.\\nm. l. gick. problem-solving strategies. educational\\npsychologist, 21(1-2):99–120, 1986.\\nj. g. greeno. natures of problem-solving abilities. handbook\\nof learning and cognitive processes, 5:239–270, 1978.\\ne. harpstead, c. j. maclellan, k. r. koedinger, v. aleven,\\ns. p. dow, and b. myers. investigating the solution space of\\nan open-ended educational game using conceptual feature\\nextraction. in proceedings of the 6th conference on\\neducational data mining, 2013.\\nw. hung, d. h. jonassen, r. liu, et al. problem-based\\nlearning. handbook of research on educational\\ncommunications and technology, 3:485–506, 2008.\\nm. johnson, m. eagle, and t. barnes. invis: an interactive\\nvisualization tool for exploring interaction networks. in\\nproceedings of the 6th conference on educational data\\nmining, 2013.\\nd. h. jonassen. toward a design theory of problem solving.\\neducational technology research and development,\\n48(4):63–85, dec 2000.\\nd. a. joyner. expert evaluation of 300 projects per day. in\\nproceedings of the third (2016) acm conference on\\nlearning@ scale, pages 121–124. acm, 2016.\\nf. khatib, s. cooper, m. d. tyka, k. xu, i. makedon,\\nz. popović, and d. baker. algorithm discovery by protein\\nfolding game players. proceedings of the national academy\\nof sciences, 108(47):18949–18953, 2011.\\nf. khatib, f. dimaio, s. cooper, m. kazmierczyk, m. gilski,\\ns. krzywda, h. zabranska, i. pichova, j. thompson,\\nz. popović, et al. crystal structure of a monomeric retroviral\\nprotease solved by protein folding game players. nature\\nstructural & molecular biology, 18(10):1175–1177, 2011.\\nl. malkiewich, r. s. baker, v. shute, s. kai, and l. paquette.\\nclassifying behavior to elucidate elegant problem solving in\\nan educational game. in proceedings of the 9th conference on\\neducational data mining, 2016.\\ne. rowe, r. s. baker, and j. asbell-clarke. strategic game\\nmoves mediate implicit science learning. in proceedings of\\nthe 8th conference on educational data mining, 2015.\\nh. a. simon. information-processing theory of human\\nproblem solving. handbook of learning and cognitive\\nprocesses, 5:271–295, 1978.\\nk. tóth, h. rölke, s. greiff, and s. wüstenberg. discovering\\nstudents’ complex problem solving strategies in educational\\nassessment. in proceedings of the 7th conference on\\neducational data mining, 2014.\\ng. wallner and s. kriglstein. visualization-based analysis of\\ngameplay data–a review of literature. entertainment\\ncomputing, 4(3):143–155, 2013.',\n",
       " '23\\n\\n\\x0cadaptive sequential recommendation for discussion\\nforums on moocs using context trees\\nfei mi\\nboi faltings\\nartificial intelligence lab\\nécole polytechnique fédérale de lausanne, switzerland\\nfirstname.lastname@epfl.ch\\nabstract\\n\\nmassive open online courses (moocs) have demonstrated growing popularity and rapid development in recent years. discussion\\nforums have become crucial components for students and instructors to widely exchange ideas and propagate knowledge. it is important to recommend helpful information from forums to students\\nfor the benefit of the learning process. however, students or instructors update discussion forums very often, and the student preferences over forum contents shift rapidly as a mooc progresses.\\nso, mooc forum recommendations need to be adaptive to these\\nevolving forum contents and drifting student interests. these frequent changes pose a challenge to most standard recommendation\\nmethods as they have difficulty adapting to new and drifting observations. we formalize the discussion forum recommendation\\nproblem as a sequence prediction problem. then we compare different methods, including a new method called context tree (ct),\\nwhich can be effectively applied to online sequential recommendation tasks. the results show that the ct recommender performs\\nbetter than other methods for moocs forum recommendation task.\\nwe analyze the reasons for this and demonstrate that it is because\\nof better adaptation to changes in the domain. this highlights the\\nimportance of considering the adaptation aspect when building recommender system with drifting preferences, as well as using machine learning in general.\\n\\nkeywords\\n\\nmoocs forum recommendation, context tree, model adaptation\\n\\n1. introduction\\n\\nwith the increased availability of data, machine learning has become the method of choice for knowledge acquisition in intelligent\\nsystems and various applications. however, data and the knowledge derived from it have a timeliness, such that in a dynamic environment not all the knowledge acquired in the past remains valid.\\ntherefore, machine learning models should acquire new knowledge incrementally and adapt to the dynamic environments. today, many intelligent systems deal with dynamic environments: information on websites, social networks, and applications in com-\\n\\nmercial markets. in such evolving environments, knowledge needs\\nto adapt to the changes very frequently. many statistical machine\\nlearning techniques interpolate between input data and thus their\\nmodels can adapt only slowly to new situations. in this paper,\\nwe consider the dynamic environments for recommendation task.\\ndrifting user interests and preferences [3, 11] are important in building personal assistance systems, such as recommendation systems\\nfor social networks or for news websites where recommendations\\nneed be adaptive to drifting trends rather than recommending obsolete or well-known information. we focus on the application\\nof recommending forum contents for massive open online courses\\n(moocs) where we found that the adaptation issue is a crucial aspect for providing useful and trendy information to students.\\nthe rapid emergence of some mooc platforms and many moocs\\nprovided on them has opened up a new era of education by pushing\\nthe boundaries of education to the general public. in this special online classroom setting, sharing with your classmates or asking help\\nfrom instructors is not as easy as in traditional brick-and-mortar\\nclassrooms. so discussion forums there have become one of the\\nmost important components for students to widely exchange ideas\\nand to obtain instructors’ supplementary information. mooc forums play the role of social learning media for knowledge propagation with increasing number of students and interactions as a course\\nprogresses. every member in the forum can talk about course content with each other, and the intensive interaction between them\\nsupports the knowledge propagation between members of the learning community.\\nthe online discussion forums are usually well structured via the\\ndifferent threads which are created by students or instructors; they\\ncan contain several posts and comments within the topic. an example of the discussion forum from a famous “machine learning”\\ncourse by andre ng on coursera1 is shown in figure 1. the left\\nfigure shows various threads and the right figure illustrates some\\nreplies within the last thread (\"having a problem with the collaborative filtering cost\"). in general, the replies within a thread are\\nrelated to the topic of the thread and they can also refer to some\\nother threads for supplementary information, like the link in the\\nsecond reply. our goal is to point the students towards useful forum threads through effectively mining forum visit patterns.\\ntwo aspects set forum recommendation system for moocs apart\\nfrom other recommendation scenarios. first, student interests and\\npreferences drift fast during the span of a course, which is influenced by the dynamics in forums and the content of the course;\\nsecond, the pool of items to be recommended and the items them1\\n\\nhttps://www.coursera.org/24\\n\\n\\x0cfigure 1: an sample discussion forum. left: sample threads. right: replies within the last thread (\"having a problem with the collaborative filtering cost\").\\nselves are evolving over time because forum threads can be edited\\nvery frequently by either students or instructors. so the recommendations provided to students need to be adaptive to these drifting\\npreferences and evolving items. traditional recommendation techniques, such as collaborative filtering and methods based on matrix factorization, only adapt slowly, as they build an increasingly\\ncomplex model of users and items. therefore, when a new item is\\nsuperseded by a newer version or a new preference pattern appears,\\nit takes time for recommendations to adapt. to better address the\\ndynamic nature of recommendation in moocs, we model the recommendation problem as a dynamic and sequential machine learning problem for the task of predicting the next item in a sequence of\\nitems consumed by a user. during the sequential process, the challenge is combining old knowledge with new knowledge such that\\nboth old and new patterns can be identified fast and accurately. we\\nuse algorithms for sequential recommendation based on variableorder markov models. more specifically, we use a structure called\\ncontext tree (ct) [21] which was originally proposed for lossless\\ndata compression. we apply the ct method for recommending\\ndiscussion forum contents for moocs, where adapting to drifting preferences and dynamic items is crucial. in experiments, it is\\ncompared with various sequential and non-sequential methods. we\\nshow that both old knowledge and new patterns can be captured effectively through context activation using ct, and that this is why\\nit is particularly strong at adapting to drifting user preferences and\\nperforms extremely well for mooc forum recommendation tasks.\\nthe main contribution of this paper is fourfold:\\n• we applied the context tree structure to a sequential recommendation tasks where dynamic item sets and drifting user\\npreferences are of great concern.\\n• analyze how the dynamic changes in user preferences are\\nfollowed in different recommendation techniques.\\n• extensive experiments are conducted for both sequential and\\nnon-sequential recommendation settings. through the experimental analysis, we validate our hypothesis that the ct\\nrecommender adapts well to drifting preferences.\\n\\n• partial context matching (pct) technique, built on top of the\\nstandard ct method, is proposed and tested to generalize to\\nnew sequence patterns, and it further boosts the recommendation performance.\\n\\n2.\\n\\nrelated work\\n\\ntypical recommender systems adopt a static view of the recommendation process and treat it as a prediction problem over all historical\\npreference data. from the perspective of generating adaptive recommendations,we contend that it is more appropriate to view the\\nrecommendation problem as a sequential decision problem. next,\\nwe mainly review some techniques developed for recommender\\nsystems with temporal or sequential considerations.\\nthe most well-known class of recommender system is based on\\ncollaborative filtering (cf) [19]. several attempts have been made\\nto incorporate temporal components into the collaborative filtering\\nsetting to model users’ drifting preferences over time. a common\\nway to deal with the temporal nature is to give higher weights to\\nevents that happened recently. [6, 7, 15] introduced algorithms\\nfor item-based cf that compute the time weightings for different\\nitems by adding a tailored decay factor according to the user’s own\\npurchase behavior. for low dimensional linear factor models, [11]\\nproposed a model called “timesvd” to predict movie ratings for\\nnetflix by modeling temporal dynamics, including periodic effects,\\nvia matrix factorization. as retraining latent factor models is costly,\\none alternative is to learn the parameters and update the decision\\nfunction online for each new observation [1, 16]. [10] applied the\\nonline cf method, coupled with an item popularity-aware weighting scheme on missing data, to recommending social web contents\\nwith implicit feedbacks.\\nmarkov models are also applied to recommender systems to learn\\nthe transition function over items. [24] treated recommendation as\\na univariate time series problem and described a sequential model\\nwith a fixed history. predictions are made by learning a forest of\\ndecision trees, one for each item. when the number of items is big,\\nthis approach does not scale. [17] viewed the problem of generating\\nrecommendations as a sequential decision problem and they con-25\\n\\n\\x0csidered a finite mixture of markov models with fixed weights. [4]\\napplied markov models to recommendation tasks using skipping\\nand weighting techniques for modeling long-distance relationships\\nwithin a sequence. a major drawback of these markov models is\\nthat it is not clear how to choose the order of markov chain.\\nonline algorithms for recommendation are also proposed in several literatures. in [18], a q-learning-based travel recommender is\\nproposed, where trips are ranked using a linear function of several\\nattributes and the weights are updated according to user feedback.\\na multi-armed bandit model called linucb is proposed by [13]\\nfor news recommendation to learn the weights of the linear reward\\nfunction, in which news articles are represented as feature vectors;\\nclick-through rates of articles are treated as the payoffs. [20] proposed a similar recommender for music recommendation with rating feedback, called bayes-ucb, that optimizes the nonlinear reward function using bayesian inference. [14] used a markov decision process (mdp) to model the sequential user preferences for\\nrecommending music playlists. however, the exploration phase of\\nthese methods makes them adapt slowly. as user preferences drift\\nfast in many recommendation setting, it is not effective to explore\\nall options before generating useful ones.\\nwithin the context of recommendation for moocs, [23] proposed\\nan adaptive feature-based matrix factorization framework for course\\nforum recommendation, and the adaptation is achieved by utilizing\\nonly recent features. [22] designed a context-aware matrix factorization model to predict student preferences for forum contents, and\\nthe context considered includes only supplementary statistical features about students and forum contents. in this paper, we focus on\\na class of recommender systems based on a structure, called context tree [21], which was originally used to estimate variable-order\\nmarkov models (vmms) for lossless data compression. then, [2,\\n12, 5] applied this structure to various discrete sequence prediction tasks. recently it was applied to news recommendation by\\n[8, 9]. the most important property of online algorithms is the noregret property, meaning that the model learned online is eventually\\nas good as the best model that could be learned offline. according to [21], the no-regret property is achieved by context trees for\\nthe data compression problem. regret analysis for ct was conducted through simulation by [5] for stochastically generated hidden markov models with small state space. they show that ct\\nachieves the no-regret property when the environment is stationary.\\nas we focus on dynamic recommendation environments with timevarying preferences and limited observations, the no-regret property can be hardly achieved while the model adaptation is a bigger\\nissue for better performance.\\n\\n3.\\n\\ncontext tree recommender\\n\\ndue to the sequential item consumption process, user preferences\\ncan be summarized by the last several items visited. when modeling the process as a fixed-order markov process [17], it is difficult\\nto select the order. a variable-order markov model (vmm), like a\\ncontext tree, alleviates this problem by using a context-dependent\\norder. the context tree is a space efficient structure to keep track\\nof the history in a variable-order markov chain so that the data\\nstructure is built incrementally for sequences that actually occur. a\\nlocal prediction model, called expert, is assigned to each tree node,\\nit only gives predictions for users who have consumed the sequence\\nof items corresponding to the node. in this section, we first introduce how to use the ct structure and the local prediction model for\\nsequential recommendation. then, we discuss adaptation properties and the model complexity of the ct recommender.\\n\\n3.1 the context tree data structure\\n\\nin ct, a sequence s = hn1 , . . . , nl i is an ordered list of items\\nni ∈ n consumed by a user. the sequence of items viewed until\\ntime t is st and the set of all possible sequences s.\\na context s = {s ∈ s : ξ ≺ s} is the set of all possible sequences\\nin s ending with the suffix ξ. ξ is the suffix (≺) of s if last elements\\nof s are equal to ξ. for example, one suffix ξ of the sequence\\ns = hn2 , n3 , n1 i is given by ξ = hn3 , n1 i.\\na context tree t = (v, e) with nodes v and edges e is a partition\\ntree over all contexts of s. each node i ∈ v in the context tree\\ncorresponds to a context si . if node i is the ancestor of node j then\\nsj ⊂ si . initially the context tree t only contains a root node\\nwith the most general context. every time a new item is consumed,\\nthe active leaf node is split into a number of subsets, which then\\nbecome nodes in the tree. this construction results in a variableorder markov model. figure 2 illustrates a simple ct with some\\nsequences over an item set hn1 , n2 , n3 i. each node in the ct corresponds to a context. for instance, the node hn1 i represents the\\ncontext with all sequences end with item n1 .\\n\\nfigure 2: an example context tree. for the sequence s = hn2 , n3 , n1 i,\\nnodes in red-dashed are activated.\\n\\n3.2 context tree for recommendation\\n\\nfor each context si , an expert µi is associated in order to compute\\nthe estimated probability p(nt+1 |st ) of the next item nt+1 under\\nthis context. a user’s browsing history st is matched to the ct and\\nidentifies a path of matching nodes (see figure 2). all the experts\\nassociated with these nodes are called active. the set of active\\nexperts a(st ) = {µi : ξi ≺ st } is the set of experts µi associated\\nto contexts si = {s : ξi ≺ st } such that ξi are suffix of st . a(st )\\nis responsible for the prediction for st .\\n\\n3.2.1\\n\\nexpert model\\n\\nthe standard way for estimating the probability p(nt+1 |st ), as proposed by [5], is to use a dirichlet-multinomial prior for each expert\\nµi . the probability of viewing an item x depends on the number of\\ntimes αxt the item x has been consumed when the expert is active\\nuntil time t. the corresponding marginal probability is:\\npi (nt+1 = x|st ) = p\\n\\nαxt + α0\\nαjt + α0\\n\\n(1)\\n\\nj ∈n\\n\\nwhere α0 is the initial count of the dirichlet prior26\\n\\n\\x0c3.2.2\\n\\ncombining experts to prediction\\n\\nwhen making recommendation for a sequence st , we first identify\\nthe set of contexts and active experts that match the sequence. the\\npredictions given by all the active experts are combined by mixing\\nthe recommendations given by them:\\nx\\np(nt+1 = x|st ) =\\nui (st )pi (nt+1 = x|st )\\n(2)\\n\\nin their old contexts. it allows the model to make predictions using more complex contexts as more data is acquired so that old and\\nnew knowledge can be elegantly combined. for new knowledge\\nor patterns added to an established ct, they can immediately be\\nidentified through context matching. this context organization and\\ncontext matching mechanism help new patterns to be recognized to\\nadapt to changing environments.\\n\\ni∈a(st )\\n\\nthe mixture coefficient ui (st ) of expert µi is computed in eq. 3\\nusing the weight wi ∈ [0, 1]. weight wi is the probability that\\nthe chosen recommendation stops at node i given that the it can be\\ngenerated by the first i experts, and it can be updated in using eq.5.\\n( q\\nwi j:sj ⊂si (1 − wj ), if st ∈ si\\n(3)\\nui (st ) =\\n0,\\notherwise\\nthe combined prediction of the first i experts is defined as qi and\\nit can be computed using the recursion in eq. 4. the recursive\\nconstruction that estimates, for each context at a certain depth i,\\nwhether it makes better prediction than the combined prediction\\nqi−1 from depth i − 1.\\nqi = wi pi (nt+1 = x|st ) + (1 − wi )qi−1\\n\\n(4)\\n\\nthe weights are updated by taking into account the success of a\\nrecommendation. when a user consumes a new item x, we update\\nthe weights of the active experts corresponding to the suffix ending\\nbefore x according to the probability qi (x) of predicting x sequentially via bayes’ theorem. the weights are updated in closed form\\nin eq. 5, and a detailed derivation can be found in [5].\\nwi0 =\\n\\n3.2.3\\n\\nwi pi (nt+1 = x|st )\\nqi (x)\\n\\n(5)\\n\\nct recommender algorithm\\n\\nthe whole recommendation process first goes through all users’ activity sequences over time incrementally to build the ct; the local\\nexperts and weights updated using equations 1 and 5 respectively.\\nas users browse more contents, more contexts and paths are added\\nand updated, thus building a deeper, more complete ct. the recommendation for an activity or context in a sequence is generated\\nusing eq. 2 continuously as experts and weights are updated. at\\nthe same time, a pool of candidate items is maintained through a\\ndynamically evolving context tree. as new items are added, new\\nbranches are created. at the same time, nodes corresponding to old\\nitems are removed as soon as they disappear from the current pool.\\nthe ct recommender is a mixture model. on the one hand, the\\nprediction p(nt+1 = x|st ) is a mixture of the predictions given\\nby all the activated experts along the activated path so that it’s a\\nmixtures of local experts or a mixture of variable order markov\\nmodels whose oder are defined by context depths. on the other\\nhand, one path in a ct can be constructed or updated by multiple\\nusers so that it’s a mixture of users’ preferences.\\n\\n3.3\\n\\nadaptation analysis\\n\\nour hypothesis, which is validated in later experiments, is that the\\nct recommender can be applied elegantly to domains where adaptation and timeliness are of concern. two properties of the ct\\nmethods are crucial to the goal. first, the model parameter learning process and recommendations generated are online such that\\nthe model adapts continuously to a dynamic environment. second,\\nadaptability can be achieved by the ct structure itself as knowledge is organized and activated by context. new items or paths are\\nrecognized in new contexts, whereas old items can still be accessed\\n\\n3.4 complexity analysis\\n\\nlearning ct uses the recursive update defined in eq. 4 and recommendations are generated by weighting the experts’ predictions\\nalong the activated path given by eq. 2. for trees of depth d, the\\ntime complexity of model learning and prediction for a new observation are both o(d). for input sequence of length t , the updating and recommending complexity are o(m 2 ), where m =\\nmin(d, t ). space complexity in the worst case is exponential to\\nthe depth of the tree. however, as we do not generate branches\\nunless the sequence occurs in the input, we achieve a much lower\\nbound determined by the total size of the input. so the space complexity is o(n ), where n is the total number of observations.\\ncompared with the way that markov models are learned, in which\\nthe whole transition matrix needs to be learned simultaneously, the\\nspace efficiency of ct offers us an advantage for model learning.\\nfor tasks that involve very long sequences, we can limit the depth\\nd of the ct for space and time efficiency.\\n\\n4.\\n\\ndataset and problem analysis\\n\\n4.1 dataset description\\n\\nin this paper, we work with recommending discussion forum threads\\nto mooc students. a forum thread can be updated frequently and\\nit contains multiple posts and comments within the topic. as we\\nmentioned before that the challenge is adapting to drifting user\\npreferences and evolving forum threads as a course progresses. for\\nthe experiments elaborated in the following section, we use forum\\nviewing data from three courses offered by école polytechnique\\nfédérale de lausanne on coursera. these three courses include\\nthe first offering of “digital signal processing”, the third offering of “functional program design in scala”, and the first offering of “’reactive programming’. they are referred to course 1,\\ncourse 2 and course 3. some discussion forum statistics for the\\nthree courses are given in table 1. from the number of forum\\nparticipants, forum threads, and thread views, we can see that the\\ncourse scale increase from course 1 to course 3. a student on\\nmoocs often accesses course forums many times during the span\\nof a mooc. each time the threads she views are tracked as one\\nvisit session by the web browser. the total number of visit sessions\\nand the average session lengths for three courses are presented in\\ntable 1. the length of a session is the number of threads she viewed\\nwithin a visit session. the thread viewing sequences corresponding to these regular visit sessions are called separated sequences\\nin our later experiments and they treat threads in one visit session\\nas one sequence. models built using separated sequences try to\\ncatch short-term patterns within one visit session and we do not\\ndifferentiate the patterns from different students. another setting,\\ncalled combined sequences, concatenates all of a student’s visit sessions into one longer sequence so that models built using combined\\nsequences try to learn long-term patterns across students. the average length of combined sequences is the average session length\\ntimes the average number of sessions per student. from course 1\\nto course 3, average lengths for separated and combined sequences\\nboth increase.27\\n\\n\\x0c# of forum participants\\n# of forum threads\\n# of thread views\\n# of sessions\\navg. session length\\navg. # of sessions per student\\n\\ncourse 1 course 2 course 3\\n5,399\\n12,384\\n13,914\\n1,116\\n1,646\\n2,404\\n130,093 379,456 777,304\\n19,892\\n40,764\\n30,082\\n6.5\\n9\\n25.8\\n3.7\\n3.3\\n2.2\\n\\ntable 1: course forum statistics for three datasets.\\nanother important issue that we can discover from the statistics is\\nthat thread viewing data available for sequential recommendation is\\nvery sparse. for example in course 1, the average session length is\\n6.5 and the number of threads is around 1116. then the complete\\nspace to be explored will be 11166.5 , which is much larger than\\nthe size of observations (130,093 thread views). the similar data\\nsparsity issue is even more severe in the other two datasets.\\n\\n4.2\\n\\nforum thread view pattern\\n\\nnext, we study the thread viewing pattern which highlights the significance of adaptation issues for thread recommendation. figure\\n3 illustrates the distribution of thread views against freshness for\\nthree courses. the freshness of an item is defined as the relative\\ncreation order of all items that have been created so far. for example, when a student views a thread tm which is the m-th thread\\ncreated in the currently existing pool of n threads, then freshness\\nof tm is defined as:\\nm\\n(6)\\nf reshness =\\nn\\nwe can see from figure 3 that there is a sharp trend that the new\\nforum threads are viewed much more frequently than the old ones\\nfor all three courses. it is mainly due to the fact that fresh threads\\nare closely relevant to the current course progress. moreover, fresh\\nthreads can also supersede the contents in some old ones to be\\nviewed. this tendency to view fresh items leads to drifting user\\npreferences. such drifting preferences, coupled with the evolving nature of forum contents, requires recommendations adaptive\\nto drifting or recent preferences.\\n\\nold. in general, sequential patterns are observed more often within\\nspecific threads as some specific follow-up threads might be related\\nand useful to the one that you are viewing. so the patterns learned\\ncould be used to guide your forum browsing process. on the contrary, sequential patterns on general threads are relatively random\\nand imperceptible.\\ngeneral threads\\n“using gnu octave”\\n“any one from india??”\\n“where is everyone from?\\n“numerical examples in pdf”\\n“how to get a certificate”\\n\\nspecific threads\\n“homework day 1 / question 9”\\n“quiz for module 4.2”\\n“quiz -1 question 04”\\n‘homework 3, question 11”\\n“week 1: q10 gema problem”\\n\\ntable 2: sample thread titles of general and specific threads.\\n\\n5. results and evaluation\\n\\nin this section, we compare the proposed ct method against various baseline methods in both non-sequential and sequential settings. the results show that the ct recommender performs better\\nthan other methods under different setting for all three moocs\\nconsidered. through the adaptation analysis, we validate our hypothesis that the superior performance of ct recommender comes\\nfrom the adaptation power to drifting preferences and trendy patterns in the domain. in the end, a regularization technique for ct,\\ncalled partial context matching (pct), is introduced. it is demonstrated that pct helps better generalize among sequence patterns\\nand further boost performance.\\n\\n5.1 baseline methods\\n\\n5.1.1 non-sequential methods\\n\\nfigure 3: thread viewing activities against freshness\\n\\nmatrix factorization methods proposed by [23, 22] are the state-ofthe-art for moocs course content recommendation. besides the\\nuser-based mf given in [23], we also consider item-based mf that\\ngenerates recommendations based on the similarity of the latent\\nitem features learned from standard mf. in our case, each entry in\\nthe user-item matrix of mf contains the number of times a student\\nviews a thread. we also test a version where the matrix had a 1 for\\nany number of views, but the performance was not as good, so the\\ndevelopment of this version was not taken any further. mf models considered here are updated periodically (week-by-week). to\\nenable a fair comparison against non-sequential matrix factorization techniques, we implemented versions where the ct model is\\nupdated at fixed time intervals, equal to those of the mf models.\\nin the “one-shot ct” version, we compute the ct recommendations for each user based on the data available at the time of the\\nmodel update, and the user then receives these same recommendations at every future time step until the next update. this mirrors\\nthe conditions of user-based mf. to compare with item-based mf,\\nthe “slow-update ct” version updates the recommendations, but\\nnot the model, at each time point based on the sequential forum\\nviewing information available at that time.\\n\\na further investigation through those views on old threads leads us\\nto a classification of threads into two categories: general threads\\nand specific threads. some titles of the general and specific threads\\nare listed in table 2. we could see the clear difference between\\nthese two classes of threads as the general ones corresponds to\\nbroad topics and specific ones are related to detailed course contents or exercises. we also found that only a very small part of the\\nold threads are still rather active to be viewed and they are mostly\\ngeneral ones. different from general threads, specific threads that\\nsubject to a fine timeliness are viewed very few times after they get\\n\\nsequential methods update model parameters and recommendations continuously as items are consumed. the first two simple\\nmethods are based on the observation and heuristic that fresh threads\\nare viewed much frequently than old ones. fresh_1 recommends\\nthe last 5 updated threads, and fresh_2 recommends the last 5 created threads. another baseline method, referred as popular, recommends the top 5 threads among the last 100 threads viewed before\\nthe current one. we also consider an online version of mf [10] that\\n\\ndistribution of thread views against freshness\\n\\n0.3\\n\\nprobability\\n\\n0.25\\n\\ncourse 1\\ncourse 2\\ncourse 3\\n\\n0.2\\n0.15\\n0.1\\n0.05\\n0\\n0\\n\\n0.2\\n\\n0.4\\n\\n0.6\\n\\n0.8\\n\\n1\\n\\nfreshness\\n\\n5.1.2 sequential methods28\\n\\n\\x0c40%\\n\\n60%\\n\\n30%\\n20%\\n10%\\n0%\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n50%\\n40%\\n\\noverall perforamance (course 2)\\n\\n60%\\n\\nct\\nslow-update ct\\none-shot ct\\nitem-base mf\\nuser-based mf\\n\\nsucc@5ahead\\n\\n50%\\n\\noverall perforamance (course 1)\\nct\\nslow-update ct\\none-shot ct\\nitem-base mf\\nuser-based mf\\n\\nsucc@5ahead\\n\\nsucc@5ahead\\n\\n60%\\n\\n30%\\n20%\\n10%\\n0%\\n1\\n\\n2\\n\\n3\\n\\nweek\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n50%\\n40%\\n\\noverall perforamance (course 3)\\nct\\nslow-update ct\\none-shot ct\\nitem-base mf\\nuser-based mf\\n\\n30%\\n20%\\n10%\\n0%\\n1\\n\\n2\\n\\n3\\n\\nweek\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\nweek\\n\\nfigure 4: overall performance comparison of ct and non-sequential methods\\nis currently the state-of-the-art sequential recommendation method,\\nreferred to “online-mf”, in which the corresponding latent factor\\nof the item i and user u are updated when a new observation rui\\narrive. the model optimization is implemented based on elementwise alternating least squares. the number of latent factors is\\ntuned to be 15, 20, 25 for three datasets, and the regularization parameter is set as 0.01. moreover, the weight of a new observation is\\nthe same as old ones during optimization for achieving the best performance. furthermore, the proposed ct recommender refers to\\nthe full context tree algorithm with a continuously updated model.\\n\\n5.2\\n\\n5.2.1\\n\\nperformance and adaptation analysis\\nevaluation metrics\\n\\nin our case, all methods recommend top-5 threads each time. two\\nevaluation metrics are adopted in the following experiments:\\n• succ@5: the mean average precision (map) of predicting\\nthe immediately next thread view in a sequence.\\n• succ@5ahead: the map of predicting the future thread\\nviews within a sequence. in this case, a recommendation\\nis successful even if it is viewed later in a sequence.\\n\\n5.2.2\\n\\ncomparison of non-sequential methods\\n\\nfigure 4 shows the performance comparison between different versions of methods based on mf and ct on three datasets. “ct”\\nis the sequential method with a continuously updated model, and\\nall other methods figure 4 are non-sequential versions. combined\\nsequences are used for the ct methods here to have a parallel comparison against mf. we found that a small value of the depth limit\\nof the cts hurts performances, yet a very large depth limit does\\nnot increase performance at the cost of computation and memory.\\nthrough experiments, we tune depths empirically and set them as\\n15, 20, 30 for three datasets.\\namong non-sequential methods, one-shot ct and user-based mf\\nperform the worst for all three courses, which means that recommending the same content for the next week without any sequence\\nconsideration is ineffective. slow-update ct performs consistently\\nthe best among non-sequential methods, and it proves that adapting\\nrecommendations through context tree helps boost performance although the model itself is not updated continuously. compared\\nto slow-update ct, item-based mf performs much worse. they\\nboth update model parameters periodically and the recommendations are adjusted given the current observation. however, using\\nthe contextual information within a sequence and the corresponding prediction experts of slow-update ct are much more powerful\\nthan just using latent item features of item-base mf. moreover, we\\ncan clearly see that the normal ct with continuous update outperforms all other non-sequential methods by a large margin for three\\ndatasets. it means that drifting preferences need to be followed\\nthough continuous and adaptive model update, so sequential methods are better choices. next, we focus on sequential methods, and\\n\\nwe validate our hypothesis that the ct model has superior performances because it better handles drifting user preferences.\\n\\n5.2.3\\n\\ncomparison of sequential methods\\n\\nthe results presented in table 3 show the performance of the full\\nct recommender compared with other sequential baseline methods under different settings and evaluation metrics. each result\\ntuple contains the performance on the three datasets. we also consider a tail performance metric, referred to personalized evaluation,\\nwhere the most popular threads (20, 30, and 40 for three courses)\\nare excluded from recommendations. the depth limits of cts using separated sequences are set to 8, 10, and 15 for three courses.\\nwe notice that the online-mf method, with continuous model update, performs much worse compared with the ct recommender\\nfor all three datasets. this result shows that matrix factorization,\\nwhich is based on interpolation over the user-item matrix, is not\\nsensitive enough to rapidly drifting preferences with limited observations. the performances of two versions of the fresh recommender are comparable with online-mf, and fresh_1 even outperforms online-mf in many cases, especially for succ@5ahead.\\nit means that simply recommending fresh items even does a better job than online-mf for this recommendation task with drifting\\npreferences. we can see that the ct recommender outperforms\\nall other sequential methods under various settings, except for using non-personalized succ@5ahead for course 2. the popular\\nrecommender is indeed a very strong contender when using nonpersonalized evaluation since there is a bias that students can click a\\n“top threads” tag from user interface to view popular threads which\\nare similar to the ones given by popular recommender. from the\\neducational perspective, the setting using separated sequences and\\npersonalized evaluation is the most interesting as it reflects shotterm visiting patterns within a session over those specific and less\\npopular forum threads. we could see from the upper right part of\\ntable 3 that the ct recommender outperforms all other methods by\\na large margin under this setting.\\n\\nct\\nonline-mf\\npopular\\nfresh_1\\nfresh_2\\nct\\nonline-mf\\npopular\\nfresh_1\\nfresh_2\\n\\nnon-personalized\\npersonalized\\nsucc@5 succ@5ahead succ@5 succ@5ahead\\nseparated sequences\\n[25, 23, 21]% [48, 53, 52]% [19, 14, 16]% [41, 37, 42]%\\n[15, 12, 8]% [33, 29, 23]% [10, 7 ,6 ]% [27, 25, 20]%\\n[15, 20, 16]% [40, 61, 51]% [9, 8 ,8 ]% [34, 31, 36]%\\n[12, 14, 10]% [37, 43, 41]% [10, 10, 8]% [33, 31, 37]%\\n[9, 8, 6]% [31, 31, 29]% [8, 7, 6 ]% [30, 30, 28]%\\ncombined sequences\\n[21, 20, 20]% [55, 55, 56]% [16, 13, 14]% [46, 39, 46]%\\n[9, 8, 7]% [34, 27, 23]%\\n[7,6,6]%\\n[29, 24, 20]%\\n[13, 14, 14]% [52, 62, 58]% [9, 8, 7]% [45, 36, 43]%\\n[10, 12, 9]% [48, 44, 44]% [8, 9, 8]% [44, 34, 42]%\\n[7, 6, 6]% [43, 34, 32]% [6, 6, 6]% [42, 32, 31]%\\n\\ntable 3: performance comparison of sequential methods\\n\\n5.2.4\\n\\nadaptation comparison29\\n\\n\\x0cct\\nonline-mf\\n\\n0.8\\n0.6\\n0.4\\n0.2\\n0\\n0\\n\\n0.2\\n\\naverage cdf of recommendation freshness (course 2)\\n\\n0.4\\n\\n0.6\\n\\n0.8\\n\\n1\\n\\n1\\n\\naverage cdf of recommendation freshness (course 3)\\n\\nct\\nonline-mf\\n\\nrecommended probability\\n\\n1\\n\\nrecommended probability\\n\\nrecommended probability\\n\\naverage cdf of recommendation freshness (course 1)\\n\\n0.8\\n0.6\\n0.4\\n0.2\\n0\\n0\\n\\n0.2\\n\\nfreshness\\n\\n0.4\\n\\n0.6\\n\\n0.8\\n\\n1\\n\\n1\\n\\nct\\nonline-mf\\n\\n0.8\\n0.6\\n0.4\\n0.2\\n0\\n0\\n\\n0.2\\n\\nfreshness\\n\\n0.4\\n\\n0.6\\n\\n0.8\\n\\n1\\n\\nfreshness\\n\\nfigure 5: distribution of recommendation freshness of ct and online-mf\\n0.4\\n\\nct\\nonline-mf\\n\\n0.2\\n0.1\\n0\\n0\\n\\n0.4\\n\\n0.3\\n\\nprobability\\n\\nprobability\\n\\n0.3\\n\\np (success|f reshness) for course 2\\nct\\nonline-mf\\n\\n0.2\\n0.1\\n\\n0.2\\n\\n0.4\\n\\n0.6\\n\\n0.8\\n\\n1\\n\\n0\\n0\\n\\nfreshness\\n\\np (success|f reshness) for course 3\\nct\\nonline-mf\\n\\n0.3\\n\\nprobability\\n\\n0.4\\n\\np (success|f reshness) for course 1\\n\\n0.2\\n0.1\\n\\n0.2\\n\\n0.4\\n\\n0.6\\n\\n0.8\\n\\n1\\n\\n0\\n0\\n\\nfreshness\\n\\n0.2\\n\\n0.4\\n\\n0.6\\n\\n0.8\\n\\n1\\n\\nfreshness\\n\\nfigure 6: conditional success rate of ct and online-mf\\nafter seeing the superior performance of the ct recommender, we\\nmove to an insight analysis of the results. to be specific, we compare ct and online-mf in terms of their adaptation capabilities\\nto new items. figure 5 illustrates the cumulative density function (cdf) of the threads recommended by different methods against\\nthread freshness. we can see that the cdfs of ct increase sharply\\nwhen thread freshness increases, which means that the probability\\nof recommending fresh items is high compared to online-mf. in\\nother words, ct recommends more fresh items than online-mf. as\\nwe mentioned before that a large portion of fresh threads are specific ones, instead of general ones, so ct recommends more specific and trendy threads to students while methods based on matrix\\nfactorization recommend more popular and general threads.\\nother than the quantity of recommending fresh and specific threads,\\nthe quality is crucial as well. figure 6 shows the conditional success rate p (success|f reshness) across different degrees of freshness for three courses. p (success|f reshness) is defined as the\\nfraction of the items successfully recommended given the item freshness. for instance, if an item with freshness 0.5 is viewed 100 times\\nthroughout a course, then p (success|f reshness = 0.5) = 0.25\\nmeans it is among the top 5 recommended items 25 times. as\\nthe freshness increases, the conditional success rate of online-mf\\ndrops speedily while the ct method keeps a solid and stable performance. it is significant that ct outperforms online-mf by a\\nlarge margin when freshness is high, in other words, it is particularly strong for recommending fresh items. fresh items are often\\nnot popular in terms of the total number of views at the time point\\nof recommendation. so identifying fresh items accurately implies\\na strong adaptation power to new and evolving forum visiting patterns. the analysis above validates our hypothesis that the ct recommender can adapt well to drifting user preferences. another\\nconclusion drawn from figure 6 is that the performance of ct is as\\ngood as online-mf for items with low freshness. this is because\\nthat the context organization and context matching mechanism help\\nold items to be identifiable though old contexts. to conclude, ct is\\nflexible at combining old knowledge and new knowledge so that it\\nperformances well for items with various freshness, especially for\\nfresh ones with drifting preferences.\\n\\n5.3\\n\\npartial context matching (pct)\\n\\nat last, we introduce another technique, built on top of the standard ct, to generalize to new sequence patterns and further boost\\nthe recommendation performance. the standard ct recommender\\nadopts a complete context matching mechanism to identify active\\nexperts for a sequence s. that is, active experts of s come exactly\\nfrom the set of suffixes of s. we design a partial context matching (pct) mechanism where active experts of a sequence are not\\nconstrained by exact suffixes, yet they can be those very similar\\nones. two reasons bring us to design the pct mechanism for context tree learning. first, pct mechanism is a way of adding regularization. sequential item consumption process does not have to\\nfollow exactly the same order, and slightly different sequences are\\nalso relevant for both model learning and recommendation generation. second, the data sparsity issue we discussed before for sequential recommendation setting can be solved to some extent by\\nconsidering similar contexts for learning model experts. the way\\npct does aims to activate more experts to train the model, and to\\ngenerate recommendations from a mixture of similar contexts.\\nwe will focus on a skip operation that we add on top of the standard\\nct recommender. some complex operations, like swapping item\\norders, are also tested, but they do not generate better performance.\\nfor a sequence hsp , . . . , s1 i with length p, the skip operation generates p candidate partially matched contexts that skip one sk for\\nk ∈ [1 . . . p]. all the contexts on the paths from root to partially\\nmatched contexts are activated. for example, the path to context\\nhn2 , n1 i can be activated from the context hn2 , n3 , n1 i by the skipping n3 . however, for each partially matched context, there may\\nnot exist a fully matched path in the current context tree. in this\\ncase, for each partially matched context, we identify the longest\\npath that corresponds it with length q. if q/p is larger than some\\nthreshold t, we update experts on this paths and use them to generate recommendations for the current observation. predictions from\\nmultiple paths are combined by averaging the probabilities.\\n\\npct-0.5\\npct-0.6\\npct-0.7\\npct-0.8\\npct-0.9\\n\\nsuccess@5\\n[+0.4, +0.6, +0.2]%\\n[+0.5, +0.8, +0.3]%\\n[+0.7, +0.9, +0.5]%\\n[+0.8, +1.1, +0.6]%\\n[+1.0, +1.4, +0.7]%\\n\\nsuccess@5ahead\\n[+0.8, +0.9, +0.4]%\\n[+1.1, +1.3, +0.5]%\\n[+1.6, +1.9, +0.7]%\\n[+1.9, +2.4, +1.0]%\\n[+2.0, +2.7, +1.3]%\\n\\nratio\\n[4.9, 4.5, 3.3]\\n[4.4, 4.1, 2.9]\\n[3.7, 3.2, 2.5]\\n[3.2, 2.9, 2.1]\\n[2.4, 2.2, 1.4]\\n\\ntable 4: performance comparison of pct against ct for three courses30\\n\\n\\x0ctable 4 shows the performance of applying pct for both model\\nupdate and recommendation with threshold t (pct-t). results are\\ncompared with the full ct recommender with separated sequences\\nand non-personalized evaluation. for cases where the threshold is\\nsmaller than 0.5, we sometimes obtain negative results since partially matched contexts are too short to be relevant. the “ratio”\\ncolumn is the ratio of the number of updated paths in pct compared with standard ct. we can see that pct updates more paths\\nand it offers us consistent performance boosts at the cost of computation.\\n\\n6. conclusion and future work\\n\\nin this paper, we formulate the mooc forum recommendation\\nproblem as a sequential decision problem. through experimental\\nanalysis, both performance boost and adaptation to drifting preferences are achieved using a new method called context tree. furthermore, a partial context matching mechanism is studied to allow a\\nmixture of different but similar paths. as a future work, exploratory\\nalgorithms are interesting to be tried. as exploring all options for\\nall contexts are not feasible, we consider to explore only those top\\noptions from similar contexts. deploying the ct recommender in\\nsome moocs for online evaluation would be precious to obtain\\nmore realistic evaluation.\\n\\n7. references\\n\\n[1] j. abernethy, k. canini, j. langford, and a. simma. online\\ncollaborative filtering. university of california at berkeley,\\ntech. rep, 2007.\\n[2] r. begleiter, r. el-yaniv, and g. yona. on prediction using\\nvariable order markov models. journal of artificial\\nintelligence research, pages 385–421, 2004.\\n[3] r. m. bell, y. koren, and c. volinsky. the bellkor 2008\\nsolution to the netflix prize. statistics research department\\nat at&t research, 2008.\\n[4] g. bonnin, a. brun, and a. boyer. a low-order markov\\nmodel integrating long-distance histories for collaborative\\nrecommender systems. in international conference on\\nintelligent user interfaces, pages 57–66. acm, 2009.\\n[5] c. dimitrakakis. bayesian variable order markov models. in\\ninternational conference on artificial intelligence and\\nstatistics, pages 161–168, 2010.\\n[6] y. ding and x. li. time weight collaborative filtering. in\\nacm international conference on information and\\nknowledge management, pages 485–492. acm, 2005.\\n[7] y. ding, x. li, and m. e. orlowska. recency-based\\ncollaborative filtering. in australasian database conference,\\npages 99–107. australian computer society, inc., 2006.\\n[8] f. garcin, c. dimitrakakis, and b. faltings. personalized\\nnews recommendation with context trees. in acm\\nconference on recommender systems, pages 105–112.\\nacm, 2013.\\n[9] f. garcin, b. faltings, o. donatsch, a. alazzawi, c. bruttin,\\nand a. huber. offline and online evaluation of news\\nrecommender systems at swissinfo.ch. in acm conference\\non recommender systems, pages 169–176. acm, 2014.\\n[10] x. he, h. zhang, m.-y. kan, and t.-s. chua. fast matrix\\nfactorization for online recommendation with implicit\\nfeedback. in international acm conference on research and\\ndevelopment in information retrieval, volume 16, 2016.\\n[11] y. koren. collaborative filtering with temporal dynamics.\\ncommunications of the acm, 53(4):89–97, 2010.\\n\\n[12] s. s. kozat, a. c. singer, and g. c. zeitler. universal\\npiecewise linear prediction via context trees. ieee\\ntransactions on signal processing, 55(7):3730–3745, 2007.\\n[13] l. li, w. chu, j. langford, and r. e. schapire. a\\ncontextual-bandit approach to personalized news article\\nrecommendation. in international conference on world\\nwide web, pages 661–670. acm, 2010.\\n[14] e. liebman, m. saar-tsechansky, and p. stone. dj-mc: a\\nreinforcement-learning agent for music playlist\\nrecommendation. in international conference on\\nautonomous agents and multiagent systems, pages 591–599.\\nifaamas, 2015.\\n[15] n. n. liu, m. zhao, e. xiang, and q. yang. online\\nevolutionary collaborative filtering. in acm conference on\\nrecommender systems, pages 95–102. acm, 2010.\\n[16] j. mairal, f. bach, j. ponce, and g. sapiro. online learning\\nfor matrix factorization and sparse coding. journal of\\nmachine learning research, 11(jan):19–60, 2010.\\n[17] g. shani, r. i. brafman, and d. heckerman. an mdp-based\\nrecommender system. in conference on uncertainty in\\nartificial intelligence, pages 453–460. morgan kaufmann\\npublishers inc., 2002.\\n[18] a. srivihok and p. sukonmanee. e-commerce intelligent\\nagent: personalization travel support agent using\\nq-learning. in international conference on electronic\\ncommerce, pages 287–292. acm, 2005.\\n[19] x. su and t. m. khoshgoftaar. a survey of collaborative\\nfiltering techniques. advances in artificial intelligence,\\n2009:4, 2009.\\n[20] x. wang, y. wang, d. hsu, and y. wang. exploration in\\ninteractive personalized music recommendation: a\\nreinforcement learning approach. acm transactions on\\nmultimedia computing, communications, and applications,\\n11(1):7, 2014.\\n[21] f. m. willems, y. m. shtarkov, and t. j. tjalkens. the\\ncontext-tree weighting method: basic properties. ieee\\ntransactions on information theory, 41(3):653–664, 1995.\\n[22] d. yang, d. adamson, and c. p. rosé. question\\nrecommendation with constraints for massive open online\\ncourses. in acm conference on recommender systems,\\npages 49–56. acm, 2014.\\n[23] d. yang, m. piergallini, i. howley, and c. rose. forum\\nthread recommendation for massive open online courses. in\\neducational data mining, 2014.\\n[24] a. zimdars, d. m. chickering, and c. meek. using temporal\\ndata for making recommendations. in conference on\\nuncertainty in artificial intelligence, pages 580–588.\\nmorgan kaufmann publishers inc., 2001.',\n",
       " '127\\n\\n\\x0cassessing computer literacy of adults with low literacy\\nskills\\nandrew m. olney\\n\\ninstitute for intelligent systems\\nuniversity of memphis\\nmemphis, tn 38152\\n\\naolney@memphis.edu\\ndaphne greenberg\\n\\ndepartment of educational psychology, special\\neducation, and communication disorders\\ngeorgia state university\\natlanta, ga 30302\\n\\ndgreenberg@gsu.edu\\n\\nabstract\\nadaptive learning technologies hold great promise for improving the reading skills of adults with low literacy, but\\nadults with low literacy skills typically have low computer\\nliteracy skills. in order to determine whether adults with\\nlow literacy skills would be able to use an intelligent tutoring system for reading comprehension, we adapted a 44 task\\ncomputer literacy assessment and delivered it to 114 adults\\nwith reading skills between 3rd and 8th grade levels. this\\npaper presents four analyses on these data. first, we report\\nthe pass/fail data natively exported by the assessment for\\nparticular computer-based tasks. second, we undertook a\\ngoms analysis of each computer-based task, to predict the\\ntask completion time for a skilled user, and found that it\\nnegatively correlated with proportion correct for each item,\\nr(42) = −.4, p = .01. third, we used the goms task decomposition to develop a q-matrix of component computer\\nskills for each task, and using logistic mixed effects models\\non this matrix identified five component skills highly predictive of the success or failure of an individual on a computer task: function keys, typing, using icons, right clicking,\\nand mouse dragging. and finally, we assessed the predictive\\nvalue of all component skills using logistic lasso.\\n\\nkeywords\\nadult literacy, computer literacy, goms, q-matrix, mixed\\nmodel, lasso\\n\\n1. introduction\\nof adults with the lowest literacy levels, 43% live in poverty,\\nand low literacy costs the u.s. economy $225 billion annu-\\n\\ndariush bakhtiari\\n\\ndepartment of educational psychology, special\\neducation, and communication disorders\\ngeorgia state university\\natlanta, ga 30302\\n\\ndbakhtiari1@gsu.edu\\nart graesser\\n\\ninstitute for intelligent systems\\nuniversity of memphis\\nmemphis, tn 38152\\n\\na-graesser@memphis.edu\\nally [14]. the need for literacy interventions is matched\\nby the complexity of delivering interventions to this population. low literacy adults have difficulty attending face\\nto face programs at literacy centers because of work, child\\ncare, and transportation [5], and even when these challenges\\nare met, two-thirds of literacy centers have long waiting\\nlists [14]. adaptive computer-based interventions for literacy hold promise to overcome these challenges. such interventions can be deployed in homes and local libraries, in\\naddition to literacy centers. however, computer-based interventions raise another question: can adults with low literacy\\nskills use computers well enough to benefit? several surveys\\nsuggest that this might be a problem. the demographics\\nmost affected by low literacy are the same demographics\\nleast likely to use the internet (over age 50, making less\\nthan $30 thousand a year, and with less than a high school\\neducation [1]).\\nseveral decades of research have investigated computer literacy using self-report measures as well as objective tests,\\ni.e. multiple choice, and find that self-report measures tend\\nto exaggerate proficiency while objective tests are more reliable (see [3] for a review). for an adult literacy population, however, multiple-choice tests delivered as print create\\nadditional concerns as to whether the questions themselves\\ncan be comprehended. recently a new type of assessment,\\nknown as the northstar digital literacy assessment (the\\nnorthstar), has been created that directly measures ability\\nto perform computer tasks [13]. unlike multiple choice assessments, the northstar can simulate a computer desktop,\\nuse voice prompts to instruct users to perform tasks on that\\ndesktop, and then record their mouse clicks and keystrokes\\nto determine if the task has been completed. almost all\\nof the tasks can be completed without reading by listening\\nto the voice prompt instructions. the few tasks that do\\ninvolve reading are word recognition tasks rather than sentence reading, e.g. a task to log in may require the user\\nto copy a name and password to the appropriate boxes and\\nso require reading of “username,” “password,” and the corresponding fillers. the northstar has been adopted as the\\ncomputer literacy standard for adult basic education in the128\\n\\n\\x0cstate of minnesota, which further supports its appropriateness for assessing the computer literacy skills of adults with\\nlow literacy skills.\\nthe present study investigated the computer literacy skills\\nof adults with low literacy skills for the purpose of developing an intelligent tutoring system for reading comprehension for this population [7]. it includes a set of northstar\\nitems that were collected to cover a range of potential interface and interaction components. in the remainder of the\\npaper we describe the data collection procedure and four\\nanalyses performed, including pass/fail frequencies for each\\ntask, relation of these frequencies to goms-predicted execution times for skilled users, a logistic mixed-model using a\\nq-matrix decomposition of the tasks into component skills,\\nand a logistic lasso model to assess the predictive value of\\ncomponent skills. from these analyses we identify specific\\ntasks that are problematic for adults with low literacy skills\\nas well as component skills that make it more likely adults\\nwith low literacy skills will succeed or fail at a computerbased task.\\n\\n2. analysis 1: proportion correct\\n2.1 participants\\nparticipants (n = 114) were recruited through adult literacy\\ncenters in atlanta, ga and toronto, on, from classes where\\nthe reading level was between 3rd and 8th grade. reading\\nlevel was determined by the centers using their “business as\\nusual” assessments. demographic surveys were completed\\nby 90 participants (79% completion rate). completed surveys indicated that participants were slightly more female\\nthan male (55 vs. 35) and that participant age ranged from\\n17 to 69 (m = 42.74, sd = 13.73).\\n\\n2.2\\n\\nmaterials\\n\\nforty-four items were selected from four (out of seven) of\\nthe northstar modules available at the time of the study:\\nbasic computer skills (21), www (13), windows (6), and\\nemail (4). task descriptions are given in table 1. basic\\ncomputer skills covered such topics as turning a computer\\non, identifying components of a computer, files and folders, menus, and windows. www focused on browser-based\\nactivities like searching, search results, browser functionalities, and logging in. although the windows module focused\\non windows overall, the items selected were fairly generic\\nto any windowed operating system and mostly pertained to\\ndesktop applications. email questions used a webmail interface (browser-based email client) and queried how one would\\ncreate a new email, send an email, or similar email task.\\nbecause northstar modules are integrated assessments, the\\nnorthstar project compiled the items we selected into a custom assessment for us.\\n\\n2.3\\n\\nprocedure\\n\\nparticipants first completed informed consent and then the\\ndemographic survey. both informed consent and demographic\\nsurvey were read aloud to participants to ensure comprehension. participants were then asked to sit in front of a computer to take the northstar assessment. the assessment was\\ndelivered in the browser using adobe flash. at the start of\\nthe assessment, a 3-minute orientation video was played explaining how to answer questions in the assessment. if the\\n\\nparticipant was confused about what to do, an experimenter\\nwas available to answer questions. each question consisted\\nof an voice prompt defining the task, which was also written at the top of the screen. a replay button was available\\nto repeat the prompt. participants could select, click, type,\\ndrag, etc. on the interface in an attempt to perform the task.\\nif the participant did not know how to complete the task,\\nthey could press an “i don’t know” button, at which point\\nthe system scored their attempt as a failure. attempts were\\nonly scored as a success if the participant completed the task\\nin the manner requested in the prompt. the completion of\\neach task initiated the next task until the assessment was\\ncomplete.\\n\\n2.4 results & discussion\\nthe northstar records success/failure of each participant on\\neach task, and these data are reported in detail elsewhere [2].\\nhere we briefly note that the proportion of correct responses\\nfor each task is quite wide, ranging from .19 to .98. tasks\\nin which participants performed particularly well (proportion correct above .80) include identification tasks (e.g. for\\nmouse, keyboard, headphone jack, and websites), turning on\\na computer or monitor, and common operations like recycling a file, using checkboxes, dragging, scrolling, and using\\nhyperlinks. tasks in which participants performed poorly\\n(proportion correct below .60) include identification of various keys, double- or right-clicking, typing web addresses,\\nsigning into email, and composing email.\\nthe proportion correct results from the northstar indicate\\nthe adults with low literacy skills can power on their device\\nand perform a variety of basic operations. to the extent\\nthat these tasks exactly matched tasks that would be performed in a computer-based literacy intervention, like an\\nintelligent tutoring system, this level of results is quite useful. however, for some tasks there is not an exact match,\\nand the implications of the proportion correct results are\\nless clear. for example, difficulties performing tasks using\\nword, excel, or webmail may reflect problems with those\\nspecific interfaces that may not transfer to other programs.\\nunderstanding these more nuanced relationships would require a deeper analysis than is afforded by northstar’s success/failure output.\\n\\n3.\\n\\nanalysis 2: goms modeling\\n\\nthe purpose of this analysis was to explore whether the\\nsuccess rate of the northstar tasks could be modeled using goms (goals, operators, methods, & selection rules),\\na well-known computational technique for modeling expert\\nuser performance on a task [10]. goms decomposes a particular computer task, e.g. saving a file, into goals and subgoals, perceptual, cognitive, and motor actions in service\\nthese goals, methods or sequences of operators that achieve\\na goal, and selection rules that choose between alternative\\nmethods. an important assumption of goms is that the\\nusers are expert at the computer task in question. therefore\\ngoms models of execution time represent the upper bound\\nof performance after a user has learned the interface and\\npracticed it many times. the expert assumption of goms\\nis violated in the adult literacy population, making the outcome of this analysis non-obvious. if the goms model predictions of execution time were related to our adult’s performance, that would provide evidence that goms modeling129\\n\\n\\x0cclick on the monitor\\nclick on the keyboard\\nclick on the system unit\\nclick on the headphone jack\\nclick on picture of a mouse\\nnewline key\\ncaps key\\nshift key\\nbackspace key\\nup arrow\\nturn on monitor\\nturn on computer\\nlog on to computer\\ndouble click on documents\\nright click menu\\n\\ntable 1: northstar tasks\\nrecycle file\\ncheckboxes\\norganize folder options\\nstart menu, lauch program\\nturn up audio slider\\nmute audio\\nselect browser icons\\nclick on the website\\ndrag item in browser\\nclick on address bar\\ntype the web address\\nclick homepage button\\nclick browser back button\\nclick browser refresh\\nclick browser forward\\n\\nclick stop loading\\nselect search engines\\ngoogle query\\ngoogle scroll\\nuse hyperlink\\nmaximize window\\nminimize window\\nopen excel\\nopen word using taskbar\\nclose word\\nselect login and password\\nchoose secure password\\nsign into email\\ncompose email\\n\\nnorthstar items used in analysis 1.\\n\\n3.2 results & discussion\\n\\nfigure 1: a cogtool annotation of a northstar\\ntask. annotations appear as semi-transparent orange boxes over the northstar interface.\\n\\nhas some validity for this population.\\n\\n3.1\\n\\nprocedure\\n\\nthe cogtool system was used to perform a goms analysis\\n[11, 9]. cogtool allows the easy creation of goms models\\nby annotating an existing user interface, and then recording\\na demonstration of the task against than annotated interface. figure 1 shows the cogtool interface for the “click on\\nthe mouse” task. for example, when the northstar task required clicking on an icon, button, or other interface element\\nas in figure 1, a cogtool button annotation was overlaid on\\nthe interface, and then in demonstration mode the modeler\\nwould demonstrate the task by clicking on the annotated\\nbutton. from this demonstration on the annotation, cogtool builds a goms model that includes the perceptual,\\ncognitive, and motor tasks required to perform the task.\\nsimilar annotations were made for auditory directions, keyboard input, and other kinds of interface actions. once a\\ntask was annotated and demonstrated, a cogtool simulation\\nwas run on goms model to generate a predicted execution\\ntime of expert performance. annotations, demonstrations,\\nand execution time predictions were performed for all 44\\n\\ngoms-predicted execution times for northstar tasks ranged\\nfrom 3.0 to 17.1 seconds (m = 6.88, sd = 4.07). these execution times were significantly negatively correlated with\\nproportion correct, r(42) = −.40, p = .01, ci95 [−.61, −.10],\\nindicating that tasks predicted to take an expert longer\\nto accomplish were more likely to be answered incorrectly\\nby low literacy adults. tasks that take longer are inherently more complex and require more operations to complete. these results suggest that goms has some validity for modeling the performance of adults with low literacy\\nskills even though it was not intended for this purpose. however, by themselves these results convey little additional insight. the goms-predicted execution times, generated by\\ncogtool, are still at the task level rather than the component skills required to achieve each task. this is partly\\nbecause the orientation of cogtool is to produce execution\\ntimes and partly because of the expert orientation of goms.\\nfor example, in goms the factors involved in clicking a button are the perceptual (size, location) and motor operations\\ninvolved, but in northstar, some “buttons” are tapping specific types of knowledge, like identifying hardware, understanding icons, or various keys on a keyboard. the different\\ntypes of knowledge behind the various cogtool annotations\\nare not represented or considered in the goms analysis it\\nprovides.\\n\\n4.\\n\\nanalysis 3: q-matrix & logistic\\nmixed models\\n\\nwe would like to understand how the component skills underlying northstar tasks differentially affect the probability\\na low literacy adult will perform the task correctly. in educational data mining, component skills are typically modeled using a q-matrix analysis [4]. in its simplest form,\\na q-matrix analysis constructs a problem by skill matrix\\nsuch that a cellij in the matrix represents whether skilli is\\nneeded to solve problemj : cellij = 1 if skilli is needed to\\nsolve problemj , and cellij = 0 if skilli is not needed to solve\\nproblemj . analysis 2 provides a useful guide towards the\\ncreation of a q-matrix for the northstar tasks, as it has already captured each component action required to perform130\\n\\n\\x0ctable 2: component skills coded from goms\\ncomponent skill\\nprobability correct given skill\\ncheckboxes\\n.89\\nmouse drag\\n.86\\nhardware identify\\n.83\\nhardware function\\n.78\\ncomplex scrolling\\n.74\\nbrowser functions\\n.66\\nleft click\\n.64\\nuse icons\\n.61\\ndouble click\\n.58\\nwindow functionality\\n.56\\nprogram brands\\n.55\\ndesktop concept\\n.53\\nselect menu\\n.50\\ngood login info\\n.50\\nlogin info\\n.48\\nkeyboard function\\n.46\\nsimple typing\\n.43\\nright click\\n.19\\n\\neach task. what it lacks in some cases, however, is an annotation of the knowledge behind each component action.\\n\\n4.1\\n\\nprocedure\\n\\nthe first author recoded the goms task annotations with\\n18 novice-relevant component skills. the coding was done\\nin one pass, and component skills were defined on the fly.\\ncomponent skills that occurred in only one task were then\\nremoved as they offer no predictive utility for other tasks.\\nthe appropriateness of the component skills was evaluated\\nby correlating the total number of component skills needed\\nin each task with the goms execution time and the proportion correct for the respective task. we used a logistic\\nmixed model to predict the correctness of each participant\\non each task as a function of the presence of component\\nskills for that task. this analysis addresses the question as\\nto whether there is an effect (main effect) of the presence of\\ncomponent skills on the likelihood that an adult with low literacy skills will be able to perform the task correctly. using\\na logistic mixed model in this way has strong similarities to\\ncognitive psychometric models like diagnostic classification\\nmodels [16] or more specifically a mixed model implementation of linear logistic test models [15].\\nin the logistic mixed model, random slopes were initially included but failed to converge. random intercepts for task\\nand participant are theoretically motivated, and backward\\nselection of these effects using akaike information criterion\\n(aic) achieved a minimum when these effects were included,\\nindicating that these intercepts should remain in the model.\\nthese random intercepts can be considered as per-task difficulty not captured by component skills and per-subject\\nability, respectively. the initial model that included left\\nclick was rank deficient, so left click, which appears in\\nmost tasks, was removed from the final model. additionally, the total number of component skills in each task (i.e.\\ncolumn sums of the q-matrix) was initially considered as\\na predictor of correctness, but was excluded based on extremely high collinearity, having a variance inflation factor\\nof over 40.\\n\\n4.2 results & discussion\\nthe component skills and the conditional probability that\\na task will be correctly performed if the component skill is\\npresent are shown in figure 2. total component skills per\\ntask was marginally positive correlated with goms execution time, r(42) = .27, p = .07, ci95 [−.02, .53], suggesting\\nthat tasks with more component skills take longer to perform. total component skills per task was significantly negatively correlated with proportion correct, r(42) = −.35, p =\\n.02, ci95 [−.59, −.06], indicating that tasks with more component skills are more difficult to perform correctly. the\\ncorrelation between predicted execution time and proportion correct was not significantly different from the correlation between total component skills and proportion correct,\\nt(82) = .18, p = .86, indicating that the q-matrix decomposition of component skills is comparable to the goms execution time in terms of its relationship to proportion correctness. altogether these correlation results provide additional\\nevidence that the q-matrix decomposition is appropriate.\\nthe logistic mixed model had a marginal r2 of .18 (fixed\\neffects only) and a conditional r2 of .47 (including random effects) [12]. we found a positive main effect of mouse\\ndrag, β̂ = 2.06, se = .90, p = .02, such that tasks with\\na mouse drag component were 7.87 times as likely to be\\nanswered correctly, ci95 [1.36, 45.50], and a marginal main\\neffect of hardware identify, β̂ = .89, se = .53, p = .10,\\nsuch that tasks with a hardware identify component were\\n2.44 times as likely to be answered correctly, ci95 [.86, 6.94].\\nwe found negative main effects for keyboard function, β̂ =\\n−1.31, se = .51, p = .01, use icon β̂ = −1.35, se =\\n.55, p = .01, simple typing β̂ = −1.91, se = .64, p = .003,\\nand right click β̂ = −3.20, se = 1.34, p < .02, such that\\ntasks with a keyboard function component were .27 times\\nas likely to be answered correctly, ci95 [.10, .73], tasks with\\na use icon component were .26 times as likely to be answered correctly, ci95 [.09, .75], tasks with a simple typing\\ncomponent were .15 times as likely to be answered correctly,\\nci95 [.04, .52], and tasks with a right click component were\\n.04 times as likely to be answered correctly, ci95 [.00, .56].\\nwe found that mouse drag was extremely predictive of success. the reason is unclear, but we hypothesize that the\\nfrequency of mouse dragging in many computer tasks may\\nhave afforded participants the opportunity to become expert\\nin this skill. mouse dragging has some similarity to swiping\\non a smartphone or tablet interface, so it may be that expertise with other devices has transferred into the northstar\\ntasks. amongst the components that predict failure, perhaps the most intuitive are keyboard function and simple\\ntyping. typing is a complex skill that takes practice to master. function keys are difficult in that they don’t themselves\\nproduce a character, but either operate on a character on the\\nscreen (delete) or work in combination with another key to\\nmodify it (shift). the negative effects associated with use\\nicon and right click are somewhat surprising. icons come\\nin many different variations, and so it is possible that the\\nnegative use icon effect is attributable to a lack of knowledge of specific icons or perhaps to the conventions of icons\\ngenerally. right click is possibly rare and usually brings up\\na context menu with commands that are often available elsewhere, making it more relevant for power users but perhaps\\nless so to novice users.131\\n\\n\\x0cfigure 2: the coefficient path for the lasso model. as the l1 sparsity threshold increases along the x-axis,\\nmore coefficients are non-zero.\\n\\n5.1\\n\\nprocedure\\n\\na logistic regression base model without random effects was\\ninitialized with 17 component skills (left click excluded)\\nand submitted to lasso. because lasso has a free parameter,\\nλ, that controls sparsity of the regression, a lasso analysis\\nvaries the level of λ and generates regression coefficient estimates at each level. this sequence of regression coefficients\\nis known as the regularization path. the value of λ that\\nminimized prediction error was estimated using both cross\\nvalidation and aic.\\n\\n5.2\\n\\nresults & discussion\\n\\nthe coefficient (regularization) path for the lasso model is\\nshown in figure 2 and the corresponding aic curve is shown\\n\\n●\\n●\\n\\n6400\\n\\n6500\\n\\nanalysis 4: q-matrix lasso\\n\\n●\\n\\n●\\n\\n6200\\n\\naic\\n\\n6300\\n\\n●\\n\\n●●\\n●\\n\\n●\\n\\n6100\\n\\n●\\n\\n●\\n\\n●\\n●\\n\\n6000\\n\\nanalysis 3 provides a more traditional analysis of significant predictors in our study, but must be interpreted with\\ncaution with respect to generalizing to new data. it may\\nbe that insignificant predictors in analysis 3 nevertheless\\nhave predictive value on new data. the problems of relying on p-values or criteria like aic to select variables are\\nwell known [8]. to explore the predictive potential of the\\nq-matrix component skills, we created a lasso model (least\\nabsolute shrinkage and selection operator [18]), a form of\\nregression that promotes sparsity (i.e. zero coefficients) and\\npredictive accuracy simultaneously. while not necessarily\\nthe best predictive model (cf. gradient boosting [6]), lasso\\nhas the advantage of being simple to interpret, and thus our\\nresults can guide what variables to use in future models.\\n\\n●\\n\\n●\\n●\\n●\\n\\n5900\\n\\n5.\\n\\n●\\n\\n●\\n\\n●\\n\\n0\\n\\n2\\n\\n4\\n\\n6\\n\\n8\\n\\n10\\n\\n12\\n\\n●\\n\\n●\\n\\n14\\n\\n|beta|\\n\\nfigure 3: the aic curve for the lasso model. lower\\nvalues of aic indicate better model fit.\\n\\nin figure 3. in figure 2, the center line represents coefficients having zero values. as the l1 sparsity threshold\\n(|beta|) increases, more coefficients become non-zero. for selecting the optimal λ that minimizes overall prediction error,\\nten-fold cross validation and aic yielded congruent results.\\naic results are depicted in the curve in figure 3, which\\nshows that that aic improves as |beta| increases, coming to\\na minimum at |beta| = 13.40. accordingly, most coefficients\\nfor the optimal lasso model are non-zero.132\\n\\n\\x0ctable 3: lasso component skill coefficients\\ncomponent skill\\nβ̂ exp(β̂)\\nmouse drag\\n1.80\\n6.02\\ncheckboxes\\n1.27\\n3.55\\nlogin information\\n.88\\n2.41\\nhardware identify\\n.60\\n1.82\\nhardware function\\n.50\\n1.65\\ndesktop concept\\n.35\\n1.43\\nbrowser functions\\n.24\\n1.27\\ndouble click\\n.11\\n1.12\\ncomplex scrolling\\n.03\\n1.03\\nprogram brands\\n.00\\n1.00\\nselect menu\\n-.21\\n.81\\nwindow functionality\\n-.44\\n.64\\nkeyboard function\\n-.86\\n.42\\nuse icons\\n-1.01\\n.36\\ngood login info\\n-1.28\\n.28\\nsimple typing\\n-1.47\\n.23\\nright click\\n-2.39\\n.09\\n\\ntable 3 gives the β̂ coefficients (log odds) for the aicoptimal model as well as the odds ratio exp(β̂) for each coefficient. the coefficients converted to odds ratios have the\\nsame interpretation as in the logistic mixed model, e.g. tasks\\nwith a mouse drag component are 6.02 times as likely to be\\nanswered correctly as those without. although the logistic\\nlasso model does not include random intercepts corresponding to task difficulty and subject ability, the magnitudes of\\ncoefficients in the logistic lasso are highly comparable to the\\nlogistic mixed model. however, the strength of the coefficients in the logistic lasso are weaker, in general, than in\\nthe logistic mixed model, suggesting that the logistic mixed\\nmodel may be slightly over-fitted. for example, according to\\nthe logistic mixed model, mouse drag tasks are 7.87 times\\nas likely to be answered correctly, but according to the logistic lasso model, mouse drag tasks are only 6.02 times as\\nlikely to be answered correctly; similarly right click containing tasks in the mixed model are .04 times as likely to\\nbe answered correctly compared to .09 times as likely in the\\nlogistic lasso. these results suggest that while the logistic\\nmixed model might be more appropriate for assessment purposes, as it additionally estimates task difficulty and subject\\nability, the logistic lasso model might be more appropriate\\nfor predicting the effects of component skills on success rates\\nfor new tasks.\\n\\n6. general discussion\\ntogether, our results suggest that not only are there specific northstar tasks that are informative with regard to\\nbuilding an adaptive computer-based intervention for adults\\nwith low literacy skills but also that these tasks can themselves be decomposed into component skills that can be\\nfurther used for this purpose. the main effects of analysis 3 and coefficient rankings of analysis 4 are consistent\\nand complimentary with the proportion correct results in\\nanalysis 1. the marginal main effect for hardware identify explains the high proportion correctness for identification tasks for mouse, keyboard, and headphone jack, and the\\nmain effect for mouse drag explains the high proportion correctness for recycling a file (dragging to the recycle bin),\\ndragging, and scrolling (by dragging a scroll bar). these\\n\\ncorrectness-enhancing main effects are also reflected in odds\\nratios greater than one in analysis 4. similarly the main effects for keyboard function and simple typing explain the\\nlow proportion correctness for identifying various keys, typing web addresses, signing into email, and composing email,\\nand these main effects are likewise reflected in odds ratios\\nless than one in analysis 4. in these cases we infer that\\nthe problem is not specific to the interface in question, e.g.\\nemail, but rather that there is a deficiency in a component\\nskill needed for the task taking place in the context of that\\ninterface.\\nthe implications for building adaptive computer-based interventions for adults with low literacy skills are clear. first,\\nit is important to keep typing to a minimum, either by having users select response options or by using speech recognition. second, right clicking should be eliminated or at\\nleast made optional. third, icons should be close to icon\\narchetypes. and finally, mouse dragging is a good skill\\naround which to build user interaction. interestingly, all\\nof these implications seem to point to tablet and smartphone platforms, which have a minimum of typing (and\\nbuilt in speech interfaces), no right clicking, minimal icons\\nin-app, and plenty of swiping/dragging. moreover, smartphone ownership has been rapidly increasing – now 64% of\\nhouseholds earning below $30 thousand own a smartphone\\n[17]. it may be the case that deploying interventions on\\nsmartphones and tablets better makes use of both the computer literacy strengths and the material resources of low\\nliteracy adults.\\n\\n7.\\n\\nacknowledgments\\n\\nthis research was supported by the institute of education\\nsciences (ies; r305c120001). any opinions, findings and\\nconclusions, or recommendations expressed in this paper are\\nthose of the author and do not represent the views of the\\nies.\\n\\n8.\\n\\nreferences\\n\\n[1] m. anderson and a. perrin. 13% of americans don’t\\nuse the internet. who are they? technical report, pew\\nresearch center, 2016.\\n[2] d. bakhtiari, a. olney, and d. greeberg. computer\\nliteracy skills of adult learners. in preparation.\\n[3] j. a. ballantine, p. m. larres, and p. oyelere.\\ncomputer usage and the validity of self-assessed\\ncomputer competence among first-year business\\nstudents. computers & education, 49(4):976 – 990,\\n2007.\\n[4] t. barnes, d. bitzer, and m. vouk. experimental\\nanalysis of the q-matrix method in knowledge\\ndiscovery. in international symposium on\\nmethodologies for intelligent systems, pages 603–611.\\nspringer, 2005.\\n[5] h. beder and p. medina. classroom dynamics in adult\\nliteracy education. ncsall research brief. technical\\nreport, national center for the study of adult\\nlearning and literacy, 2002.\\n[6] j. h. friedman. stochastic gradient boosting.\\ncomputational statistics & data analysis,\\n38(4):367–378, 2002.\\n[7] a. c. graesser, z. cai, w. o. baer, a. m. olney,133\\n\\n\\x0c[8]\\n\\n[9]\\n\\n[10]\\n\\n[11]\\n\\n[12]\\n\\n[13]\\n[14]\\n[15]\\n\\n[16]\\n\\n[17]\\n\\n[18]\\n\\nx. hu, m. reed, and d. greenberg. reading\\ncomprehension lessons in autotutor for the center for\\nthe study of adult literacy. in s. a. crossley and\\nd. s. mcnamara, editors, adaptive educational\\ntechnologies for literacy instruction., pages 288–293.\\nroutledge, 2016. doi: 10.4324/9781315647500 doi:\\n10.4324/9781315647500.\\nf. harrell. regression modeling strategies: with\\napplications to linear models, logistic regression,\\nand survival analysis. graduate texts in\\nmathematics. springer, 2001.\\nb. e. john. cogtool: predictive human performance\\nmodeling by demonstration. in proceedings of the 19th\\nconference on behaviour representation in modeling\\nand simulation, pages 83–84, 2010.\\nb. e. john and d. e. kieras. the goms family of user\\ninterface analysis techniques: comparison and\\ncontrast. acm transactions on computer-human\\ninteraction (tochi), 3(4):320–351, 1996.\\nb. e. john, k. prevas, d. d. salvucci, and\\nk. koedinger. predictive human performance\\nmodeling made easy. in proceedings of the sigchi\\nconference on human factors in computing systems,\\nchi ’04, pages 455–462, new york, ny, usa, 2004.\\nacm.\\ns. nakagawa and h. schielzeth. a general and simple\\nmethod for obtaining r2 from generalized linear\\nmixed-effects models. methods in ecology and\\nevolution, 4(2):133–142, 2013.\\nn. d. l. project, 2016.\\nproliteracy. u.s. adult literacy facts. technical\\nreport, 2017.\\nf. rijmen, p. d. boeck, and k. u. leuven. the\\nrandom weights linear logistic test model. applied\\npsychological measurement, 26(3):271–285, 2002.\\na. a. rupp and j. l. templin. unique characteristics\\nof diagnostic classification models: a comprehensive\\nreview of the current state-of-the-art. measurement:\\ninterdisciplinary research and perspectives,\\n6(4):219–262, 2008.\\na. smith. record shares of americans now own\\nsmartphones, have home broadband. technical report,\\npew research center, 2017.\\nr. tibshirani. regression shrinkage and selection via\\nthe lasso. journal of the royal statistical society.\\nseries b (methodological), 58(1):267–288, 1996.',\n",
       " \"\\x0czone out no more: mitigating mind wandering during\\ncomputerized reading\\nsidney k. d’mello, caitlin mills, robert bixler, & nigel bosch\\nuniversity of notre dame\\n118 haggar hall\\nnotre dame, in 46556, usa\\nsdmello@nd.edu\\n\\nabstract\\nmind wandering, defined as shifts in attention from task-related\\nprocessing to task-unrelated thoughts, is a ubiquitous\\nphenomenon that has a negative influence on performance and\\nproductivity in many contexts, including learning. we propose\\nthat next-generation learning technologies should have some\\nmechanism to detect and respond to mind wandering in real-time.\\ntowards this end, we developed a technology that automatically\\ndetects mind wandering from eye-gaze during learning from\\ninstructional texts. when mind wandering is detected, the\\ntechnology intervenes by posing just-in-time questions and\\nencouraging re-reading as needed. after multiple rounds of\\niterative refinement, we summatively compared the technology to\\na yoked-control in an experiment with 104 participants. the key\\ndependent variable was performance on a post-reading\\ncomprehension assessment. our results suggest that the\\ntechnology was successful in correcting comprehension deficits\\nattributed to mind wandering (d = .47 sigma) under specific\\nconditions, thereby highlighting the potential to improve learning\\nby “attending to attention.”\\n\\nkeywords\\nmind wandering; gaze tracking; student modeling; attentionaware.\\n\\n1. introduction\\ndespite our best efforts to write a clear and engaging paper,\\nchances are high that within the next 10 pages you might fall prey\\nto what is referred to as zoning out, daydreaming, or mind\\nwandering [45]. despite your best intention to concentrate on our\\npaper, at some point your attention might drift away to unrelated\\nthoughts of lunch, childcare, or an upcoming trip. this prediction\\nis not based on some negative or cynical opinion of the\\nreader/reviewer (we read and review papers too), but on what is\\nknown about attentional control, vigilance, and concentration\\nwhile individuals are engaged in complex comprehension\\nactivities, such as reading for understanding.\\none recent study tracked mind wandering of 5,000 individuals\\nfrom 83 countries with a smartphone app that prompted people\\nwith thought-probes at random intervals throughout the day [24].\\npeople reported mind wandering for 46.9% of the prompts, which\\nconfirmed lab studies on the pervasiveness of mind wandering\\n(see [45] for a review). mind wandering is more than merely\\nincidental; a recent meta-analysis of 88 samples indicated a\\nnegative correlation between mind wandering and performance\\nacross a variety of tasks [34], a correlation which increases with\\ntask complexity. when compounded with its high frequency,\\nmind wandering can have serious consequences on the\\nperformance and productivity of society at large.\\n\\nof learning with technology. traditional learning technologies\\nrely on the assumption that students are attending to the learning\\nsession, although this is not always the case. for example, it has\\nbeen estimated that students mind wander approximately 40% of\\nthe time when engaging with online lectures [38], which are an\\nimportant component of moocs. some advanced technologies\\ndo aim to detect and respond to affective states like boredom, but\\nevidence for their effectiveness is still equivocal (see [9] for a\\nreview). further, boredom is related to but not the same as\\nattention [12]. there are technologies that aim to prevent mind\\nwandering by engendering a highly immersive learning\\nexperience and have achieved some success in this regard [40,\\n41]. but what is to be done when attentional focus inevitably\\nwanes as the session progresses and the novelty of the system and\\ncontent fades?\\nour central thesis is that next-generation learning technologies\\nshould include mechanisms to model and respond to learners’\\nattention in real-time [8]. such attention-aware technologies can\\nmodel various aspects of learner attention (e.g., divided attention,\\nalternating attention). here, we focus on detecting and mitigating\\nmind wandering, a quintessential signal of waning engagement.\\nwe situate our work in the context of reading because reading is\\na common activity shared across multiple learning technologies,\\nthereby increasing the generalizability of our results. further,\\nstudents mind wander approximately 30% of the time during\\ncomputerized reading [44]. and although mind wandering can\\nfacilitate certain cognitive processes like future planning and\\ndivergent thinking [2, 28], it negatively correlates with\\ncomprehension and learning (reviewed in [31, 45]), suggesting\\nthat it is important to address mind wandering during learning.\\ntowards this end, we developed and validated a closed-loop\\nattention-aware learning technology that combines a machinelearned mind wandering detector with a real-time interpolated\\ntesting and re-study intervention. our attention-aware technology\\nworks as follows. learners read a text on a computer screen using\\na self-paced screen-by-screen (also called page-by-page) reading\\nparadigm. we track eye-gaze during reading using a remote eye\\ntracker that does not restrict head movements. we focus on eyegaze for mind wandering detection due to decades of research\\nsuggesting a tight coupling between attentional focus and eye\\nmovements during reading [36]. when mind wandering is\\ndetected, the system intervenes in an attempt to redirect\\nattentional focus and correct any comprehension deficits that\\nmight arise due to mind wandering. the interventions consist of\\nasking comprehension question on pages where mind wandering\\nwas detected and providing opportunities to re-read based on\\nlearners’ responses. in this paper, we discuss the mind wandering\\n\\nmind wandering is also unfortunately an under-addressed\\nproblem in education and is yet to be deeply studied in the context8\\n\\n\\x0cdetector, intervention approach, and results of a summative\\nevaluation study 1.\\n\\n1.1 related work\\nthe idea of attention-aware user interfaces is not new, but was\\nproposed almost a decade ago by roda and thomas [39]. there\\nwas even an article on futuristic applications of attention-aware\\nsystems in educational contexts [35]. prior to this, gluck, et al.\\n[15] discussed the use of eye tracking to increase the bandwidth\\nof information available to an intelligent tutoring system (its).\\nsimilarly, anderson [1] followed up on some of these ideas by\\ndemonstrating how particular beneficial instructional strategies\\ncould only be launched via a real-time analysis of eye gaze.\\nmost of the recent work has been on leveraging eye gaze to\\nincrease the bandwidth of learner models [22, 23, 29]. conati, et\\nal. [5] provide an excellent review of much of the existing work\\nin this area. we can group the research into three categories: (1)\\noffline-analyses of eye gaze to study attentional processes, (2)\\ncomputational modeling of attentional states, and (3) closed-loop\\nsystems that respond to attention in real-time. offline-analysis of\\neye movements has received considerable attention in cognitive\\nand educational psychology for several decades [e.g., 16, 19], so\\nthis area of research is relatively healthy. online computational\\nmodels of learner attention are just beginning to emerge [e.g., 6,\\n11], while closed-loop attention-aware systems are few and far\\nbetween (see [7, 15, 42, 48] for a more or less exhaustive list).\\ntwo known examples, gazetutor and attentivereview, are\\ndiscussed below.\\ngazetutor [7] is a learning technology for biology. it has an\\nanimated conversational agent that provides spoken explanations\\non biology topics which are synchronized with images. the\\nsystem uses a tobii t60 eye tracker to detect inattention, which\\nis assumed to occur when learners’ gaze is not on the tutor agent\\nor image for at least five consecutive seconds. when this occurs,\\nthe system interrupts its speech mid utterance, directs learners to\\nreorient their attention (e.g., “i’m over here you know”), and\\nrepeats speaking from the start of the current utterance. in an\\nevaluation study, 48 learners (undergraduate students) completed\\na learning session on four biology topics with the attention-aware\\ncomponents enabled (experimental group) or disabled (control\\ngroup). the results indicated that gazetutor was successful in\\ndynamically reorienting learners’ attentional patterns towards the\\ninterface. importantly, learning gains for deep reasoning\\nquestions were significantly higher for the experimental vs.\\ncontrol group, but only for high aptitude learners. the results\\nsuggest that even the most basic attention-aware technology can\\nbe effective in improving learning, at least for a subset of learners.\\nhowever, a key limitation is that the researchers simply assumed\\nthat off-screen gaze corresponded to inattention, but did not test\\nthis assumption (e.g., students could have been concentrating\\nwith their eyes closed and this would have been perceived as\\nbeing inattentive).\\nattentivereview [32] is a closed-loop system for mooc learning\\non mobile phones. the system uses video-based\\nphotoplethysmography (ppg) to detect a learners’ heart rate from\\nthe back camera of a smartphone while they view mooc-like\\nlectures on the phone. attentivereview ranks the lectures based\\n1\\n\\non its estimates of learners’ “perceived difficulty,” selecting the\\nmost difficult lecture for subsequent review (called adaptive\\nreview). in a 32-participant between-subjects evaluation study,\\nthe authors found that learning gains obtained from the adaptive\\nreview condition were statistically on par with a full review\\ncondition, but were achieved in 66.7% less review time. although\\nthis result suggests that attentivereview increased learning\\nefficiency, there is the question as to whether the system should\\neven be considered to be an “attention-aware” technology. this is\\nbecause it is arguable if the system has anything to do with\\nattention (except for “attention” appearing in its name) as it\\nselects items for review based on a model of “perceived\\ndifficulty” and not on learners’ “attentional state.” the two might\\nbe related, but are clearly not the same.\\n\\n1.2 novelty\\nour paper focuses on closing the loop between research on\\neducational data and learning outcomes by developing and\\nvalidating the first (in our view) real-time learning technology\\nthat detects and mitigates mind wandering during computerized\\nreading. although automated detection of complex mental states\\nwith the goal of developing intelligent learning technologies that\\nrespond to the sensed states is an active research area (see reviews\\nby [9, 18]), mind wandering has rarely been explored as an aspect\\nof a learner’s mental state that warrants detection and corrective\\naction. and while there has been some work on modeling the\\nlocus of learner attention (see review by [5]), mind wandering is\\ninherently different than more commonly studied forms of\\nattention (e.g., selective attention, distraction), because it involves\\nmore covert forms of involuntary attentional lapses spawned by\\nself-generated internal thought [45]. simply put, mind wandering\\nis a form of “looking without seeing” because the eyes might be\\nfixated on the appropriate external stimulus, but very little is\\nbeing processed as the mind is consumed by stimulusindependent internal thoughts. offline automated approaches to\\ndetect mind wandering have been developed (e.g., [3, 11, 27, 33]),\\nbut these detectors have not yet been used to trigger online\\ninterventions. here, we adapt an offline gaze-based automated\\nmind wandering detector [13] to trigger real-time interventions to\\naddress mind wandering during reading. we conduct a\\nrandomized control trial to evaluate the efficacy of our attentionaware learning technology in improving learning.\\n\\n2. mind wandering detection\\nwe adopted a supervised learning approach for mind wandering\\ndetection. below we provide a high-level overview of the\\napproach; readers are directed to [3, 13] for a detailed discussion\\nof the general approach used to build gaze-based detectors of\\nmind wandering.\\n\\n2.1 training data\\nwe obtained training data from a previous study [26] that\\ninvolved 98 undergraduate students reading a 57-page text on the\\nsurface tension of liquids [4] on a computer screen for an average\\nof 28 minutes. the text contained around 6500 words, with an\\naverage of 115 words per page, and was displayed on a computer\\nscreen with courier new typeface. we recorded eye-gaze with a\\ntobii tx300 eye tracker set to a sampling frequency of 120 hz.\\n\\nthis paper reports updated results of an earlier version [10] presented\\nas a “late-breaking work” (lbw) poster at the 2016 acm chi\\nconference. lbw “extended abstracts” are not included in the main\\nconference proceedings and copyright is retained by the authors.9\\n\\n\\x0cparticipants could read normally and were free to move or gesture\\nas they pleased.\\nparticipants were instructed to report mind wandering (during\\nreading) by pressing a predetermined key when they found\\nthemselves “thinking about the task itself but not the actual\\ncontent of the text” or when they were “thinking about anything\\nelse besides the task.” this is consistent with contemporary\\napproaches (see [45]) that rely on self-reporting because mind\\nwandering is an internal conscious phenomena. further, selfreports of mind wandering have been linked to predictable\\npatterns in physiology [43], pupillometry [14], eye-gaze [37], and\\ntask performance [34], providing validity for this approach.\\n\\non the page, we classified the page as a positive instance of mind\\nwandering. this was done because analyses indicated that\\nparticipants were more likely to be mind wandering in those cases\\n(but see [13] for alternate strategies to handle missing instances).\\n\\non average, we received mind wandering reports for 32% of the\\npages (sd = 20%), although there was considerable variability\\namong participants (ranging from 0% to 82%). self-reported\\nmind wandering negatively correlated (r = -.23, p < .05) with\\nscores on a subsequent comprehension assessment [26], which\\nprovides evidence for the predictive validity of the self-reports.\\n\\n2.2 model building\\nthe stream of eye-gaze data was filtered to produce a series of\\nfixations, saccades, and blinks, from which global eye gaze\\nfeatures were extracted (see figure 1). global features are\\nindependent of the words being read and are therefore more\\ngeneralizable than so-called local features. a full list of 62 global\\nfeatures along with detailed descriptions is provided in [13], but\\nbriefly the features can be grouped into the following four\\ncategories: (1) eye movement descriptive features (n = 48) were\\nstatistical functionals (e.g., min, median) for fixation duration,\\nsaccade duration, saccade amplitude, saccade velocity, and\\nrelative and absolute saccade angle distributions; (2) pupil\\ndiameter descriptive features were statistical functionals (n = 8)\\ncomputed from participant-level z-score standardized estimates\\nof pupil diameter; (3) blink features (n = 2) consisted of the\\nnumber of blinks and the mean blink duration; (4) miscellaneous\\ngaze features (n = 4) consisted of the number of saccades,\\nhorizontal saccade proportion, fixation dispersion, and the\\nfixation duration/saccade duration ratio. we proceeded with a\\nsubset of 32 features after eliminating features exhibiting\\nmulticollinearity.\\nfeatures were calculated from only a certain amount of gaze data\\nfrom each page, called the window. the end of the window was\\npositioned 3 seconds before a self-report so as to not overlap with\\nthe key-press. the average amount of time between self-reports\\nand the beginning of the page was 16 seconds. we used this time\\npoint as the end of the window for pages with no self-report.\\npages that were shorter than the target window size were\\ndiscarded, as were pages with windows that contained fewer than\\nfive gaze fixations as there was insufficient data to compute some\\nof the features. there were a total of 4,225 windows with\\nsufficient data for supervised classification.\\nwe experimented with a number of supervised classifiers on\\nwindow sizes of 4, 8, and 12 seconds to discriminate positive\\n(pages with a self-report = 32%) from negative (pages without a\\nself-report) instances of mind wandering. the training data were\\ndownsampled to achieve a 50% base rate; testing data were\\nunaltered. a leave-one-participant-out validation approach was\\nadopted where models were built on data from n-1 participants\\nand evaluated on the held-out participant. the process was\\nrepeated for all participants. model validation was conducted in a\\nway to simulate a real-time system by analyzing data from every\\npage. when classification was not possible due to a lack of valid\\ngaze data and/or because participants did not spend enough time\\n\\nfigure 1: gaze fixations during mind wandering (top)\\nand normal reading (bottom)\\n\\n2.3 detector accuracy\\nthe best model was a support vector machine that used global\\nfeatures and operated on a window size of 8-seconds. the area\\nunder the roc curve (auc or auroc or a’) was .66, which\\nexceeds the 0.5 chance threshold [17].\\nwe assigned each instance as mind wandering or not mind\\nwandering based on whether the detector’s predicted likelihood\\nof mind wandering (ranges from 0 to 1) was below or above 0.5\\nwe adopted the default 0.5 threshold as it led to a higher rate of\\ntrue positives while maintaining a moderate rate of true negatives.\\nthis resulted in the following confusion matrix shown in table 1.\\nthe model had a weighted precision of 72.2% and a weighted\\nrecall of 67.4%, which we deemed to be sufficiently accurate for\\nintervention.\\ntable 1: proportionalized confusion matrix for mind\\nwandering detection\\npredicted mind wandering (mw)\\nactual mw\\n\\nyes\\n\\nno\\n\\nyes\\n\\n0.715 (hit)\\n\\n0.285 (miss)\\n\\nno\\n\\n0.346 (false positive)\\n\\n0.654 (correct rejection)\\n\\n3. intervention to address mind wandering\\nour intervention approach is grounded in the basic idea that\\nlearning of conceptual information involves creating and\\nmaintaining an internal model (mental model) by integrating\\ninformation from the text with prior knowledge from memory\\n[25]. this integration process relies on attentional focus and\\nbreaks down during mind wandering because information from\\nthe external environment is no longer being integrated into the\\ninternal mental model. this results in an impaired model which\\nleads to less effective suppression of off-task thoughts. this\\nincrease in mind wandering further impairs the mental model,10\\n\\n\\x0cresulting in a vicious cycle. our intervention targets this vicious\\ncycle by redirecting attention to the primary task and attempting\\nto correct for comprehension deficits attributed to mind\\nwandering. based on research demonstrating the effectiveness of\\ninterpolated testing [47], we propose that asking questions on\\npages where mind wandering is detected and encouraging rereading in response to incorrect responses will aid in re-directing\\nattention to the text and correct knowledge deficits.\\n\\npage regardless of whether the second question was answered\\ncorrectly, so as not to be overly burdensome.\\n\\n3.1 intervention implementation\\nour initial intervention was implemented for the same text used\\nto create the mind wandering detector (although it could be\\napplied to any text). the text was integrated into the computer\\nreading interface. mind wandering detection occurred when the\\nlearner navigated to the next page using the right arrow key. in\\norder to address ambiguity in mind wandering detection, we used\\nthe detector’s mind wandering likelihood to probabilistically\\ndetermine when to intervene. for example, if the mind wandering\\nlikelihood was 70%, then there was a 70% chance of intervention\\non any given page (all else being equal). we did not intervene for\\nthe first three pages in order to allow the learner to become\\nfamiliar with the text and interface. to reduce disruption, there\\nwas a 50% reduced probability of intervening on adjacent pages,\\nand the maximum number of interventions was capped at 1/3 ×\\nthe number of pages (19 for the present 57-page text). table 2\\npresents pseudo code for when to launch an intervention.\\ntable 2: pseudo code for intervention strategy\\nlaunch_intervention:\\nif current_page >= waitpages\\nand\\ntotal_interventions < maxintrv)\\nand\\ngaze_likelihood > random(0,1)\\nand\\n(!has_intervened(previous_page)\\nor 0.5 < random (0,1)):\\ndo_intervention()\\nelse:\\nshow_next_page()\\ndo_intervention:\\nanswer1 = show_question1()\\nif answer1 is correct:\\nshow_positive_feedback()\\nshow_next_page()\\nelse:\\nshow_neg_feedback()\\nsuggest_rereading()\\nif page advance detected:\\nanswer2 = show_question2();\\nshow_next_page()\\n\\nfigure 2 presents an outline of the intervention strategy. the\\nintervention itself relied on two multiple choice questions for\\neach page (screen) of the text. when the system decided to\\nintervene, one of the questions (randomly selected) was presented\\nto the learner. if the learner answered this online question\\ncorrectly, positive feedback was provided, and the learner could\\nadvance to the next page. if the learner answered incorrectly,\\nnegative feedback was provided, and the system encouraged the\\nlearner to re-read the page. the learner was then provided with a\\nsecond (randomly selected) online question, which could either\\nbe the same or the alternate question for that page. feedback was\\nnot provided and the learner was allowed to advance to the next\\n\\nfigure 2: outline of intervention strategy\\n\\n3.2 iterative refinement\\nthe technology was refined through multiple rounds of formative\\ntesting with 67 participants, recruited from the same institution\\nused to build the detector. participants were observed while\\ninteracting with the technology, their responses were analyzed,\\nand they were interviewed about their experience. we used the\\nfeedback gleaned from these tests to refine the intervention\\nparameters (i.e., when to launch, how many interventions to\\nlaunch, whether to launch interventions on subsequent pages),\\nintervention questions themselves, and instructions on how to\\nattend to the intervention. for example, earlier versions of the\\nintervention used a fixed threshold (instead of the aforementioned\\nprobabilistic approach) to trigger an intervention. despite many\\nattempts to set this threshold, the end result was that some\\nparticipants received many interventions while others received\\nalmost no interventions. this issue was corrected by\\nprobabilistically rather than deterministically launching the\\nintervention. additional testing/refinement of the comprehension\\nquestions used in the intervention was done using crowdsourcing\\nplatforms, specifically amazon’s mechanical turk (mturk).\\n\\n4. evaluation study\\nwe conducted a randomized controlled trial to evaluate the\\ntechnology. the experiment had two conditions: an intervention\\ncondition and a yoked control condition (as described below). the\\nyoked control was needed to verify that any learning benefits are\\nattributed to the technology being sensitive to mind wandering\\nand not merely to the added opportunities to answer online\\nquestions and re-read. this is because we know that interpolated\\ntesting itself has beneficial comprehension effects [47].\\n\\n4.1 method\\nparticipants (n = 104) were a new set of undergraduate students\\nwho participated to fulfill research credit requirements. they\\nwere recruited from the same university used to build the mw\\ndetector and for the iterative testing and refinement cycles.\\nwe did not use a pretest because we expected participants to be\\nunfamiliar with the topic. participants were not informed that the\\ninterface would be tracking their mind wandering (until the11\\n\\n\\x0cdebriefing at the end), instead, they were instructed as follows:\\n“while reading the text, you will occasionally be asked some\\nquestions about the page you just read. depending on your\\nanswer, you will re-read the same page and you will be asked\\nanother question that may or may not be the same question.”\\nparticipants in the intervention condition received the\\nintervention as described above (i.e., based on detected mind\\nwandering likelihoods). each participant in the yoked control\\ncondition was paired with a participant in the intervention\\ncondition. he or she received an intervention question on the\\nsame pages as their paired intervention participant regardless of\\nmind wandering likelihood. for example, if participant a (i.e.,\\nintervention condition) received questions on pages 5, 7, 10, and\\n25, participant b (i.e., yoked control condition) would receive\\nintervention questions on the same pages. however, if the yoked\\nparticipant answered incorrectly, then (s)he had the opportunity\\nto re-read and answer another question regardless of the outcome\\nof their intervention-condition partner.\\nafter reading, participants completed a 38-item multiple choice\\ncomprehension assessment to measure learning. the questions\\nwere randomly selected from the 57 pages (one per page) with the\\nexception that a higher selection priority was given to pages that\\nwere re-read on account of the intervention. participants in the\\nyoked control condition received the same posttest questions as\\ntheir intervention condition counterparts.\\n\\n4.2 results\\nparticipants received an average of 16 (min of 7 and max of 19)\\ninterventions. they spent an average of 27.5 seconds on each\\nscreen prior to receiving an intervention. there was no significant\\ndifference across conditions (p = .998), suggesting that reading\\ntime was not a confound. in what follows, we compared each\\nintervention participant to his/her yoked control with a two-tailed\\npaired-samples t-test and a 0.05 criteria for statistical\\nsignificance.\\nmind wandering detection. the detector’s likelihood of mind\\nwandering was slightly higher for participants in the yokedcontrol condition (m = .431; sd = .170) compared to the\\nintervention condition (m = .404; sd = .112), but the difference\\nwas not statistically significant (p = .348). this was unsurprising\\nas participants in both groups received the same interventions,\\nwhich itself was expected to reduce mind wandering. importantly,\\nmind wandering likelihoods were negatively correlated with\\nperformance on the online questions (r = -.296, p = .033) as well\\nas on posttest questions (r = -.319, p = .021). this provides\\nevidence for the validity of the mind wandering detector when\\napplied to a new set of learners and under different conditions\\n(i.e., reading interspersed with online questions compared to\\nuninterrupted reading).\\ncomprehension assessment. there was some overlap between\\nthe online questions and the posttest questions. to obtain an\\nunbiased estimate of learning, we only analyzed performance on\\npreviously unseen posttest questions. that is, questions that were\\nused as part of the intervention were first removed before\\ncomputing posttest scores.\\nthere were no significant condition differences on overall\\nposttest scores (p = .846). the intervention condition answered\\n57.6% (sd = .157) of the questions correctly while the yoked\\ncontrol condition answered 58.1% (sd = .129) correctly. this\\nfinding was not surprising as both conditions received the exact\\nsame treatment except that the interventions were triggered based\\n\\non detected mind wandering in the intervention condition but not\\nthe control condition.\\nnext, we examined posttest performance as a function of mind\\nwandering during reading. each page was designated as a low or\\nhigh mind wandering page based on a median split of mind\\nwandering likelihoods (medians = .35 and .36 on a 0 to 1 scale for\\nintervention and control conditions, respectively). we then\\nanalyzed performance on posttest questions corresponding to\\npages with low vs. high likelihoods of mind wandering (during\\nreading). the results are shown in table 3.\\nwe found no significant posttest differences on pages where both\\nthe intervention and control participants had low (p = .759) or\\nhigh (p = .922) mind wandering likelihoods (first and last rows in\\ntable 3, respectively). there was also no significant posttest\\ndifference (p = .630) for pages where the intervention condition\\nhad high mind wandering likelihoods but the control condition\\nhad low mind wandering likelihoods (row 3). however, the\\nintervention condition significantly (p = .003, d = .47 sigma)\\noutperformed the control condition for pages where the\\nintervention participants had low likelihoods of mind wandering\\nbut control participants had high mind wandering likelihoods\\n(row 2). these last two finding suggests that the intervention had\\nthe intended effect of reducing comprehension deficits\\nattributable to mind wandering because it led to equitable\\nperformance when mind wandering was high and improved\\nperformance when it was low.\\ntable 3: posttest performance (proportion of correct\\nresponses) as a function of mind wandering during reading.\\nstandard deviations in parenthesis.\\nmind\\nwandering\\n\\nposttest\\n\\nn\\n\\nint.\\n\\ncntrl.\\n\\nint.\\n\\ncntrl.\\n\\n43\\n\\nlow\\n\\nlow\\n\\n.604 (.288)\\n\\n.623 (.287)\\n\\n40\\n\\nlow\\n\\nhigh\\n\\n.643 (.263)\\n\\n.489 (.298)\\n\\n43\\n\\nhigh\\n\\nlow\\n\\n.535 (.295)\\n\\n.566 (.305)\\n\\n45\\n\\nhigh\\n\\nhigh\\n\\n.522 (.312)\\n\\n.515 (.291)\\n\\nscores\\n\\nnote. int. = intervention. cntrl. = control. bolded cells represent a\\nstatistically significant difference. n = number of pairs (out of 52) in each\\nanalysis. it differs slightly across analyses as not all participants were\\nassigned to each mind wandering group.\\n\\nafter-task interview. we interviewed a subset of the participants\\nin order to gauge their subjective experience with the\\nintervention. a few key themes emerged. participants reported\\npaying closer attention to the text after realizing they would be\\nperiodically answering multiple-choice questions. this was good.\\nhowever, participants also reported that they adapted their\\nreading strategies in one of two ways in response to the questions.\\nsince the questions targeted factual information (sometimes\\nverbatim) from the text, some participants paid more attention to\\ndetails and precise wordings instead of the broader concepts being\\ndiscussed in the text. more discouragingly, some participants\\nreported adopting a preemptive skimming strategy in that they\\nwould only look for keywords that they expected to appear in a\\nsubsequent question.\\nparticipants were encouraged to re-read text when they answered\\nincorrectly before receiving another question (or the same\\nquestion in some cases). many participants reported simply\\nscanning the text (when re-reading) to locate keywords from the\\nquestion before moving on. since the scanning strategy was often12\\n\\n\\x0csuccessful to answer the subsequent question, participants\\nreported that the questions were too easy and it took relatively\\nlittle effort to locate the correct answer compared to re-reading.\\nthey suggested that it may have been better if the questions had\\ntargeted key concepts rather than facts.\\nfinally, participants reported difficulties with re-engaging with\\nthe text after answering an online question because the text was\\ncleared when an intervention question was displayed; an item that\\ncan be easily corrected in subsequent versions.\\n\\n5. discussion\\nwe developed the first educational technology capable of realtime mind wandering detection and dynamic intervention during\\ncomputerized reading. in the remainder of this section, we discuss\\nthe significance of our main findings, limitations, and avenues for\\nfuture work.\\n\\n5.1 significance of main findings\\nwe have three main findings. first, we demonstrated that a\\nmachine-learned mind wandering detector built in one context\\ncan be applied to a different (albeit related) interaction context.\\nspecifically, the detector was trained on a data set involving\\nparticipants silently reading and self-reporting mind wandering,\\nbut was applied to an interactive context involving interpolated\\nassessments, which engendered different reading strategies.\\nfurther, self-reports of mind wandering were not collected in this\\ninteractive context, which might have influenced mind wandering\\nrates in and of itself. despite these differences, we were able to\\ndemonstrate the predictive validity of the detector by showing\\nthat it negatively correlated with both online and offline\\ncomprehension scores when evaluated on new participants.\\nsecond, we showed promising effects for our intervention\\napproach despite a very conservative experimental design, which\\nensured that the intervention and control groups were equated\\nalong all respects, except that the intervention was triggered based\\non the mind wandering detector (key manipulation). further, we\\nused a probabilistic approach to trigger an intervention, because\\nthe detector is inherently imperfect. as a result, participants could\\nhave received an intervention when they were not mind\\nwandering and/or could have failed to receive one when they were\\nmind wandering. therefore, it was essential to compare the two\\ngroups under conditions when the mind wandering levels\\ndiffered. this more nuanced analysis revealed that although the\\nintervention itself did not lead to a boost in overall comprehension\\n(because it is remedial), it equated comprehension scores when\\nmind wandering was high (i.e., scores for the intervention group\\nwere comparable when the control group was low on mind\\nwandering). it also demonstrated the cost of not intervening\\nduring mind wandering (i.e., scores for the intervention group\\nwere greater when the control group was high on mind\\nwandering). in other words, the intervention was successful in\\nmitigating the negative effects of mind wandering.\\nthird, despite the advantages articulated above, the intervention\\nitself was reactive and engendered several unintended (and\\npresumably suboptimal) behaviors. in particular, students altered\\ntheir reading strategies in response to the interpolated questions,\\nwhich were a critical part of the intervention. in a sense, they\\nattempted to “game the intervention” by attempting to proactively\\npredict the types of questions they might receive and then\\nadopting a complementary reading strategy consisting of\\nskimming and/or focusing on factual information. this reliance\\non surface- rather than deeper-levels of processing was\\nincongruent with our goal of promoting deep comprehension.\\n\\n5.2 limitations\\nthere are a number of methodological limitations with this work\\nthat go beyond limitations with the intervention (as discussed\\nabove). first, we focused on a single text that is perceived as\\nbeing quite dull and consequently triggers rather high levels of\\nmind wandering [26]. this raises the question of whether the\\ndetector will generalize to different texts. we expect some level\\nof generalizability in terms of features used because the detector\\nonly used content- and position- (on the screen) free global gaze\\nfeatures. however, given that several supervised classifiers are\\nvery sensitive to differences in base rates, the detector might overor under- predict mind wandering when applied to texts that\\nengender different rates of mind wandering. therefore, retraining\\nthe detector with a more diverse set of texts is warranted.\\nanother limitation is the scalability of our learning technology.\\nthe eye tracker we used was a cost-prohibitive tobii tx300 that\\nwill not scale beyond the laboratory. fortunately, commercialoff-the-shelf (cots) eye trackers, such as eye tribe and tobii\\neyex, can be used to surpass this limitation. it is an open question\\nas to whether the mind wandering detector can operate with\\nsimilar fidelity with these cots eye trackers. our use of global\\ngaze features which do not require high-precision eye tracking\\nholds considerable promise in this regard. nevertheless,\\nreplication with scalable eye trackers and/or scalable alternatives\\nto eye tracking (e.g., facial-feature tracking [46] or monitoring\\nreading patterns [27]) is an important next step (see section 5.3).\\nour use of surface-level questions for both the intervention and\\nthe subsequent comprehension assessment is also a limitation as\\nis the lack of a delayed comprehension assessment. it might be\\nthe case that the intervention effects manifest as richer encodings\\nin long-term memory, a possibility that cannot be addressed in the\\ncurrent experiment that only assessed immediate learning.\\nother limitations include a limited student sample (i.e.\\nundergraduates from a private midwestern college) and a\\nlaboratory setup. it is possible that the results would not\\ngeneralize to a more diverse student population or in more\\necological environments (but see below for evidence of\\ngeneralizability of the detector in classroom environments).\\nreplication with data from more diverse populations and\\nenvironments would be a necessary next step to increase the\\necological validity of this work.\\n\\n5.3 future work\\nour future work is progressing along two main fronts. one is to\\naddress limitations in the intervention and design of the\\nexperimental evaluation as discussed above. accordingly, we are\\nexploring alternative intervention strategies, such as: (a) tagging\\nitems for future re-study rather than interrupting participants\\nduring reading; (b) highlighting specific portions of the text as an\\novert cue to facilitate comprehension of critical information; (c)\\nasking fewer intervention questions, but selecting inference\\nquestions that target deeper levels of comprehension and that span\\nmultiple pages of the text; and (d) asking learners to engage in\\nreflection by providing written self-explanations of the textual\\ncontent. we are currently evaluating one such redesigned\\nintervention – open-ended questions targeting deeper levels of\\ncomprehension (item c). our revised experimental design taps\\nboth surface- and inference-level comprehension and assesses\\ncomprehension immediately after reading (to measure learning)\\nand after a one-week delay (to measure retention).\\nwe are also developing attention-aware versions of more\\ninteractive interfaces, such as learning with an intelligent tutoring13\\n\\n\\x0csystem called gurututor [30]. this project also addresses some\\nof the scalability concerns by replacing expensive research-grade\\neye tracking with cost-effective cots eye tracking (e.g., the eye\\ntribe or tobii eyex) and provides evidence for real-world\\ngeneralizability by collecting data in classrooms rather than the\\nlab. we recently tested our implementation on 135 students (total)\\nin a noisy computer-enabled high-school classroom where eyegaze of entire classes of students was collected during their\\nnormal class periods [20]. using a similar approach to the present\\nwork, we used the data to build and validate a studentindependent gaze-based mind wandering detector. the resultant\\nmind wandering detection accuracy (f1 of 0.59) was substantially\\ngreater than chance (f1 of 0.24) and outperformed earlier work on\\nthe same domain [21]. the next step is to develop interventions\\nthat redirect attention and correct learning deficiencies\\nattributable to mind wandering and to test the interventions in\\nreal-world environments. by doing so, we hope to advance our\\nfoundational vision of developing next-generation technologies\\nthat enhance the process and products of learning by “attending\\nto attention.”\\n\\n[6]\\n\\n[7]\\n\\n[8]\\n\\n[9]\\n\\n[10]\\n\\n[11]\\n\\n[12]\\nfigure 3: guru tutor interface overlaid with eye-gaze\\nobtained via the eyetribe\\n\\n[13]\\n\\n6. acknowledgements\\nthis research was supported by the national science foundation\\n(nsf) (drl 1235958 and iis 1523091). the authors are grateful\\nto kris kopp and jenny wu for their contributions to the study.\\nany opinions, findings and conclusions, or recommendations\\nexpressed in this paper are those of the authors and do not\\nnecessarily reflect the views of nsf.\\n\\n[14]\\n\\n[15]\\n\\n7. references\\n[1] anderson, j.r. 2002. spanning seven orders of magnitude:\\na challenge for cognitive modeling. cognitive science, 26\\n(1), 85-112.\\n[2] baird, b., smallwood, j., mrazek, m.d., kam, j.w.,\\nfranklin, m.s. and schooler, j.w. 2012. inspired by\\ndistraction mind wandering facilitates creative incubation.\\npsychological science, 23 (10), 1117-1122.\\n[3] bixler, r. and d'mello, s.k. 2016. automatic gaze-based\\nuser-independent detection of mind wandering during\\ncomputerized reading. user modeling & user-adapted\\ninteraction, 26, 33-68.\\n[4] boys, c.v. 1895. soap bubbles, their colours and the forces\\nwhich mold them. society for promoting christian\\nknowledge.\\n[5] conati, c., aleven, v. and mitrovic, a. 2013. eye-tracking\\nfor student modelling in intelligent tutoring systems. in\\nsottilare, r., graesser, a., hu, x. and holden, h. eds.\\ndesign recommendations for intelligent tutoring systems -\\n\\n[16]\\n\\n[17]\\n\\n[18]\\n\\n[19]\\n\\nvolume 1: learner modeling, army research laboratory,\\norlando, fl.\\nconati, c. and merten, c. 2007. eye-tracking for user\\nmodeling in exploratory learning environments: an\\nempirical evaluation. knowledge-based systems, 20 (6),\\n557-574.\\nd'mello, s., olney, a., williams, c. and hays, p. 2012.\\ngaze tutor: a gaze-reactive intelligent tutoring system.\\ninternational journal of human-computer studies, 70 (5),\\n377-398.\\nd'mello, s.k. 2016. giving eyesight to the blind: towards\\nattention-aware aied. international journal of artificial\\nintelligence in education, 26 (2), 645-659.\\nd'mello, s.k., blanchard, n., baker, r., ocumpaugh, j. and\\nbrawner, k. 2014. i feel your pain: a selective review of\\naffect-sensitive instructional strategies. in sottilare, r.,\\ngraesser, a., hu, x. and goldberg, b. eds. design\\nrecommendations for adaptive intelligent tutoring\\nsystems: adaptive instructional strategies (volume 2), us\\narmy research laboratory, orlando, fl.\\nd'mello, s.k., kopp, k., bixler, r. and bosch, n. 2016.\\nattending to attention: detecting and combating mind\\nwandering during computerized reading in extended\\nabstracts of the acm sigchi conference on human\\nfactors in computing systems (chi 2016), acm, new\\nyork.\\ndrummond, j. and litman, d. 2010. in the zone: towards\\ndetecting student zoning out using supervised machine\\nlearning. in aleven, v., kay, j. and mostow, j. eds.\\nintelligent tutoring systems., springer-verlag, berlin /\\nheidelberg.\\neastwood, j.d., frischen, a., fenske, m.j. and smilek, d.\\n2012. the unengaged mind: defining boredom in terms of\\nattention. perspectives on psychological science, 7 (5), 482495.\\nfaber, m., bixler, r. and d'mello, s.k. in press. an\\nautomated behavioral measure of mind wandering during\\ncomputerized reading. behavior research methods.\\nfranklin, m.s., broadway, j.m., mrazek, m.d., smallwood,\\nj. and schooler, j.w. 2013. window to the wandering mind:\\npupillometry of spontaneous thought while reading. the\\nquarterly journal of experimental psychology, 66 (12),\\n2289-2294.\\ngluck, k.a., anderson, j.r. and douglass, s.a. 2000.\\nbroader bandwidth in student modeling: what if its were\\n“eye” ts? in gauthier, c., frasson, c. and vanlehn, k. eds.\\nproceedings of the 5th international conference on\\nintelligent tutoring systems, springer, berlin.\\ngraesser, a., lu, s., olde, b., cooper-pye, e. and whitten,\\ns. 2005. question asking and eye tracking during cognitive\\ndisequilibrium: comprehending illustrated texts on devices\\nwhen the devices break down. memory and cognition, 33,\\n1235-1247.\\nhanley, j.a. and mcneil, b.j. 1982. the meaning and use\\nof the area under a receiver operating characteristic (roc)\\ncurve. radiology, 143 (1), 29-36.\\nharley, j.m., lajoie, s.p., frasson, c. and hall, n.c. in\\npress. developing emotion-aware, advanced learning\\ntechnologies: a taxonomy of approaches and features.\\ninternational journal of artificial intelligence in education.\\nhegarty, m. and just, m. 1993. constructing mental models\\nof machines from text and diagrams. journal of memory and\\nlanguage, 32 (6), 717-742.14\\n\\n\\x0c[20] hutt, s., mills, c., bosch, n., krasich, k., brockmole, j.r.\\nand d'mello, s.k. in review. out of the fr-eye- ing pan:\\ntowards gaze-based models of attention during learning\\nwith technology in the classroom.\\n[21] hutt, s., mills, c., white, s., donnelly, p.j. and d’mello,\\ns.k. 2016. the eyes have it: gaze-based detection of mind\\nwandering during learning with an intelligent tutoring\\nsystem. in proceedings of the 9th international conference\\non educational data mining (edm 2016), international\\neducational data mining society.\\n[22] jaques, n., conati, c., harley, j.m. and azevedo, r. year.\\npredicting affect from gaze data during interaction with an\\nintelligent tutoring system. in intelligent tutoring systems,\\n(2014), springer, 29-38.\\n[23] kardan, s. and conati, c. 2012. exploring gaze data for\\ndetermining user learning with an interactive simulation. in\\ncarberry, s., weibelzahl, s., micarelli, a. and semeraro, g.\\neds. proceedings of the 20th international conference on\\nuser modeling, adaptation, and personalization (umap\\n2012), springer, berlin.\\n[24] killingsworth, m.a. and gilbert, d.t. 2010. a wandering\\nmind is an unhappy mind. science, 330 (6006), 932-932.\\n[25] kintsch, w. 1998. comprehension: a paradigm for\\ncognition. cambridge university press, new york.\\n[26] kopp, k., d’mello, s. and mills, c. 2015. influencing the\\noccurrence of mind wandering while reading. consciousness\\nand cognition, 34 (1), 52-62.\\n[27] mills, c. and d’mello, s.k. 2015. toward a real-time (day)\\ndreamcatcher: detecting mind wandering episodes during\\nonline reading. in romero, c., pechenizkiy, m., boticario,\\nj. and santos, o. eds. proceedings of the 8th international\\nconference on educational data mining (edm 2015),\\ninternational educational data mining society.\\n[28] mooneyham, b.w. and schooler, j.w. 2013. the costs and\\nbenefits of mind-wandering: a review. canadian journal of\\nexperimental psychology/revue canadienne de psychologie\\nexpérimentale, 67 (1), 11.\\n[29] muir, m. and conati, c. 2012. an analysis of attention to\\nstudent–adaptive hints in an educational game. in cerri,\\ns.a., clancey, w.j., papadourakis, g. and panourgia, k.\\neds. proceedings of the international conference on\\nintelligent tutoring systems, springer, berlin.\\n[30] olney, a., d'mello, a., person, n., cade, w., hays, p.,\\nwilliams, c., lehman, b. and graesser, a. 2012. guru: a\\ncomputer tutor that models expert human tutors. in cerri, s.,\\nclancey, w., papadourakis, g. and panourgia, k. eds.\\nproceedings of the 11th international conference on\\nintelligent\\ntutoring\\nsystems,\\nspringer-verlag,\\nberlin/heidelberg.\\n[31] olney, a., risko, e.f., d'mello, s.k. and graesser, a.c.\\n2015. attention in educational contexts: the role of the\\nlearning task in guiding attention. in fawcett, j., risko, e.f.\\nand kingstone, a. eds. the handbook of attention, mit\\npress, cambridge, ma.\\n[32] pham, p. and wang, j. 2016. adaptive review for mobile\\nmooc learning via implicit physiological signal sensing.\\nin proceedings of the 18th acm international conference\\non multimodal interaction (icmi 2016), acm, new york,\\nny.\\n[33] pham, p. and wang, j. 2015. attentivelearner: improving\\nmobile mooc learning via implicit heart rate tracking. in\\ninternational conference on artificial intelligence in\\neducation, springer, berlin heidelberg.\\n\\n[34] randall, j.g., oswald, f.l. and beier, m.e. 2014. mindwandering, cognition, and performance: a theory-driven\\nmeta-analysis of attention regulation. psychological\\nbulletin, 140 (6), 1411-1431.\\n[35] rapp, d.n. 2006. the value of attention aware systems in\\neducational settings. computers in human behavior, 22 (4),\\n603-614.\\n[36] rayner, k. 1998. eye movements in reading and information\\nprocessing: 20 years of research. psychological bulletin, 124\\n(3), 372-422.\\n[37] reichle, e.d., reineberg, a.e. and schooler, j.w. 2010. eye\\nmovements during mindless reading. psychological science,\\n21 (9), 1300.\\n[38] risko, e.f., buchanan, d., medimorec, s. and kingstone, a.\\n2013. everyday attention: mind wandering and computer\\nuse during lectures. computers & education, 68 (1), 275283.\\n[39] roda, c. and thomas, j. 2006. attention aware systems:\\ntheories, applications, and research agenda. computers in\\nhuman behavior, 22 (4), 557-587.\\n[40] rowe, j., mott, b., mcquiggan, s., robison, j., lee, s. and\\nlester, j. year. crystal island: a narrative-centered learning\\nenvironment for eighth grade microbiology. in workshop on\\nintelligent educational games at the 14th international\\nconference on artificial intelligence in education,\\nbrighton, uk, (2009), 11-20.\\n[41] shute, v.j., ventura, m., bauer, m. and zapata-rivera, d.\\n2009. melding the power of serious games and embedded\\nassessment to monitor and foster learning: flow and grow.\\nin ritterfeld, u., cody, m. and vorderer, p. eds. serious\\ngames: mechanisms and effects, routledge, taylor and\\nfrancis, mahwah, nj.\\n[42] sibert, j.l., gokturk, m. and lavine, r.a. 2000. the reading\\nassistant: eye gaze triggered auditory prompting for reading\\nremediation. in proceedings of the 13th annual acm\\nsymposium on user interface software and technology,\\nacm, new york, ny.\\n[43] smallwood, j., davies, j.b., heim, d., finnigan, f.,\\nsudberry, m., o'connor, r. and obonsawin, m. 2004.\\nsubjective experience and the attentional lapse: task\\nengagement and disengagement during sustained attention.\\nconsciousness and cognition, 13 (4), 657-690.\\n[44] smallwood, j., fishman, d.j. and schooler, j.w. 2007.\\ncounting the cost of an absent mind: mind wandering as an\\nunderrecognized influence on educational performance.\\npsychonomic bulletin & review, 14 (2), 230-236.\\n[45] smallwood, j. and schooler, j.w. 2015. the science of mind\\nwandering: empirically navigating the stream of\\nconsciousness. annu. rev. psychol, 66, 487-518.\\n[46] stewart, a., bosch, p., chen, h., donnelly, p.j. and\\nd’mello, s.k. 2016. where's your mind at? video-based\\nmind wandering detection during film viewing. in aroyo,\\nl., d'mello, s., vassileva, j. and blustein, j. eds.\\nproceedings of the 2016 acm on international conference\\non user modeling, adaptation, & personalization (acm\\numap 2016), acm, new york.\\n[47] szpunar, k.k., khan, n.y. and schacter, d.l. 2013.\\ninterpolated memory tests reduce mind wandering and\\nimprove learning of online lectures. proceedings of the\\nnational academy of sciences, 110 (16), 6313-6317.\\n[48] wang, h., chignell, m. and ishizuka, m. 2006. empathic\\ntutoring software agents using real-time eye tracking. in\\nproceedings of the 2006 symposium on eye tracking\\nresearch &applications, acm, new york.\",\n",
       " '15\\n\\n\\x0cmeasuring similarity of educational items using data on\\nlearners’ performance\\njiří řihák\\n\\nfaculty of informatics\\nmasaryk university\\nbrno, czech republic\\n\\nthran@mail.muni.cz\\nabstract\\neducational systems typically contain a large pool of items\\n(questions, problems). using data mining techniques we can\\ngroup these items into knowledge components, detect duplicated items and outliers, and identify missing items. to\\nthese ends, it is useful to analyze item similarities, which can\\nbe used as input to clustering or visualization techniques.\\nwe describe and evaluate different measures of item similarity that are based only on learners’ performance data, which\\nmakes them widely applicable. we provide evaluation using\\nboth simulated data and real data from several educational\\nsystems. the results show that pearson correlation is a suitable similarity measure and that response times are useful\\nfor improving stability of similarity measures when the scope\\nof available data is small.\\n\\n1. introduction\\ninteractive educational systems offer learners items (problems, questions) for solving. realistic educational systems\\ntypically contain a large number of such items. this is particularly true for adaptive systems, which try to present suitable items for different kinds of learners. the management\\nof a large pool of items is difficult. however, educational\\nsystems collect data about learners’ performance and the\\ndata can be used to get insight into item properties. in this\\nwork we focus on methods for computing item similarities\\nbased on learners’ performance data, which consists of binary information about the answers (correct/incorrect).\\nautomatically detected item similarities are the first and\\nnecessary step in further analysis such as clustering of the\\nitems, which is useful in several ways, with one particular\\napplication being learner modeling [9]. learner models estimate knowledge and skills of learners and are the basis\\nof adaptive behavior of educational systems. a learner’s\\nmodels requires a mapping of items into knowledge components [17]. item clusters can serve as a basis for knowledge\\ncomponent definition or refinement. the specified knowledge components are relevant not only for modeling, but\\n\\nradek pelánek\\n\\nfaculty of informatics\\nmasaryk university\\nbrno, czech republic\\n\\npelanek@mail.muni.cz\\nthey are typically directly visible to\\nterface of a system, e.g., in a form\\nvisualizing the estimated knowledge\\nized overview of mistakes, which is\\ncomponents.\\n\\nlearners in the user inof open learner model\\nstate, or in a personalgrouped by knowledge\\n\\ninformation about items is also very useful for management\\nof the content of educational systems – preparation of new\\nitems, filtering of unsuitable items, preparation of explanations, and hint messages. information about item similarities and clusters can be also relevant for teachers as it can\\nprovide them an inspiration for “live” discussions in class.\\nthis type of applications is in line with baker’s argument [1]\\nfor focusing on the use of learning analytics for “leveraging\\nhuman intelligence” instead of its use for automatic intelligent methods.\\nitem similarities and clusters are studied not only in educational data mining but also in a closely related area of\\nrecommender systems. the setting of recommender systems\\nis in many aspects very similar to educational systems – in\\nboth cases we have users and items, just instead of “performance” (the correctness of answers, the speed of answers)\\nrecommender systems consider “ratings” (how much a user\\nlikes an item). item similarities and clustering techniques\\nhave thus been also considered in the recommender systems\\nresearch (we mention specific techniques below). there is a\\nslight, but important difference between the two areas. in\\nrecommender systems item similarities and clusterings are\\ntypically only auxiliary techniques hidden within a “recommendation black box”. in educational system, it is useful to\\nmake these results explicitly available to system developers,\\ncurriculum production teams, or teachers.\\nthere are two basic approaches to dealing with item similarities and knowledge components: a “model based approach”\\nand an “item similarity approach”. the basic idea of the\\nmodel based approach is to construct a simplified model that\\nexplains the observed data. based on a matrix of learners’\\nanswers to items we construct a model that predicts these\\nanswers. typically, the model assigns several latent skills to\\nlearners and uses a mapping of items to corresponding latent\\nfactors. this kind of models can often be naturally expressed\\nusing matrix multiplication, i.e., fitting a model leads to matrix factorization. once we fit the model to data, items that\\nhave the same value of a latent factor can be denoted as\\n“similar”. this approach leads naturally to multiple knowledge components per skill. the model is typically computed16\\n\\n\\x0cusing some optimization technique that leads only to local\\noptima (e.g., gradient descent). it is thus necessary to address the role of initialization, and parameter setting of the\\nsearch procedure. in recommender systems this approach is\\nused for implementation of collaborative filtering; it is often\\ncalled “singular value decomposition” (svd) [18]. in educational context many variants of this approach have been\\nproposed under different names and terminology, e.g., qmatrix [3], non-negative matrix factorization techniques [8],\\nsparse factor analysis [19], or matrix refinement [10].\\n\\nsyntactic\\nsimilarity\\nof items\\n\\nlearner\\ndata\\n\\nexpert\\ninput\\n\\nitem similarity\\nmatrix\\nvisualization\\n\\nclusters\\n\\nwith the item similarity approach we do not construct an\\nexplicit model of learners’ behavior, but we compute directly\\na similarity measure for each pairs of items. these similarities are then used to compute clusters of items, to project\\nitems into a plane, or for other analysis (e.g., for each item\\nlisting the 3 most similar items). this approach naturally\\nleads to a mapping with a single knowledge component per\\nitem (i.e., different kind of output from most model based\\nmethods). one advantage of this approach is easier interpretability. in recommender system research this approach\\nis called neighborhood-based methods [11] or item-item collaborative filtering [7]. similarity has been used for clustering of items [23, 24] and also for clustering of users [29].\\nin educational setting item similarity has been analyzed using correlation of learners’ answers [22] and problem solving\\ntimes [21], and also using learners’ wrong answers [25].\\n\\nfigure 1: high-level illustration of the general approach to item analysis based on item similarities.\\n\\nso far we have discussed methods that are based only on\\ndata about learners’ answers. often we have some additional\\ninformation about items and their similarities, e.g., a manual labeling or data based on syntactic similarity of items\\n(text of questions). for both model based and item similarity approaches previous research has studied techniques for\\ncombination of these different types of inputs [10, 21].\\n\\nin this work we focus on computing item similarities using\\nlearners’ performance data. as figure 1 shows, the similarity computation can also utilize information from domain\\nexperts or automatically determined information based on\\nthe inner structure of items (e.g., text of questions or some\\navailable meta-data).\\n\\nin this work we focus on the item similarity approach, because in the educational setting this approach is less explored than the model based approach. we discuss specific\\ntechniques, clarify details of their usage, and provide evaluation using both data from real learners and simulated data.\\nsimulated data are useful for evaluation of the considered\\nunsupervised machine learning tasks, because in the case of\\nreal-world data we do not know the “ground truth”.\\nthe specific contributions of this work are the following. we\\nprovide guidelines for the choice of item similarity measures\\n– we discuss different options and provide results identifying\\nsuitable measures (pearson, yule, cohen); we also demonstrate the usefulness of “two step similarity measures”. we\\nexplore benefits of the use of response time information as\\nsupplement to usual information of correctness of answer.\\nwe use and discuss several evaluation methods for the considered tasks. we specifically consider the issue of “how\\nmuch data do we need”. this is often practically more important than the exact choice of a used technique, but the\\nissue is rather neglected in previous work.\\n\\n2.\\n\\nmeasures of item similarity\\n\\nfigure 1 provides a high-level illustration of the item similarity approach. this approach consist of two steps that\\nare to a large degree independent. at first, we compute an\\nitem similarity matrix, i.e., for each pair of items i, j we\\n\\ncompute similarity sij of these items. at second, we can\\nconstruct clusters or visualizations of items using only the\\nitem similarity matrix.\\nexperience with clustering algorithms suggests that the appropriate choice of similarity measure is more important\\nthan choice of clustering algorithm [13]. the choice of similarity measure is domain specific and it is typically not explored in general research on clustering. therefore, we focus\\non the first step – the choice of similarity measure – and explore it for the case of educational data.\\n\\n2.1 basic setting\\n\\nwe discuss different possibilities for computation of item\\nsimilarities. note that in our discussion we consistently use\\n“similarity measures” (higher values correspond to higher\\nsimilarity), some related works provide formulas for dissimilarity measures (distance of items; lower values correspond\\nto higher similarity). this is just a technical issue, as we can\\neasily transform similarity into dissimilarity by subtraction.\\nthe input to item similarity computation are data about\\nlearner performance, i.e., a matrix l × i, where l is the\\nnumber of learners and i is the number of items. the matrix values specify learners’ performance. the matrix is typically very sparse (many missing values). the output of the\\ncomputation is an item similarity matrix, which specifies\\nsimilarity for each pair of items.\\nnote that in our discussion we mostly ignore the issue of\\nlearning (change of learners skill as they progress through\\nitems). when learning is relatively slow and items are presented in a randomized order, learning is just a reasonably\\nsmall source of noise and does not have a fundamental impact on the computation of item similarities. in cases where\\nlearning is fast or items are presented in a fixed order, it\\nmay be necessary to take learning explicitly into account.\\n\\n2.2 correctness of answers\\nthe basic type of information available in educational systems is the correctness of learners’ answers. so we start with17\\n\\n\\x0csimilarity measures that utilize only this type of information, i.e., dichotomous data (correct/incorrect) on learners’\\nanswers on items. the advantage of these measures is that\\nthey are applicable in wide variety of settings.\\nwith dichotomous data we can summarize learners’ performance on items i and j using an agreement matrix with\\njust four values (table 1). although we have just four values to quantify the similarity of items i and j, previous research has identified large number of different measures for\\ndichotomous data and analyzed their relations [5, 12, 20].\\nfor example choi et al. [5] discuss 76 different measures, albeit many of them are only slight variations on one theme.\\nsimilarity measures over dichotomous data are often used in\\nbiology (co-occurrence of species) [14]. a more directly relevant application is the use of similarity measures for recommendations [30]. recommender systems typically use either\\npearson correlation or cosine similarity for computation of\\nitem similarities [11], but they consider richer than binary\\ndata.\\ntable 1: an agreement matrix for two items and definitions of similarity measures based on the agreement matrix (n = a + b + c + d is the total number of\\nobservations).\\nitem i\\nincorrect correct\\nitem j\\n\\nincorrect\\ncorrect\\n\\na\\nc\\n\\nb\\nd\\n\\ncohen\\n\\nsy = (ad − bc)/(ad + bc)\\np\\nsp = (ad − bc)/ (a + b)(a + c)(b + d)(c + d)\\n\\nsokal\\n\\nss = (a + d)/(a + b + c + d)\\n\\njaccard\\n\\nsj = a/(a + b + c)\\np\\nso = a/ (a + b)(a + c)\\n\\nyule\\npearson\\n\\nochiai\\n\\nsc = (po − pe )/(1 − pe )\\npo = (a + d)/n\\npe = ((a + b)(a + c) + (b + d)(c + d))/n2\\n\\ntable 1 provides definitions of 6 measures that we have chosen for our comparison. in accordance with previous research (e.g., [5, 14]) we call measures by names of researchers\\nwho proposed them. the choice of measures was done in\\nsuch a way as to cover measures used in the most closely related work and measures which achieved good results (even\\nif the previous work was in other domains). we also tried\\nto cover different types of measures.\\npearson measure is the standard pearson correlation coefficient evaluated over the dichotomous data. in the context of dichotomous data it is also called phi coefficient or\\nmatthews correlation coefficient. yule measure is similar\\nmeasure, which achieved good results in previous work [30].\\ncohen measure is typically used as a measure of inter-rater\\nagreement (it is more commonly called “cohen’s kappa”).\\nin our setting it makes sense to consider this measure when\\n\\nwe view learners’ answers as “ratings” of items. relations\\nbetween these three measures are discussed in [32].\\nochiai coefficient is typically used in biology [14]. it is also\\nequivalent to cosine similarity evaluated over dichotomous\\ndata; cosine similarity is often used in recommender systems for computing item similarity, albeit typically over interval data [7]. sokal measure is also called sokal-michener\\nor “simple matching”. it is equivalent to accuracy measure\\nused in information retrieval. together with jaccard measure they are often used in biology, but they have also been\\nused for clustering of educational data [12].\\nnote that some similarity measures are asymmetric with respect to 0 and 1 values. these measures are typically used\\nin contexts where the interpretation of binary values is presence/absence of a specific feature (or observation). in the\\neducational context it is more natural to use measures which\\ntreat correct and incorrect answers symmetrically. nevertheless, for completeness we have included also some of the\\ncommonly used asymmetric measures (ochiai and jaccard).\\nin these cases we focus on incorrect answers (value a as opposed to d) as these are typically less frequent and thus bear\\nmore information.\\n\\n2.3\\n\\nother data sources\\n\\nthe correctness of answers is the basic source of information about item similarities, but not the only one. we\\ncan also use other data. the second major type of performance data is response time (time taken to answer an\\nitem). the basic approach to utilization of response time\\nis to combine it with the correctness of an answer. given\\nthe correctness value c ∈ {0, 1}, a response time t ∈ r+ ,\\nand the median of all response times τ , we combine them\\ninto a single score r. examples of such transformations\\nare: linear transformation for correct answers only (r =\\nc · max(1 − t/2τ, 0)); exponential discounting used in matmat [28] (r = c · min(1, 0.9t/τ −1 )); linear transformation\\ninspired by high speed, high stakes scoring rule used in math\\ngarden [16] (r = (2c − 1) · max(1 − t/2τ, 0)). the first\\napproach was used in our experiment due to its simplicity\\nand high influence of response time information.\\nthe scores obtained in this way are real numbers. given the\\nscores it is natural to compute similarity of two items using\\npearson correlation coefficient of scores (over learners who\\nanswered both items). it is also possible to utilize specific\\nwrong answers for computation of item similarity [25].\\nit is also possible to combine performance based measures\\nwith other types of data. for example we may estimate\\nitem similarity based on analysis of the content of items\\n(syntactical similarity of texts), or collect expert opinion\\n(manual categorization of items into several groups). the\\nadvantage of the similarity approach (compared to model\\nbased approach) is that different similarity measures can be\\nusually combined in straightforward way by using a weighted\\naverage of different measures.\\n\\n2.4 second level of item similarity\\nthe basic computation of item similarities computes similarity of items i and j using only data about these two items.\\nto improve a similarity measure, it is possible to employ a18\\n\\n\\x0c“second of level of item similarity” that is based on the computed item similarity matrix and uses information on all\\nitems. examples of such a second step is euclidean distance\\nor correlation. similarity of items i and j is given by the\\neuclidean distance or pearson correlation of rows i and j\\nin the similarity matrix. note that euclidean distance may\\nbe used implicitly when we use standard implementation of\\nsome clustering algorithms (e.g., k-means).\\nwith the basic approach to item similarity, we consider\\nitems similar when performance of learners on these items is\\nsimilar. with the second step of item similarity, we consider\\ntwo items similar when they behave similarly with respect\\nto other items. the main reason for using this second step\\nis the reduction of noise in data by using more information. this may be useful particularly to deal with learning.\\ntwo very similar items may have rather low direct similarity, because getting a feedback on the first item can strongly\\ninfluence the performance on the second item. however, we\\nexpect both items to have similar similarities to other items.\\na more technical reason to using the second step (particularly the euclidean distance) is to obtain a measure that\\nis a distance metric. the measures described above mostly\\ndo not satisfy triangle inequality and thus do not satisfy\\nthe requirements on distance metric; this property may be\\nimportant for some clustering algorithms.\\n\\n3. evaluation\\nin this work we focus on item similarity, but we keep the\\noverall context depicted in figure 1 in mind. the quality of\\na visualization is to a certain degree subjective and difficult\\nto quantify, but the quality of clusters can be quantified and\\nthus we can use it to compare similarity measures. from\\nthe large pool of existing clustering algorithms [15] we consider k-means, which is the most common implementation\\nof centroid-based clustering, and hierarchical clustering. we\\nused agglomerative or “bottom up” approach where items\\nare successively merged to clusters using ward’s method as\\nlinkage criteria.\\n\\n3.1\\n\\ndata\\n\\nwe use data from real educational systems as well as simulated learner data. real-world data provide information\\nabout the realistic performance of techniques, but the evaluation is complicated by the fact that we do not know the\\n“ground truth” (the “correct” similarity or clusters of items).\\nsimulated data provide a setting that is in many aspects\\nsimplified but allows easier evaluation thanks to the access\\nto the ground truth.\\nfor generating simulated data we use a simple approach\\nwith minimal number of assumptions and ad hoc parameters. each item belongs to one of k knowledge components. each knowledge component contains n items. each\\nitem has a difficulty generated from the standard normal\\ndistribution di ∼ n (0, 1). skills of learners with respect to\\nindividual knowledge components are independent. skill of\\na learner l with respect to knowledge component j is generated from the standard normal distribution θlj ∼ n (0, 1).\\nwe assume no learning (constant skills). answers are generated as bernoulli trials with the probability of a correct\\nanswer given by the logistic function of the difference of a\\n\\ntable 2: data used for analysis.\\nlearners items\\nanswers\\nczech 1 (adjectives)\\nczech 2\\nmatmat: numbers\\nmatmat: addition\\nmath garden: addition\\nmath garden: multiplic.\\n\\n1 134\\n4 567\\n6 434\\n3 580\\n83 297\\n97 842\\n\\n108\\n210\\n60\\n135\\n30\\n30\\n\\n62 613\\n336 382\\n67 753\\n20 337\\n881 994\\n1 233 024\\n\\nrelevant skill and an item difficulty (a rasch model): p =\\nexp(θlj − di )−1 . this approach is rather standard, for example piech at al. [26] use very similar procedure and also\\nother works use closely related procedures [4, 12]. in the\\nexperiment reported below the basic setting is 100 learners,\\n5 knowledge components with 20 items each.\\nto evaluate techniques on realistic educational data, we use\\ndata from three educational systems. table 2 describes the\\nsize of the used data sets.\\numı́me česky (umimecesky.cz) is a system for practice of\\nczech spelling and grammar. we use data only from one exercise from the system – simple “fill-in-the-blank” questions\\nwith two options. we use only data on the correctness of\\nanswers (response time is available, but since it depends on\\nthe text of a particular item its utilization is difficult). we\\nfocus particularly on one subset of items: questions about\\nthe choice between i/y in suffixes of czech adjectives. for\\nthis subset we have manually determined 7 groups of items\\ncorresponding to czech grammar rules.\\nmatmat (matmat.cz) is a system for practice of basic arithmetic (e.g., counting, addition, multiplication). for each\\nitem we know the underlying construct (e.g., “13” or “7 +\\n8”) and also the specific form of questions (e.g., what type of\\nvisualization has been used). we use data on both correctness and response time. we selected the two largest subsets:\\nmultiplication and numbers (practice of number sens, counting).\\nmath garden is another system for practice of basic arithmetic [16]. this system is more widely used than matmat,\\nbut we do not have direct access to the system and detailed\\ndata. for the analysis we reuse publicly available data from\\nprevious research [6]. the available data contain both correctness of answers and response times, but they contain\\ninformation only about 30 items without any identification\\nof these items.\\n\\n3.2 comparison of similarity measures\\nto evaluate similarity measures we consider several types\\nof analysis. with simulated data, we analyze the similarity\\nmeasures with respect to the ground truth while for realworld data we evaluate correlations among similarity measures. we also compare the quality of subsequent clusterings using adjusted rand index (ari) [27, 31], which measures the agreement of two clusterings (with a correction for\\nagreement due to chance). typically, we use the adjusted\\nrand index to compare the clustering with a ground truth\\n(available for simulated data) or with a manually provided19\\n\\n\\x0cpearson\\n\\npearson → pearson\\n\\nwithin-cluster\\n\\nwithin-cluster\\n\\nbetween-cluster\\n\\nbetween-cluster\\n\\nyule\\n\\nczech 1 (adjectives)\\n\\nmatmat: numbers\\n\\nyule → pearson\\n\\nwithin-cluster\\n\\nwithin-cluster\\n\\nbetween-cluster\\n\\nbetween-cluster\\n\\njaccard\\n\\nsokal\\n\\nwithin-cluster\\n\\nwithin-cluster\\n\\nbetween-cluster\\n\\nbetween-cluster\\n\\nfigure 3: correlations of similarity measures.\\n\\nfigure 2: differences between similarity values\\ninside knowledge components and between them.\\nsimulated data set with the basic setting were used.\\n\\nclassification (available for the czech 1 data set). it can be\\nalso used to compare two detected clusterings (clusterings\\nbased on two different algorithms or clusterings based on\\ntwo independent halves of data).\\nas a first step in the evaluation of similarity measures, we\\nconsider experiments with simulated data where we can utilize the ground truth. in clustering we expect high withincluster similarity values and low between-cluster similarity\\nvalues. figure 2 shows distribution of the similarity values\\nfor selected measures and suggest which measures separate\\nwithin-cluster and between-cluster values better and therefore which measures will be more useful in clustering. the\\nresults show that for jaccard and sokal measures the values overlap to a large degree, whereas pearson and yule\\nmeasures provide better results. adding the second step –\\npearson correlation in this example – to the similarity measure separates within-cluster and between-cluster values better. that suggests that extending similarities in this way is\\nnot only necessary step for some subsequent algorithms such\\nas k-means but also a useful technique with better performance.\\nfor data coming from real systems we do not know the\\nground truth and thus we can only compare the similarity measures to each other. to evaluate how similar two\\nmeasures are we take all similarity values for all item pairs\\nand computed correlation coefficient. figure 3 shows results\\nfor two data sets which are good representatives of overall results. pearson and cohen measures are highly correlated (> 0.98) across all data sets and have nearly the same\\nvalues (although not exactly the same). larger differences\\n(but only up to 0.1) can be found typically when one of the\\nvalues in the agreement matrix is small and that happens\\nonly for poorly correlated items with the resulting similarity value around 0. the second pair of highly correlated\\nmeasures is ochiai and jaccard, which are both asymmetric\\nwith respect to the agreement matrix. the correlation between these two pairs of measures vary depending on data\\nset and in some cases drops up to 0.5. because of this high\\ncorrelation within these pairs we further report results only\\n\\nfor pearson and jaccard measures. yule measure is usually\\nsimilar to pearson measure (correlation usually around 0.9).\\nthe main difference is that the yule measure spreads values\\nmore evenly across the interval [-1, 1]. sokal is the most\\noutlying measure with no correlation or small correlation\\n(usually < 0.6) with all other measures.\\nfigure 4 shows the effect of the second levels of item similarity on the pearson measure (results for other measures\\nare analogical). the euclid distance as second level similarity brings larger differences (lower correlation) than pearson\\ncorrelation. the correlations for large data sets such as math\\ngarden are usually high (> 0.9) and conversely the lowest\\ncorrelations are found in results for small data sets. this\\nsuggests that the second level of similarity is more significant, and thus potentially more useful, where only limited\\namount of data is available.\\nczech 1 (adjectives)\\n\\nczech 2\\n\\nmatmat: numbers\\n\\n6\\nmatmat: addition\\n\\nmath garden: addition\\n\\nmath garden: multiplic.\\n\\nfigure 4: correlations of pearson measure and pearson with different second levels.\\nfinally, we evaluate the quality of the similarity measures\\naccording to the performance of the subsequent clustering.\\nfrom the two considered clustering methods we used the hierarchical clustering in this comparison because it naturally\\nworks with similarity measure and does not require metric\\nspace. the other two methods have similar result with same\\nconclusions. table 3 and figure 5 show results. although\\nthe results are dependent on the specific data set and the\\nused clustering algorithm, there is quite clear general conclusion. pearson and yule measures provide better results20\\n\\n\\x0c1223\\n\\ncorrelation\\n\\n882\\n\\ndata set\\nczech 1 (adjectives)\\nczech 2\\nmatmat: numbers\\nmatmat: addition\\nmath garden: addition\\nmath garden: multiplic.\\n\\n336\\n63\\n68\\n\\n20\\n\\nsample size\\n\\nfigure 6: stability of similarity measure (yule) for\\nreal-world data sets. data set was sampled, split\\nto halves and pearson correlation was computed for\\nsimilarity values. numbers on the right side indicate\\nthousands of answers in data sets.\\n\\nfigure 5: the quality of clustering for different measures used in the second step of item similarity. top:\\nsimulated data with 5 correlated skills. bottom:\\nczech grammar with 7 manually determined clusters.\\nthan jaccard and sokal, i.e., for the considered task the\\nlater two measures are not suitable. the pearson is usually\\nslightly better than yule but the choice between them seems\\nnot to be fundamental (which is not surprising given that\\nthey are highly correlated). the results also show that the\\n“second step” is always useful. the result for simulated data\\nfavor euclidean distance over pearson but there are almost\\nno differences for real-world data.\\n\\n3.3\\n\\ndo we have enough data?\\n\\nin machine learning the amount of available data often is\\nmore important than the choice of a specific algorithm [2].\\nour results suggest that once we choose a suitable type of\\nsimilarity measure (e.g., pearson, cohen, or yule), the differences between these measures are not fundamental, the\\nmore important issue becomes the size of available data.\\nspecifically, for a given data set we want to know whether\\nthe data are sufficiently large so that the computed item\\nsimilarities are meaningful and stable. this issue can be explored by analyzing confidence intervals for computed similarity values. as a simple approach to analysis of similarity stability we propose the following approach: we split\\nthe available data into two independent halves (in a learner\\nstratified manner), for each half we compute the item similarities, and we compute the correlation of the resulting item\\nsimilarities.\\n\\nwe can also perform this computation for artificially reduced\\ndata sets – this shows how the stability of results increases\\nwith the size of data. figure 6 shows this kind of analysis\\nfor our data (real-world data sets). we clearly see large differences among individual data sets. math garden data set\\ncontains large number of answers and only a few items, the\\nresults show excellent stability, clearly in this case we have\\nenough data to analyze item similarities. for the czech\\ngrammar data set we have large number of answers, but\\nthese are divided among relatively large number of items.\\nthe results show a reasonably good stability, the data are\\nusable for analysis, but clearly more data can bring improvement. for matmat data the stability is poor, to draw solid\\nconclusions about item similarities we need more data.\\n\\n3.4 response time utilization\\nthe incorporation of response time information to similarity measure can change the meaning of similarity. figure 7\\ngives such example and shows projection of items from matmat practicing number sense. similar items according to\\nmeasures using only correctness of answers tend to be items\\nwith the same graphical representation in the system. on\\nthe other hand, similar items according to measures using\\nalso response time are usually items practicing close numbers.\\nwe used this method also on data sets from math garden,\\nwhich are much larger. in this case the use of response\\ntimes has only small impact on the computed item similarities (correlations between 0.9 and 0.95). however, the use of\\nresponse times influences how quickly does the computation\\nconverge, i.e., how much data do we need. to explore this\\nwe consider as the ground truth the average of computed\\nsimilarity matrices with and without response times for the\\nwhole data set. then we used smaller samples of the data\\nset, used them to compute item similarities and checked the\\nagreement with this ground truth. figure 8 shows the difference between speed of convergence of measure with and\\nwithout response time utilization. results shows that the\\nmeasure which use addition information from response time\\nconverges to ground truth much faster. this result suggests\\nthat the use of response time can improve clustering or visualizations when only small number of answers are available.21\\n\\n\\x0ctable 3: comparison of similarity measures for one real-world data (with sampled students) set and simulated\\ndata sets with c knowledge components and l learners. the values provide the adjusted rand index (with\\n0.95 confidence interval) for a hierarchical clustering computed based on the specific similarity measure. the\\ntop result for every data set is highlighted.\\nczech 1 (c=7)\\n\\nl = 50, c = 5\\n\\nl = 100, c = 5\\n\\nl = 200, c = 5\\n\\nl = 100, c = 2\\n\\nl = 100, c = 10\\n\\n0.32 ± 0.02\\n0.31 ± 0.03\\n0.31 ± 0.03\\n0.15 ± 0.06\\n0.43 ± 0.01\\n0.32 ± 0.02\\n0.41 ± 0.03\\n0.32 ± 0.03\\n\\n0.26 ± 0.04\\n0.06 ± 0.03\\n0.19 ± 0.04\\n0.11 ± 0.02\\n0.45 ± 0.05\\n0.36 ± 0.05\\n0.39 ± 0.05\\n0.38 ± 0.05\\n\\n0.48 ± 0.05\\n0.15 ± 0.04\\n0.43 ± 0.05\\n0.18 ± 0.03\\n0.80 ± 0.06\\n0.65 ± 0.07\\n0.73 ± 0.06\\n0.72 ± 0.06\\n\\n0.84 ± 0.05\\n0.29 ± 0.08\\n0.77 ± 0.07\\n0.25 ± 0.05\\n0.98 ± 0.01\\n0.94 ± 0.04\\n0.96 ± 0.02\\n0.97 ± 0.02\\n\\n0.77 ± 0.12\\n0.32 ± 0.18\\n0.60 ± 0.15\\n0.12 ± 0.11\\n0.95 ± 0.03\\n0.89 ± 0.11\\n0.92 ± 0.03\\n0.94 ± 0.04\\n\\n0.34 ± 0.04\\n0.09 ± 0.02\\n0.31 ± 0.03\\n0.14 ± 0.02\\n0.67 ± 0.04\\n0.43 ± 0.03\\n0.55 ± 0.04\\n0.55 ± 0.05\\n\\ncorrelation with ground truth\\n\\npearson\\njaccard\\nyule\\nsokal\\npearson → euclid\\nyule → euclid\\npearson → pearson\\nyule → pearson\\n\\nwithout\\nresponse times\\nwith\\n\\nsample size\\n\\nfigure 7: projection of items practicing number\\nsense from matmat system. left: measure based\\nonly correctness. right: measure using response\\ntime. opacity corresponds to the number value of\\nthe item and color corresponds to the graphical representation of the task.\\n\\n4. discussion\\nour focus is the automatic computation of item similarities\\nbased on learners’ performance data. these similarities can\\nbe then used in further analysis of an item relations such as\\nan item clustering or a visualization. this outlines direction\\nfor future work in which methods using the item similarities\\nshould be studied in more detail. compared to alternative\\napproaches that have been proposed for the task (e.g., matrix factorizations, neural networks), the item similarity approach is rather straightforward, easy to realize, and it can\\nbe easily combined with other sources of information about\\nitems (text of items, expert opinion). for these reasons the\\nitem similarity approach should be used at least as a baseline\\nin proposals for more complex methods like deep knowledge\\ntracing [26].\\nthe most difficult step in this approach is the choice of a\\nsimilarity measure. once we make a specific choice, the realization of the approach is easy. our results provide some\\nguidelines for this choice. pearson, yule, and cohen measures lead to significantly better results than ochiai, sokal,\\nand jaccard measures. it is also beneficial to use the second\\nstep of item similarity (e.g., the euclidean distance over vec-\\n\\nfigure 8: the speed of convergence to ground truth\\nfor measures with and without response time on\\nmath garden addition data set.\\n\\ntors of item similarities). the exact choice of details does not\\nseem to make fundamental difference (e.g., pearson versus\\nyule in the first step, the euclidean distance versus pearson correlation in the second step). the pearson correlation coefficient is a good “default choice”, since it provides\\nquite robust results and is applicable in several settings and\\nsteps. it also has the pragmatic advantage of having fast,\\nreadily available implementation in nearly all computational\\nenvironments, whereas measures like yule may require additional implementation effort.\\nthe amount of data available is the critical factor for the success of automatic analysis of item relations. a key question\\nfor practical applications is thus: “do we have enough data\\nto use automated techniques?” in this work we used several\\nspecific methods for analysis of this question, but the issue\\nrequires more attention – not just for the item similarity\\napproach, but also for other methods proposed in previous\\nwork. for example previous work on deep knowledge tracing [26], which studies closely related issues, states only that\\ndeep neural networks require large data without providing\\nany specific quantification what ‘large’ means. the necesssary quantity of data is, of course, connected to the quality\\nof data – some data sources are more noisy than other, e.g.,\\nanswers from voluntary practice contain more noise than answers from high-stakes testing. an important direction for\\nfuture work is thus to compare model based and item simi-22\\n\\n\\x0clarity approaches while taking into account the ‘amount and\\nquality of data available’ issue.\\n\\n5.\\n\\nreferences\\n\\n[1] r. s. baker. stupid tutoring systems, intelligent\\nhumans. international journal of artificial\\nintelligence in education, 26(2):600–614, 2016.\\n[2] m. banko and e. brill. scaling to very very large\\ncorpora for natural language disambiguation. in proc.\\nof association for computational linguistics, pages\\n26–33, 2001.\\n[3] t. barnes. the q-matrix method: mining student\\nresponse data for knowledge. in educational data\\nmining workshop, 2005.\\n[4] w.-h. chen and d. thissen. local dependence\\nindexes for item pairs using item response theory.\\njournal of educational and behavioral statistics,\\n22(3):265–289, 1997.\\n[5] s.-s. choi, s.-h. cha, and c. c. tappert. a survey of\\nbinary similarity and distance measures. journal of\\nsystemics, cybernetics and informatics, 8(1):43–48,\\n2010.\\n[6] f. coomans, a. hofman, m. brinkhuis, h. l. van der\\nmaas, and g. maris. distinguishing fast and slow\\nprocesses in accuracy-response time data. plos one,\\n11(5):e0155149, 2016.\\n[7] m. deshpande and g. karypis. item-based top-n\\nrecommendation algorithms. acm transactions on\\ninformation systems (tois), 22(1):143–177, 2004.\\n[8] m. c. desmarais. mapping question items to skills\\nwith non-negative matrix factorization. acm\\nsigkdd explorations newsletter, 13(2):30–36, 2012.\\n[9] m. c. desmarais and r. s. baker. a review of recent\\nadvances in learner and skill modeling in intelligent\\nlearning environments. user modeling and\\nuser-adapted interaction, 22(1-2):9–38, 2012.\\n[10] m. c. desmarais, b. beheshti, and p. xu. the\\nrefinement of a q-matrix: assessing methods to\\nvalidate tasks to skills mapping. in proc. of\\neducational data mining, pages 308–311, 2014.\\n[11] c. desrosiers and g. karypis. a comprehensive survey\\nof neighborhood-based recommendation methods. in\\nrecommender systems handbook, pages 107–144.\\nspringer, 2011.\\n[12] h. finch. comparison of distance measures in cluster\\nanalysis with dichotomous data. journal of data\\nscience, 3(1):85–100, 2005.\\n[13] j. friedman, t. hastie, and r. tibshirani. the\\nelements of statistical learning, volume 1. springer\\nseries in statistics springer, berlin, 2001.\\n[14] d. a. jackson, k. m. somers, and h. h. harvey.\\nsimilarity coefficients: measures of co-occurrence and\\nassociation or simply measures of occurrence?\\namerican naturalist, pages 436–453, 1989.\\n[15] a. k. jain. data clustering: 50 years beyond k-means.\\npattern recognition letters, 31(8):651–666, 2010.\\n[16] s. klinkenberg, m. straatemeier, and h. van der\\nmaas. computer adaptive practice of maths ability\\nusing a new item response model for on the fly ability\\nand difficulty estimation. computers & education,\\n57(2):1813–1824, 2011.\\n\\n[17] k. r. koedinger, a. t. corbett, and c. perfetti. the\\nknowledge-learning-instruction framework: bridging\\nthe science-practice chasm to enhance robust student\\nlearning. cognitive science, 36(5):757–798, 2012.\\n[18] y. koren and r. bell. advances in collaborative\\nfiltering. recommender systems handbook, pages\\n145–186, 2011.\\n[19] a. s. lan, a. e. waters, c. studer, and r. g.\\nbaraniuk. sparse factor analysis for learning and\\ncontent analytics. journal of machine learning\\nresearch, 15(1):1959–2008, 2014.\\n[20] s.-f. m. liang and l.-w. tzeng. assessing suitability\\nof similarity coefficients in measuring human mental\\nmodels. in network of ergonomics societies\\nconference, pages 1–5. ieee, 2012.\\n[21] j. nižnan, r. pelánek, and j. řihák. using problem\\nsolving times and expert opinion to detect skills. in\\nproc. of educational data mining, pages 434–434,\\n2014.\\n[22] j. nižnan, r. pelánek, and j. řihák. student models\\nfor prior knowledge estimation. in proc. of\\neducational data mining, pages 109–116, 2015.\\n[23] m. o’connor and j. herlocker. clustering items for\\ncollaborative filtering. in proc. of the acm sigir\\nworkshop on recommender systems, volume 128. uc\\nberkeley, 1999.\\n[24] y.-j. park and a. tuzhilin. the long tail of\\nrecommender systems and how to leverage it. in proc.\\nof recommender systems, pages 11–18. acm, 2008.\\n[25] r. pelánek and j. řihák. properties and applications\\nof wrong answers in online educational systems. in\\nproc. of educational data mining, 2016.\\n[26] c. piech, j. bassen, j. huang, s. ganguli, m. sahami,\\nl. j. guibas, and j. sohl-dickstein. deep knowledge\\ntracing. in advances in neural information processing\\nsystems, pages 505–513, 2015.\\n[27] w. m. rand. objective criteria for the evaluation of\\nclustering methods. journal of the american\\nstatistical association, 66(336):846–850, 1971.\\n[28] j. rihák. use of time information in models behind\\nadaptive system for building fluency in mathematics.\\nin proc. of educational data mining, 2015.\\n[29] b. m. sarwar, g. karypis, j. konstan, and j. riedl.\\nrecommender systems for large-scale e-commerce:\\nscalable neighborhood formation using clustering. in\\nproc. of computer and information technology,\\nvolume 1, 2002.\\n[30] e. şenyürek and h. polat. effects of binary similarity\\nmeasures on top-n recommendations. anadolu\\nuniversity journal of science and technology – a\\napplied sciences and engineering, 14(1):55–65, 2013.\\n[31] n. x. vinh, j. epps, and j. bailey. information\\ntheoretic measures for clusterings comparison: is a\\ncorrection for chance necessary? in proc. of machine\\nlearning, pages 1073–1080. acm, 2009.\\n[32] m. j. warrens. on association coefficients for 2× 2\\ntables and properties that do not depend on the\\nmarginal distributions. psychometrika, 73(4):777–789,\\n2008.',\n",
       " '134\\n\\n\\x0ctowards reliable and valid measurement of individualized\\nstudent parameters\\nran liu\\n\\nkenneth r. koedinger\\n\\nhuman-computer interaction institute\\ncarnegie mellon university\\n\\nhuman-computer interaction institute\\ncarnegie mellon university\\n\\nranliu@cmu.edu\\n\\nkoedinger@cmu.edu\\n\\nabstract\\nresearch in educational data mining could benefit from greater\\nefforts to ensure that models yield reliable, valid, and interpretable\\nparameter estimates. these efforts have especially been lacking\\nfor individualized student-parameter models. we collected two\\ndatasets from a sizable student population with excellent “depth”\\n– that is, many observations for each skill for each student. we fit\\ntwo models, the individualized-slope additive factors model\\n(iafm) and individualized bayesian knowledge tracing (ibkt),\\nboth of which individualize for student ability and student\\nlearning rate. estimates of student ability were reliable and valid:\\nthey were consistent across both models and across both datasets,\\nand they significantly predicted out-of-tutor pretest data. in one of\\nthe datasets, estimates of student learning rate were reliable and\\nvalid: consistent across models and significantly predictive of\\npretest-posttest gains. this is the first demonstration that\\nstatistical models of data resulting from students’ use of learning\\ntechnology can produce reliable and valid estimates of individual\\nstudent learning rates. further, we sought to interpret and\\nunderstand what differentiates a student with a high estimated\\nlearning rate from a student with a low one. we found that\\nlearning rate is significantly related to estimates of student ability\\n(prior knowledge) and self-reported measures of diligence.\\nfinally, we suggest a variety of possible applications of models\\nwith reliable estimates of individualized student parameters,\\nincluding a more novel, straightforward way of identifying wheel\\nspinning.\\n\\nkeywords\\nexplanatory models, model interpretability, individualized\\nparameters, 3, additive factors model, individualized bayesian\\nknowledge tracing\\n\\n1. introduction\\nin educational data mining, statistical models are typically\\nevaluated based on fit to overall data and/or predictive accuracy\\non test data. while this is an important initial step in evaluating\\nthe contributions of advancements in statistical and cognitive\\nmodeling, research in the field could benefit from greater efforts\\nto ensure that models are reliable and valid. more reliable and\\nvalid models offer more explanatory power, contributing to the\\nadvancement of learning science. they also inspire greater\\nconfidence that deploying model advancements in future tutoring\\nsystems will genuine result in the hypothesized improvements to\\nlearning.\\nsome recent work has been done towards interpreting, validating,\\nand acting upon cognitive/skill modeling improvements [7, 8, 10,\\n11, 17]. educational data mining efforts oriented around\\npersonalizing student constructs [3, 12, 13, 14, 18], however, have\\nremained focused on improving predictive accuracy and/or\\ndemonstrating hypothetical time savings. little has been done to\\n\\nvalidate or understand the estimates that models with\\nindividualized or clustered student parameters produce.\\nanecdotally, efforts to do so have shown that these individualized\\nstudent parameter estimates, or discovered student clusters, are\\noften difficult to interpret.\\nit is especially critical to examine the reliability and validity of\\nparameter estimates for modeling advancements that dramatically\\nincrease the parameter count, as is generally true for\\nindividualized student-parameter models. more parameters create\\ngreater degrees of freedom and increase the likelihood that the\\nmodel may be underdetermined by the data.\\nwe focus on the question: to what degree can we trust a model’s\\nparameter estimates to correctly represent the constructs they are\\nsupposed to?\\nkey to expecting reliable, valid estimates of student-level\\nconstructs is not just big data in the “long” sense, but big data in\\nthe “deep” sense. oftentimes, the datasets used in secondary\\nanalyses in edm are large in terms of total number of students (or\\ntotal observations) but highly sparse in terms of observations per\\nskill, per student. these features make it difficult to get reliable\\nmeasurements of constructs at the individual student level,\\nparticularly constructs related to learning over time.\\nhere, we collected two datasets from a sizable student population\\n(196 students) with excellent “depth” – that is, many observations\\nfor each skill for each student. we then fit two models that\\nindividualize for student ability and student learning rate (the\\nindividualized-slope additive factors model [9] and\\nindividualized bayesian knowledge tracing [18]). we assess the\\nmodels’ fit to data and predictive accuracy. we also move beyond\\nthese metrics to examine the reliability of the models’ estimates of\\nstudent ability and student learning rate. additionally, we\\nexternally validate the parameter estimates against out-of-tutor\\nassessment data.\\nwe further interpret and understand the constructs by visualizing\\nrepresentative student learning trajectories, examining the\\nrelationship between estimated student ability and student\\nlearning rate, and the relationship between those constructs and\\nself-reported data on motivational attributes. finally, we propose\\nsome useful applications of reliable and valid individualized\\nstudent-parameter models, including a new way to detect wheel\\nspinning.\\n\\n2. prior work\\nprior work on individualizing student parameters has focused on\\nvariants of bayesian knowledge tracing (bkt) [3]. this work\\nincludes modeling the parameters separately for each individual\\nstudent instead of separately for each skill [3], individualizing the\\np(init) (“initial knowledge”) parameter for each student [13], and\\nindividualizing both p(init) and p(learn) (“learning rate”) to the135\\n\\n\\x0cbase bkt model [18]. these models have generally focused on\\nassessing predictive accuracy improvements relative to their\\nrespective non-individualized baseline models.\\n\\n“general education” classrooms designed to provide the\\nopportunity for individuals with disabilities and special needs to\\nlearn alongside their non-disabled peers.\\n\\nthere have also been some “time savings” analyses [12, 18] that\\nevaluate the hypothetical real world impact that individualizing\\nstatistical model fits could have. these analyses report the effect\\nof fitting individualized bkt models, compared to traditional\\nbkt, on the hypothetical number of under- and over- practice\\nattempts that would be predicted for each student. results\\ngenerally have indicated that many more practice opportunities\\nare needed for models to infer the same level of knowledge when\\nusing whole-population parameters rather than individual student\\nparameters. these analyses show that individualized models differ\\nin their hypothetical decision points if they were to be applied to\\ndrive mastery-based learning, but they do not in and of themselves\\ninterpret the individualized parameter estimates, nor do they\\nassess the reliability and validity of such estimates.\\n\\nstudents spent five consecutive days participating in each study\\nduring their regular geometry class periods. on the first and last\\ndays, they took a computerized pretest and posttest, respectively.\\nduring the middle three days, they worked within an intelligent\\ntutoring system [19] designed to give them practice on their\\ncurrent chapter’s content. this procedure applied to both studies,\\none of which covered the students’ chapter 3 content (parallel\\nlines cut by a transversal, angles & parallel lines, finding\\nslopes of lines, slope-intercept form, point-slope form) and the\\nother of which covered the students’ chapter 4 content\\n(classifying triangles, finding measures of triangle sides &\\nangles, triangle congruence properties). figure 1 shows an\\nexample problem interface from the intelligent tutoring system,\\nwhich was designed using cognitive tutor authoring tools [1].\\n\\nin a previous effort to better understand individualized student\\nlearning rate parameters [9], we examined predictive accuracy and\\nparameter reliability in an extension of the additive factors\\nmodel [2] applied to existing educational datasets. we did not\\nfind evidence that individualizing student rate parameters\\nconsistently improved predictive accuracy improvements, nor\\ncould we validate the parameter estimates on out-of-tutor\\nassessment data. however, the datasets we analyzed either\\ncontained a small number of students or were largely sparse in\\nobservations for student-skill pairs, with the exception of two\\ndatasets. these two datasets happened to be the ones on which the\\nindividualized-slope additive factors model did achieve higher\\npredictive accuracy. thus, we wondered if the sparsity of the\\ndatasets were the primary limitation, rather than the modeling\\nadvancement itself. this idea is corroborated by the fact that\\npooling students into “groups” rather than generating\\nindividualized estimates worked well on those datasets [9].\\nfor the present modeling work, we collected our own data in\\norder to ensure the data features that we believe are necessary for\\nreliable, valid, and potentially meaningful estimates of constructs\\nat the individual student level.\\n\\n3. methods\\nit is common in edm to do secondary analyses across multiple\\ndatasets. however, it can be difficult to find datasets that (1)\\ncontain a sizable number of students, (2) contain many\\nobservations for each skill for each student (i.e., are not sparse),\\n(3) contain students spanning a range of abilities in the domain\\ncovered by the tutor, and (4) contain data from out-of-tutor\\nassessment data that is well-mapped to the content in the tutor.\\nfor the present work, we wanted to use as close to an “ideal”\\ndataset as possible for estimating student parameters. we\\ncollected our own dataset with a sizable number of students (196),\\nmany observations (5-50, depending on the skill) for each skill for\\neach student. in addition, we ensured that a wide range of student\\nability levels was represented in our data to allow for the\\npossibility that models could capture this variability.\\n\\n3.1 data collection\\n196 students, spanning 10 classes taught by three different\\nteachers, enrolled in high school geometry participated in two\\nstudies conducted about a month apart. a range of student\\nabilities were included in the study. two of the 10 classes were\\n“honors” and three of the 10 classes were “inclusion”. honors\\nclassrooms are intended for students who have strong theoretical\\ninterests and abilities in mathematics. inclusion classrooms are\\n\\nfigure 1. example problem interface from the intelligent\\ntutoring system used for data collection.\\nwe also collected self-report survey data on motivational factors\\nfalling along three dimensions. these were competitiveness (e.g.,\\n“in this unit, i am striving to do well compared to other students”\\nand “in this unit, i am striving to avoid performing worse than\\nothers”), effort (e.g., “i am striving to understand the content of\\nthis unit as thoroughly as possible” and “i work hard to do well in\\nthis class even if i don\\'t like what we are doing”), and diligence\\n(e.g., “when class work is difficult, i give up or only study the\\neasy parts” [inverted scale] and “i am diligent”). self-report\\nmeasures were indicated on a likert scale from 1-7.\\na key reason we collected two datasets, covering two distinct\\nchapters of the curriculum, is that we were interested in\\ninvestigating the consistency of student-level parameter estimates\\nacross different content, time, and contexts. we discuss this\\nfurther, along with preliminary results, in section 4.4.1.\\n\\n3.2 statistical models\\n3.2.1 the individualized-slope additive factors\\nmodel (iafm)\\nthe additive factors model (afm) [2] is a logistic regression\\nmodel that extends item response theory by incorporating a\\ngrowth or learning term.\\nln\\n\\n!!\"\\n!-!!\"\\n\\n= θ! +!∈!\"# q !\" (β!\\n\\n+ γ! t!\" )\\n\\n(1)\\n\\n136\\n\\n\\x0cthis statistical model (equation 1) gives the probability 𝑝!\" that a\\nstudent i will get a problem step j correct based on the student’s\\nbaseline ability (𝜃! ), the baseline easiness (𝛽! ) of the required\\nknowledge components on that problem step (𝑄!\" ), and the\\nimprovement (𝛾! ) in each required knowledge component (kc)\\nwith each additional practice opportunity. this kc slope, or\\n“learning rate,” parameter is multiplied by the number of practice\\nopportunities (𝑇!\" ) the student already had on it. knowledge\\ncomponents (kcs) are the underlying facts, skills, and concepts\\nrequired to solve problems [6].\\n\\nliteral comparison between the predictive accuracies of the two\\nclasses of models due to differences in whether they use incoming\\ntest data towards their predictions on later test data (bkt/ibkt\\ndo, and afm/iafm do not).\\n\\nindividualized-slope afm (iafm) builds upon this baseline\\nmodel by adding a per-student learning rate parameter (𝛿! ). this\\nparameter represents the improvement (𝛿! ) by student i with every\\nadditional practice opportunity with the kcs required on problem\\nstep j.\\n\\ncounter to the majority of findings reported in [9], iafm\\nachieved higher predictive accuracy than afm in both datasets\\nhere. this further supports the idea that the “depth” of the dataset\\nis a critical factor in whether an individualized student-parameter\\nmodel can explain unique variance in the data.\\n\\nln\\n\\n!!\"\\n!!!!\"\\n\\n= 𝜃! +\\n\\n!∈!\"# 𝑄!\" (𝛽!\\n\\n+ 𝛾! 𝑇!\" + 𝛿! 𝑇!\" )\\n\\n(2)\\n\\nthe kc and student learning rate parameters are both multiplied\\nby the number of opportunities (𝑇!\" ) the student already had to\\npractice that kc.\\n\\nboth iafm and ibkt outperform their non-individualized\\ncounterparts by all metrics, with the exception of bkt having a\\nbetter bic value than ibkt for the chapter 4 dataset. this is not\\nsurprising, as bic is known to over-penalize for added\\nparameters. we recommend cross validation as a better indicator\\nthat ibkt is the true better fitting model in this case.\\n\\ntable 1. summary of model fit and predictive accuracy\\nmetrics comparing afm vs. iafm and bkt vs. ibkt. crossvalidation values are mean rmse values across 10 runs, with\\nstandard deviations included in parentheses.\\ndata\\nset\\n\\n3.2.2 individualized bayesian knowledge tracing\\n(ibkt)\\nbayesian knowledge tracing (bkt [3]) is an algorithm that\\nmodels student knowledge as a latent variable using a hidden\\nmarkov model. the goal of bkt is to infer, for each skill,\\nwhether a student has mastered it or not based on his/her sequence\\nof performance on items requiring that skill. it assumes a twostate learning model whereby each skill is either known or\\nunknown. there are four parameters that are estimated in a bkt\\nmodel: the initial probability of knowing a skill a priori – p(init),\\nthe probability of a skill transitioning from not known to known\\nstate after an opportunity to practice it – p(learn), the probability\\nof slipping when applying a known skill – p(slip), and the\\nprobability of correctly guessing without knowing the required\\nskill – p(guess). fitting bkt produces estimates for each of these\\nfour parameters for every skill in a given dataset. bkt models are\\nusually fit using the expectation maximization method (em),\\nconjugate gradient search, or discretized brute-force search.\\nindividualized bayesian knowledge tracing (ibkt [18]) builds\\nupon this baseline bkt model by individualizing the estimate of\\nthe probability of initially knowing a skill, p(init), and the\\ntransition probability, p(learn), for each student. to accomplish\\nthe student-level individualization of these parameters, each of\\nthem is split into skill- and student-based components that are\\nsummed and passed through a logistic transform to yield the final\\nparameter estimate. details on the decomposition of p(init) and\\np(learn) into skill- and student-based components are described\\nin [18].\\n\\n4. results\\n4.1 model fit & predictive accuracy\\nas a first pass evaluation of the two individualized models, we\\nassessed them using akaike information criterion (aic) and\\nbayesian information criterion (bic), which are standard metrics\\nfor model comparison, and 10 independent runs of split-halves\\ncross validation (cv). although 10-fold cross validation has been\\npopular in the field, [4] showed that it has a high type-i error due\\nto high overlap among training sets and recommended at least 5\\nreplications of 2-fold cv instead.\\nhere, the comparison of interest is each individualized model\\nagainst its non-individualized counterpart. we do not encourage a\\n\\nch. 3\\n\\nch. 4\\n\\nmodel\\n\\naic\\n\\nbic\\n\\nafm\\n\\n57229\\n\\n57283\\n\\ncv test rmse\\n(10-run average)\\n0.38440 (0.0039)\\n\\niafm\\n\\n55931\\n\\n56003\\n\\n0.37868 (0.0044)\\n\\nbkt\\n\\n66714\\n\\n67473\\n\\n0.4222 (0.0005)\\n\\nibkt\\n\\n56325\\n\\n60479\\n\\n0.3777 (0.0006)\\n\\nafm\\n\\n18059\\n\\n18106\\n\\n0.41037 (0.0048)\\n\\niafm\\n\\n17863\\n\\n17925\\n\\n0.40789 (0.0050)\\n\\nbkt\\n\\n19908\\n\\n20376\\n\\n0.44091 (0.0014)\\n\\nibkt\\n\\n18285\\n\\n21809\\n\\n0.40725 (0.0018)\\n\\n4.2 reliability of student parameters\\nnext, we examined the degree to which we can rely on these\\nparameters to reasonably estimate the constructs that they should\\nbe estimating. we believe that a strong relationship between the\\nparameter estimates of two statistical models with entirely\\ndifferent architectures is a high bar for testing reliability. that is,\\nif a student genuinely displayed evidence of high overall ability in\\na dataset (relative to his/her peers), then both iafm and ibkt\\nshould estimate that to be the case.\\nbecause of known and observed nonlinear relationships between\\nlogistic regression and bayesian knowledge tracing parameter\\nestimates, we measured correlation based on spearman’s\\ncoefficient (rs), which is based on rank order.\\nwe observed strong and statistically significant correlations\\nbetween iafm student intercept and ibkt student p(init)\\nparameter estimates (figure 2, top row). we also observed a\\nstrong and statistically significant correlation between iafm\\nstudent slope and ibkt student p(learn) parameter estimates for\\none of the two datasets (chapter 4). this correlation was much\\nmilder, though still significant, for the other dataset (chapter 3).\\nwe hypothesize that this difference between datasets may be due\\nto the presence of more difficult kcs in chapter 4. a dataset with\\nmore difficult items should provide more sensitive measures of\\nindividual differences in improvement, since it avoids ceiling\\neffects. indeed, this was the case: the mean kc easiness parameter\\nestimate (𝛽! ) for chapter 4 was 0.799 (which translates to a137\\n\\n\\x0cprobability of 0.69), compared to 1.253 for chapter 3 (which\\ntranslates to a probability of 0.78). when students are practicing\\nmany opportunities at ceiling (which was the case in particular for\\nchapter 3, based on exploratory analyses of the data), the\\nindividualized models will often assign them a lower “learning\\nrate” due to an essentially flat learning trajectory.\\n\\nthis has several interesting implications for educational\\napplications. first, it suggests that formative assessment via\\nmodeling of process data as learning unfolds is a reasonable\\nmethod of assessment.\\nit also suggests that detailed assessment data (e.g., from a pretest)\\ncould be used to reasonable effect to improve different students’\\n“on-line” estimates of students’ knowledge of kcs. for example,\\ncombining kc parameter estimates (derived from model-fitting to\\nprior domain-relevant data) with student intercept priors based on\\npretest assessment data would allow a model like afm to\\ngenerate individualized predictions of how much each student\\nneeds to practice to reach mastery.\\nin addition, these results suggest that individualized bkt models\\ncould use pretest assessment data to “set” reasonably valid\\nstudent-specific p(init) values before collecting any within-tutor\\ndata from those students.\\nin considering the degree to which these results may generalize, it\\nis important to note that the pretests in the present datasets were\\nspecifically designed to map closely to the practice problems in\\nthe intelligent tutor. pretests contained 1-2 questions for each kc\\nthat was practiced in the tutor, and the items were similar to those\\nencountered within the tutor.\\n\\nfigure 2. relationships between iafm student intercept and\\nibkt student p(init) parameter estimates (top row), and\\nbetween iafm student slope and ibkt student p(learn)\\nparameter estimates (bottom row), for the two datasets.\\n\\n4.3 validity of student parameters\\n\\nto assess the validity of student parameter estimates, we related\\nthem to out-of-tutor assessments of the relevant student\\nconstructs. in this case, we validated parameter estimates using\\npretest and posttest assessment data collected in the study.\\n\\n4.3.1 estimates of student ability\\nthe student intercept (𝜃! ) parameter of iafm and the student\\np(init) parameter of bkt are designed to estimate baseline\\nstudent ability, as least for the knowledge domain represented in\\nthe dataset. to validate the models’ estimates of this construct, we\\nexamined relationships between the model estimates and students’\\npretest scores, which are an out-of-tutor assessment of student\\ninitial ability for the skills covered by the tutor.\\nwe report standard pearson correlation coefficients here, since the\\nrelationships between pretest scores and the parameter estimates\\ndid not appear to be particularly nonlinear.\\nfigure 3 illustrates a summary of these relationships. both\\nmodels’ estimates of the student ability construct were strongly\\nand significantly correlated with pretest scores.\\nin addition, adding an individualized student slope improved the\\nvalidity of the model’s estimate of student ability (a parameter\\nthat’s modeled in both afm and iafm). we compared the\\ncorrelations between afm’s intercept estimates to pretest scores\\n(chapter 3: r = 0.62, p < 0.0001, chapter 4: r = 0.58, p < 0.0001)\\nto iafm’s intercept estimate / pretest score correlations (chapter\\n3: 0.74, p < 0.0001, chapter 4: r = 0.66, p < 0.0001).\\n\\nfigure 3. relationships between out-of-tutor pretest scores\\nand iafm/ibkt estimates of student ability based on withintutor data.\\n\\n4.3.2 estimates of student learning rate\\ngiven that the only external assessment data collected were a\\npretest and posttest, we sought to validate the construct of student\\nlearning rate (as estimated by the models) on pretest-posttest\\ngains. students were given roughly the same amount of time to\\nengage with the tutors, so those with accelerated learning rates\\nmight be expected to gain more knowledge in the time available.\\nthus, we examined the degree to which student learning rate\\nestimates predicted pretest-posttest gains while controlling for\\npretest scores. we controlled for pretest scores because they have\\nbeen shown to negatively predict learning gains due to assessment138\\n\\n\\x0cceiling effects. that is, students who start out performing well on\\nthe pretest have less “room for improvement”.\\n\\nmodel estimates of the student ability and student learning rate\\nconstructs across units?\\n\\nfor the chapter 3 dataset, iafm student slope (𝛿! ) estimates did\\nnot significantly predict learning gains. in a linear regression\\npredicting pretest-posttest gains, pretest scores were a significant\\npredictor (β=-0.189, p=0.005) and student slope estimates were\\nnot (β=0.396, p=0.144). ibkt student p(learn) estimates did not\\nsignificant predict learning gains. in a linear regression predicting\\npretest-posttest gains, pretest scores were a significant predictor\\n(β=-0.226, p=0.005) and student slope estimates were not\\n(β=0.062, p=0.218).\\n\\nfigure 4 summarizes this relationship. estimates of student ability\\nare fairly consistent, especially as estimated by iafm. it seems\\nsensible to interpret this as suggesting that overall student ability\\non chapter 3 content is strongly related to overall student ability\\non chapter 4 content, as we have shown estimates of student\\nability to be both reliable and valid.\\n\\nfor the chapter 4 dataset, iafm student slope (𝛿! ) estimates\\nsignificantly predict learning gains. in a linear regression\\npredicting pretest-posttest gains, pretest scores (β=-0.641,\\np<0.0001) and student slope estimates (β=0.576, p=0.007) were\\nboth significant predictors. ibkt student p(learn) estimates also\\nsignificantly predict learning gains. in a linear regression\\npredicting pretest-posttest gains, pretest scores (β=-0.645,\\np<0.0001) and p(learn) estimates (β=0.133, p=0.004) were both\\nsignificant predictors.\\nfor one of the two units (chapter 4), we observed that student\\nlearning rate estimates were validated on external assessments of\\nlearning gain. interestingly, this is the same unit for which we\\nobserved a strong cross-model reliability in student learning rate\\nestimates. thus, we have converging evidence that student\\nlearning rates estimates for the chapter 4 dataset are both reliable\\nand valid.\\n\\nestimates of student learning rate are less consistent. this may\\neither be due to the fact that chapter 3 estimates of student\\nlearning rate were neither very reliable nor very valid.\\nalternatively, the differences in student learning rate estimates\\nacross the two chapters may also be due to the fact that students\\ngenuinely learn different material at different rates. unfortunately,\\nwe cannot resolve this question with the present data. we are\\ncurrently collecting more datasets from this same group of\\nstudents. if we obtain more reliable and valid student learning rate\\nestimates in future data from this group of students, we can more\\nconfidently address this question in future research.\\n\\n4.4.2 understanding student learning rate estimates\\ngiven that we established the reliability and validity of iafm and\\nibkt’s parameter estimates for the chapter 4 dataset were\\nreasonably reliable and valid, we sought to dig deeper into the\\nexplanatory power of these estimates. to this end, we conducted\\nexploratory analyses on the chapter 4 data to (1) visualize the\\nlearning trajectories of students with the highest vs. lowest\\nestimated learning rates, (2) understand the relationships between\\nestimated learning rates and prior-knowledge and motivational\\nfactors, and (3) understand the degree of variability in estimated\\nlearning rate across students.\\n\\nfirst attempt success\\n0.4\\n0.6\\n0.8\\n\\nfirst attempt success\\n0.4\\n0.6\\n0.8\\n\\n1.0\\n\\nibkt student p(learn) estimate\\n\\n1.0\\n\\niafm student slope estimate\\n\\n4\\n6\\n8\\n# practice opportunities\\n\\n10\\n\\n0\\n\\n4.4 towards understanding & using student\\nparameter estimates\\n4.4.1 consistency of individual student constructs\\nacross datasets\\na core motivating question for collecting two datasets on the\\nsame group of students was: how consistent are iafm and ibkt\\n\\n4\\n6\\n8\\n# practice opportunities\\n\\n10\\n\\n6\\n4\\n\\n5\\n\\ntop 25% ibkt learning rates\\nmiddle 50% ibkt learning rates\\nbottom 25% ibkt learning rates\\n\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\nlikert scale\\n\\n4\\n3\\n1\\n\\n2\\n\\nlikert scale\\n\\n5\\n\\n6\\n\\ntop 25% iafm learning rates\\nmiddle 50% iafm learning rates\\nbottom 25% iafm learning rates\\n\\n0\\n\\nfigure 4. relationships between student parameter estimates\\nacross the two datasets (same student population).\\n\\n2\\n\\n7\\n\\n2\\n\\n7\\n\\n0\\n\\n0.2\\n\\n0.2\\n\\ntop 25%\\nmiddle 50%\\nbottom 25%\\n\\ncompetitiveness\\n\\neffort\\n\\ndiligence\\n\\ncompetitiveness\\n\\neffort\\n\\ndiligence\\n\\nfigure 5. top row: early-opportunity learning trajectories of\\nstudents, grouped based on iafm (left) and ibkt (right)\\nestimated learning rates. solid lines are actual data; dotted\\nlines are each respective model’s predicted performance.\\nbottom row: mean self-report likert scale ratings of\\nquestions measuring dimensions of competitiveness, effort,\\nand diligence. grouped based on iafm (left) or ibkt (right)\\nestimated learning rates. error bars show standard errors on\\nthe means.139\\n\\n\\x0cfigure 5 (top row) shows the aggregate learning trajectories for\\nstudents split based either on their iafm student slope estimates\\n(top left) or their ibkt student p(learn) estimates (top right). the\\ntop 25% of student parameter estimates are plotted in blue, the\\nmiddle 50% (between 1st and 3rd quartiles) are plotted in red, and\\nthe lower 25% are plotted in black. dotted lines represent each\\nrespective model’s predicted earning trajectories.\\none striking pattern, especially in the iafm learning trajectories\\n(top left), is the apparent relationship between average success on\\ninitial practice opportunities (i.e., prior knowledge) and estimated\\nlearning rate through the remaining opportunities. this\\nobservation is corroborated by a strong and significant correlation\\nbetween iafm student intercepts and iafm student slopes\\n(r=0.78, p<0.0001). one might interpret this to suggest that\\nstudents who enter into the tutor with greater prior knowledge will\\nbe poised to gain more from the tutor (i.e., “the rich get richer”).\\nalternatively, students may have higher overall knowledge\\nbecause they are fast learners. there may also be individual traitbased variables that positively drive both learning rate and overall\\nachievement.\\nto explore the relationships between measures of traits relevant to\\nlearning, we analyzed self-report survey data grouped by three\\nfactors (as described in section 3.1): competitiveness, effort, and\\ndiligence. the relationship between these measures and the high,\\nmedium, and low learning rate estimates from iafm and ibkt\\nare shown in figure 5 (bottom row). there appears to be a\\nrelationship between the means of each self-report measure and\\nthe general range that the learning rate estimate falls in.\\nwe analyzed the continuous relationship between students’ mean\\nself-report rating along each dimension and their iafm learning\\nrate estimates. in a linear regression predicting iafm student\\nslopes, competitiveness and effort were not significant predictors\\nbut diligence (β=0.016, p=0.007) was. in a similar linear\\nregression predicting iafm student intercepts, again diligence\\nwas the only significant predictor (β=0.02, p=0.04). thus, among\\nself-reported measures, the strongest dimension predicting both\\nstudent ability/prior knowledge and student learning rate was the\\ndiligence measure. future work using causal modeling is\\nwarranted to discover the true nature of causality among these\\nstudent-level constructs.\\nfinally, we investigated the degree of variability in estimated\\nlearning rate across students. the first quantile of student learning\\nrates from iafm is 0.03 logits and the third quantile of rates from\\niafm is 0.08 logits. these can be conceptualized as canonical\\n“slow” and “fast” learners. if we were to assume starting at\\naround 70% performance (which comes from the model’s global\\nintercept estimate), it would take the “slow” (0.03 logits) student\\napproximately 25 opportunities to reach mastery (defined as 85%,\\nthe performance equivalent of a p(know)=0.95, factoring in the\\nguess and slip probabilities we used in the actual tutor). it would\\ntake the “fast” (0.08 logits) student approximately 11\\nopportunities to reach the same place.\\n\\n4.4.3 identifying wheel spinners\\nthe current definition of “wheel spinning” put forth in the\\neducational data mining community is the “phenomenon in\\nwhich a student has spent a considerable amount of time\\npracticing a skill, yet displays little or no progress towards\\nmastery” [5]. there has been some controversy around the ideal\\nway to measure mastery (e.g., 3 corrects in a row vs. reaching a\\ncertain p(know) in knowledge tracing). furthermore, some\\nstudents may be classified as wheel spinners based on not\\nmastering in a certain number of opportunities but they may still\\nbe making progress.\\n\\nwe propose that reliable and validated estimates of individual\\nstudent learning rate parameters, combined with kc learning rate\\nparameters, could be used to estimate wheel spinning student/kc\\npairs in way that is agnostic to mastery status. specifically, if the\\ncombined student and kc learning rate parameters in iafm\\npredict no improvement or negative improvement across\\nadditional practice opportunities, and aren’t already at a high level\\nof performance on their first opportunity (here we considered this\\nto be 80% or above), we could consider the student to be wheel\\nspinning on the kc. this method of estimating wheel spinning\\nwould be particularly useful for datasets with sparse data on some\\nstudent-kc pairs, as it is not performance-dependent after the\\nmodel has been fit to the full dataset.\\nbased on this operationalized definition, we found that\\napproximately 15% of student-kc pairs in the chapter 4 dataset\\nare estimated to be wheel spinning. that is, those students are not\\nmaking progress on those kcs. this is a substantially lower\\nestimate than the 25% reported by a recent wheel spinning\\ndetector in [5]. an interesting route for future work would be to\\ndo a direct comparison of the wheel spinning detector presented in\\n[5] and our proposed student/kc learning rate identifier within the\\nsame dataset. this would allow for testing the possibility that\\nsome students who are still making progress, albeit extremely\\nslowly, may be prematurely labeled as “wheel spinners” by [5].\\n\\n5. summary & limitations\\nprevious efforts towards more explanatory, interpretable, and\\nactionable modeling advancements in the realm of\\nskill/knowledge component model discovery have been promising\\nin their potential and demonstrated impact on learning science and\\neducation. the present paper represents a novel effort to bring\\nthese deeper modeling approaches, focused on ensuring\\nexplanatory power, to the realm of individualized studentparameter models.\\ntowards improving the reliability and validity of individualized\\nstudent estimates, we collected two datasets from the same student\\npopulation. both datasets were “deep” along the dimension of\\nstudent-kc observations. we fit iafm and ibkt to both datasets\\nand showed that the models outranked their non-individualized\\ncounterparts in terms of fit to data and predictive accuracy.\\nimportantly, we moved beyond these metrics to show that\\nestimates of student ability were highly reliable (iafm and ibkt\\nyielded strongly correlated estimates) and valid (estimates\\nsignificantly predicted pretest data).\\nthis demonstration of confidence in the student ability estimates\\nfrom ibkt, but even more so iafm, has promising implications\\nfor the possibility of individualizing the student models that\\ndetermine mastery in intelligent tutoring systems at least in terms\\nof overall student ability/knowledge. our results also suggest that\\nit would be reasonable to fix such student ability parameters, or\\nset priors on them, based on either well-mapped pretest\\nassessment data or prior (deep) data from those students’ learning.\\nwe also showed that estimates of student learning rate per\\npractice opportunity were reliable and valid in one of the two\\ndatasets (chapter 4). this is the first evidence, to our knowledge,\\nof obtaining both reliable and valid student learning rates through\\na statistical model with individualized student parameters. we\\nbelieve that this success is largely related to the amount and\\nquality of per-student data we collected.\\nwith the confidence of having reliable and valid parameter\\nestimates, we then proceeded to further investigate potential\\nexplanations for differences in student learning rates within the140\\n\\n\\x0cchapter 4 dataset. we found a strong and significant relationship\\nbetween student ability and improvement rate as well as an\\nadditional effect of diligence, based on self-report measures.\\nfurther research is warranted to distill the causal relationships\\nbetween these constructs.\\nknowing that a model’s estimates of individualized student\\nparameters not only fit data well, but are reliable and valid,\\nprovides greater confidence for applying the model to (1) interpret\\nthe parameter estimates to understand characteristics of students,\\nand (2) use the model to individualize the trajectory of mastery\\nestimation for future students.\\neven though both ibkt and iafm outperformed their nonindividualized counterparts in predicting performance in the\\nchapter 3 dataset, we did not find strong evidence of reliability\\nand validity of the student-specific parameter estimates. thus, we\\ndid not rely on that dataset to help us understand individual\\ndifferences in learning rates. for the same reason, we could not\\nconfidently attribute the differences, in estimated student learning\\nrates across the datasets, to true differences in students’ learning\\nrates for the two chapters’ material.\\nalthough considering reliability and validity of models’ parameter\\nestimates sets a higher bar than predictive accuracy for evaluating\\nmodeling advances, we believe those to be important\\ncharacteristics of a model that is to be explanatory, interpretable,\\nand/or actionable. here, we have demonstrated that with a\\nsufficiently good dataset, iafm and ibkt are individualized\\nstudent models that can produce reliable and valid parameter\\nestimates.\\nsince our present work was limited to two datasets on one\\npopulation of students, it is unclear the degree to which our\\nmodeling results will generalize, especially given that at least\\niafm does not produce reliable, valid parameter estimates on\\nmore sparse datasets [9]. in addition, these results are limited to\\ntwo specific statistical models produce individualized estimates\\nstudent-level parameters, with a particular focus on individual\\ndifferences in learning rate. there are other classes of models that\\ncould be extended to estimate differences in learning rate: for\\nexample, producing individualized estimates of the differential\\neffects of success versus failure [15]. this would be an interesting\\nfocus for future work on this topic.\\nnevertheless, we have laid a foundation of methodology by which\\nreliability and validity of parameter estimates, whether student- or\\nkc-level, can be assessed. we have also demonstrated ways of\\nusing the reliable and valid student parameter estimates from\\niafm and ibkt to yield interesting insights about student\\nlearning.\\n\\n6. acknowledgments\\nwe thank the institute of education sciences for support to rl\\n(training grant #r305b110003) and the national science\\nfoundation for support to carnegie mellon university’s learnlab\\n(#sbe-0836012).\\n\\n7. references\\n[1] aleven, v., sewall, j., mclaren, b.m., and koedinger, k.r.\\n(2006). rapid authoring of intelligent tutors for real-world\\nand experimental use. in proceedings of the 6th icalt.\\nieee, los alamitos, ca, pp. 847-851.\\n[2] cen, h., koedinger, k.r., & junker, b. (2006). learning\\nfactors analysis: a general method for cognitive model\\nevaluation and improvement. intelligent tutoring systems,\\n164-175.\\n\\n[3] corbett, a.t., & anderson, j.r. (1995). knowledge tracing:\\nmodeling the acquisition of procedural knowledge. user\\nmodeling and user-adapted interaction, 4, 253-278.\\n[4] dietterich, t. g. (1998). approximate statistical tests for\\ncomparing supervised classification learning algorithms.\\nneural computation, 10(7), 1895–1923.\\n[5] gong, y. & beck, j. (2015). towards detecting wheelspinning: future failure in mastery learning. in\\nproceedings of learning at scale ’15.\\n[6] koedinger, k.r., corbett, a.c., & perfetti, c. (2012). the\\nknowledge-learning-instruction (kli) framework: bridging\\nthe science-practice chasm to enhance robust student\\nlearning. cognitive science, 36(5), 757-798.\\n[7] koedinger, k.r., mclaughlin, e.a., & stamper, j.c. (2012).\\nautomated student model improvement. 5th international\\nconference on edm.\\n[8] koedinger, k. r., stamper, j. c., mclaughlin, e. a., &\\nnixon, t. (2013). using data-driven discovery of better\\ncognitive models to improve student learning. in h. c. lane,\\nk. yacef, j. mostow, & p. pavlik (eds.), proceedings of the\\n16th international conference on artificial intelligence in\\neducation (aied ʼ13), 9–13 july 2013, memphis, tn, usa\\n(pp. 421–430). springer.\\n[9] liu, r., & koedinger, k. r. (2015). variations in learning\\nrate: student classification based on systematic residual error\\npatterns across practice opportunities. in o. c. santos, j. g.\\nboticario, c. romero, m. pechenizkiy, a. merceron, p.\\nmitros, j. m. luna, c. mihaescu, p. moreno, a. hershkovitz,\\ns. ventura, & m. desmarais (eds.), proceedings of the 8th\\ninternational conference on education data mining\\n(edm2015), 26–29 june 2015, madrid, spain (pp. 420–423).\\ninternational educational data mining society.\\n[10] liu, r., & koedinger, k. r. (under review). closing the\\nloop: automated data-driven skill model discoveries lead to\\nimproved instruction and learning gains.\\n[11] liu, r., koedinger, k. r., & mclaughlin, e. a. (2014).\\ninterpreting model discovery and testing generalization to a\\nnew dataset. in j. stamper, z. pardos, m. mavrikis, & b. m.\\nmclaren (eds.), proceedings of the 7th international\\nconference on educational data mining (edm2014), 4–7\\njuly, london, uk (pp. 107–113). international educational\\ndata mining society.\\n[12] lee, j.i., & brunskill, e. (2012). the impact on\\nindividualizing student models on necessary practice\\nopportunities. 5th international conference on edm.\\n[13] pardos, z.a., & heffernan, n.t. (2010). modeling\\nindividualization in a bayesian networks implementation of\\nknowledge tracing. user modeling, adaptation, and\\npersonalization, 255-266.\\n[14] pardos, z. a., trivedi, s., heffernan, n. t., & sárközy, g.\\nn. (2012). clustered knowledge tracing. in s. a. cerri, w. j.\\nclancey, g. papadourakis, k.-k. panourgia (eds.),\\nproceedings of the 11th international conference on\\nintelligent tutoring systems (its 2012), 14–18 june 2012,\\nchania, greece (pp. 405–410). springer.\\n[15] pavlik, p.i., cen, h., & koedinger, k.r. (2009).\\nperformance factors analysis–a new alternative to knowledge\\ntracing. aied, 531–538.\\n[16] shmueli, g. (2010). to explain or to predict? statistical\\nscience, 25(3), 289–310. doi:10.1214/10-sts330\\n[17] stamper, j., & koedinger, k. r. (2011). human-machine\\nstudent model discovery and improvement using data.\\nproceedings of the 15th international conference on141\\n\\n\\x0cartificial intelligence in education (aied ʼ11), 28 june–2\\njuly, auckland, new zealand (pp. 353–360). springer.\\n[18] yudelson, m.v., koedinger, k.r., & gordon, g.j. (2013).\\nindividualized bayesian knowledge tracing models. aied,\\n171-180.\\n\\n[19] vanlehn, k. (2006). the behavior of tutoring systems.\\ninternational journal of artificial intelligence in education,\\n16, 227–265.',\n",
       " '79\\n\\n\\x0cpredicting short- and long-term vocabulary learning via\\nsemantic features of partial word knowledge\\nsungjin nam\\n\\nschool of information\\nuniversity of michigan\\nann arbor, mi 48109\\n\\nsjnam@umich.edu\\n\\ngwen frishkoff\\n\\nkevyn collins-thompson\\n\\ngfrishkoff@gmail.com\\n\\nkevynct@umich.edu\\n\\ndepartment of psychology\\nuniversity of oregon\\neugene, or 97403\\n\\nabstract\\n\\nwe show how the novel use of a semantic representation\\nbased on osgood’s semantic differential scales can lead to\\neffective features in predicting short- and long-term learning\\nin students using a vocabulary learning system. previous\\nstudies in students’ intermediate knowledge states during\\nvocabulary acquisition did not provide much information\\non which semantic knowledge students gained during word\\nlearning practice. moreover, these studies relied on human\\nratings to evaluate the students’ responses. to solve this\\nproblem, we propose a semantic representation for words\\nbased on osgood’s semantic decomposition of vocabulary\\n[16]. to demonstrate our method can effectively represent\\nstudents’ knowledge in vocabulary acquisition, we build\\nmodels for predicting the student’s short-term vocabulary\\nacquisition and long-term retention. we compare the\\neffectiveness of our osgood-based semantic representation to\\nthat provided by word2vec neural word embedding [13], and\\nfind that prediction models using features based on osgood\\nscale-based scores (osg) perform better than the baseline\\nand are comparable in accuracy to those using word2vec\\nscore-based models (w2v). by using more interpretable\\nosgood-based scales, our study results can help with better\\nunderstanding of students’ ongoing learning states and\\ndesigning personalized learning systems that can address an\\nindividual’s weak points in vocabulary acquisition.\\n\\nkeywords\\n\\nvocabulary learning, semantic similarity, prediction model,\\nintelligent tutoring system\\n\\n1. introduction\\n\\nstudies of word learning have shown that knowledge of\\nindividual words is typically not all-or-nothing. rather,\\npeople acquire varying degrees of knowledge of many words\\nincrementally over time, by exposure to them in context [9].\\nthis is especially true for so-called “academic” words that are\\nless common and more abstract — e.g., pontificate, probity,\\nor assiduous [7]. binary representations and measures model\\nword knowledge simply as correct or incorrect on a particular\\n\\nschool of information\\nuniversity of michigan\\nann arbor, mi 48109\\n\\nitem (word), but in reality, a student’s knowledge level may\\nreside between these two extremes. thus, previous studies of\\nvocabulary acquisition have suggested that students’ partial\\nknowledge be modeled using a representation that adding an\\nadditional label corresponding to an intermediate knowledge\\nstate [6] or further, in terms of continuous metrics for\\nsemantic similarity [3].\\nin addition, there are multiple dimensions to a word’s\\nmeaning [16]. measuring a student’s partial knowledge on\\na single scale may only provide abstract information about\\nthe student’s general answer quality and not give enough\\ninformation to specify which dimensions of word knowledge\\na student already has learned or needs to improve. in order\\nto achieve detailed understanding of a student’s learning\\nstate, online learning systems should be able to capture\\na student’s “learning trajectory” that tracks their partial\\nknowledge on a particular item over time, over multiple\\ndimensions of meaning in a multidimensional semantic\\nrepresentation.\\nhence, multidimensional representations of word knowledge\\ncan be an important element for building an effective\\nintelligent tutoring system (its) for reading and language.\\nmaintaining a fine-grained semantic representation of a\\nstudent’s degree of word knowledge can be helpful for\\nthe its to design more engaging instructional content,\\nmore helpful personalized feedback, and more sensitive\\nassessments [17, 19]. selecting semantic representations\\nto model, understand, and predict learning outcomes is\\nimportant to designing a more effective and efficient its.\\nin this paper, we explore the use of multidimensional\\nsemantic word representations for modeling and predicting\\nshort- and long-term learning outcomes in a vocabulary\\ntutoring system.\\nour approach derives predictive\\nfeatures using a novel application of existing methods in\\ncognitive psychology combined with methods from natural\\nlanguage processing (nlp). first, we introduce a new\\nmultidimensional representation of a word based on the\\nosgood semantic differential [16], an empirically based,\\ncognitive framework that uses a small number of scales\\nto represent latent components of word meaning. we\\ncompare the effectiveness of model features based on this\\nosgood-based representation to features based on a different\\nrepresentation, the widely-used word2vec word embedding\\n[13]. second, we evaluate our prediction models using\\ndata from a meaning-generation task that was conducted\\nduring a computer-based intervention. our study results\\ndemonstrate how similarity-based metrics based on rich80\\n\\n\\x0csemantic representation can be used to automatically\\nevaluate specific components of word knowledge, track\\nchanges in the student’s knowledge toward the correct\\nmeaning, and compute a rich set of features for use in\\npredicting short- and long-term learning outcomes. our\\nmethods could support advances in real-time, adaptive\\nsupport for word semantic learning, resulting in more\\neffective personalized learning systems.\\n\\nsemantic representation & the osgood framework.\\nto quantify the semantic characteristics of a student’s\\nintermediate knowledge of vocabulary, this paper uses a\\n“spatial analogue” for capturing semantic characteristics of\\nwords. in [16], osgood investigated how the meaning of\\na word can be represented by a series of general semantic\\nscales. by using these scales, osgood suggested that the\\nmeanings of any word can be projected and explored in a\\ncontinuous semantic space.\\n\\n2.\\n\\nosgood asked human raters to evaluate a set of words using a\\nlarge number of scales (e.g., tall-short, fat-thin, heavy-light)\\nand captured the semantic representation of a word [16].\\nrespondents gave likert ratings, which indicated whether\\nthey thought that a word meaning was closer to one extreme\\n(-3) or the other (+3), or basically irrelevant (0). a principal\\ncomponents analysis (pca) was used to represent the latent\\nsemantic features that can explain the patterns of response\\nto individual words within this task.\\n\\nrelated work\\n\\nthe present study is informed by three areas of research:\\n(1) studies of partial word knowledge; (2) the osgood\\nframework for multiple dimensions of word meaning, and (3)\\ncomputational methods for estimating semantic similarity.\\npartial word knowledge. the concept of partial word\\nknowledge has interested vocabulary researchers for several\\ndecades, particularly in the learning and instruction of “tier\\n2” words [20]. tier 2 words are low-frequency and typically\\nhave complex (multiple, nuanced) meanings. by nature,\\nthey are rarely learned through “one-shot” learning or direct\\ndefinition. instead, they are learned partially and gaps are\\nfilled in over time.\\nwords in this intermediate state, neither novel nor fully\\nknown, are sometimes called “frontier words” [5]. durso\\nand shore operationalized the frontier word as a word the\\nstudent had seen previously but was not actively using it [6].\\nbased on this definition, the student may have had implicit\\nmemory of frontier words, such as general information like\\nwhether the word indicates a good or bad situation or refers\\na person or an action. they discovered that students are\\nmore familiar with frontier words than other types of words\\nin terms of their sounds and orthographic characteristics [6].\\nthis previous work suggested that the concept of frontier\\nwords can be used to represent a student’s partial knowledge\\nstates in a vocabulary acquisition task [5, 6].\\nin some studies, partial word knowledge has been\\nrepresented using simple, categorical labels, e.g., multiplechoice tests that include “partially correct” response options,\\nas well as a single “best” (correct) response. in other studies,\\nthe student is presented with a word and is asked to say\\nwhat it means [1]. the definition is given partial credit\\nif it reflects knowledge that is partial or incomplete. for\\nexample, a student may recognize that the word probity\\nhas a positive connotation, even if she cannot give a\\ncomplete definition. however, single categorical or scorebased indicators may not explain which specific aspects of\\nvocabulary knowledge the student is missing. moreover,\\nthese studies relied on human ratings to evaluate students’\\nresponses for unknown words [6]. although widely used\\nin psychometric and psycholinguistic studies [4, 16], hiring\\nhuman raters is expensive and may not be done in real time\\nduring students’ interaction with the tutoring system.\\nto address these problems, we propose a data-driven method\\nthat can automatically extract semantic characteristics of\\na word based on a set of relatively simple, interpretable\\nscales. the method benefits from existing findings in\\ncognitive psychology and natural language processing. in\\nthe following sections, we illustrate more details of related\\nfindings and how they can be used in an intelligent tutoring\\nsystem setting.\\n\\nin our study, we suggest a method that can automatically\\nextract similar semantic information that can project a word\\ninto a multidimensional semantic space. by using semantic\\nscales selected from [16], we verify if such representation of\\nsemantic attributes of words is useful for predicting students’\\nshort- and long-term learning.\\nsemantic similarity measures. studies in nlp have\\nsuggested methods to automatically evaluate the semantic\\nassociation between two words. for example, markov\\nestimation of semantic association (mesa) [3, 9] can\\nestimate the similarity between words from a random walk\\nmodel over a synonym network such as wordnet [14]. other\\nmethods like latent semantic analysis (lsa) are based on\\nco-occurrence of the word in a document corpus. in lsa,\\nsemantic similarity between words is determined by using\\na cosine similarity measure, derived from a sparse matrix\\nconstructed from unique words and paragraphs containing\\nthe words [10].\\nfor this paper, we use word2vec [13], a widely used word\\nembedding method, to calculate the semantic similarity\\nbetween words. word2vec’s technique [11] transforms the\\nsemantic context, such as proximity between words, into a\\nnumeric vector space. in this way, linguistic regularities\\nand patterns are encoded into linear translations. for\\nexample, using outputs from word2vec, relationships\\nbetween words can be estimated by simple operations on\\ntheir corresponding vectors, e.g., madrid - spain + france\\n= paris, or germany + capital = berlin [13].\\nmeasures from these computational semantic similarity tools\\nare powerful because they can provide an automated method\\nfor evaluation of partial word knowledge. however, they\\ntypically produce a single measure (e.g., cosine similarity or\\neuclidean distance), representing semantic similarity as a\\none-dimensional construct. with such a measure, it is not\\npossible to determine represent partial semantic knowledge\\nand changes in knowledge of latent semantic features as\\nword knowledge progresses from unknown to frontier to\\nfully known. in following sections, we describe how we\\naddress this problem, using novel methods to to estimate\\nthe contribution of osgood semantic features to individual\\nword meanings.81\\n\\n\\x0c2.1\\n\\noverview of the study\\n\\nbased on findings from existing studies, this study will\\nsuggest an automatized method for evaluating students’\\npartial knowledge of vocabulary that can be used to predict\\nstudents’ short-term vocabulary acquisition and long-term\\nretention. to investigate this problem, we will answer the\\nfollowing research questions with this paper.\\nthe first research question (rq1): can semantic similarity\\nscores from word2vec be used to predict students’ shortterm learning and long-term retention? previous studies in\\nvocabulary tutoring systems tend to focus on how different\\nexperimental conditions, such as different spacing between\\nquestion items [18], difficulty levels [17], and systematic\\nfeedback [7], affect students’ short-term learning. this study\\nwill answer how computationally estimated trial-by-trial\\nscores in a vocabulary tutoring system can be used to predict\\nstudents’ short-term learning and long-term retention.\\nrq2: compared to using regular word2vec scores, how does\\nthe model using osgood’s semantic scales [16] as features\\nperform for immediate and delayed learning prediction\\ntasks? as described in the previous section, the initial\\noutcome from word2vec returns hundreds of semantic\\ndimensions to represent the semantic characteristics of\\na word. summary statistics for comparing such highdimensional vectors, such as cosine similarity or euclidean\\ndistance, only provide the overall similarity between words.\\nif measures from osgood scales work in a similar level\\nto models using regular word2vec scores for predicting\\nstudents’ learning outcomes, we can argue that it can\\nbe an effective method for representing students’ partial\\nknowledge of vocabulary.\\n\\n3. method\\n3.1 word learning study\\n\\nthis study used a vocabulary tutoring system called\\ndynamic support of contextual vocabulary acquisition\\nfor reading (dscovar) [8]). dscovar aims to support\\nefficient and effective learning vocabulary in context. all\\nparticipants accessed dscovar in a classroom-setting\\nenvironment by using chromebook devices or the school’s\\ncomputer lab in the presence of other students.\\n\\n3.1.1\\n\\nstudy participants\\n\\nparticipants included 280 middle school students (6th to\\n8th grade) from multiple schools, including children from\\ndiverse socio-economic and educational backgrounds. table\\n1 provides a summary of student demographics, including\\nlocation (p1 or p2), age and grade level, sex. location p1 is\\na laboratory school affiliated with a large urban university in\\nthe northeastern united states. students from location p1\\nwere generally of high socio-economic status (e.g., children\\nof university faculty and staff). location p2 includes three\\npublic middle schools in a southern metropolitan area of the\\nunited states. all students from location p2 qualified for\\nfree or reduced lunch. the study included a broad range of\\nstudents so that the results of this analysis were more likely\\nto generalize to future samples.\\n\\n3.1.2\\n\\nstudy materials\\n\\ndscovar presented students with 60 sat-level english\\nwords (also known as tier 2 words). these “target words,”\\nlesser-known words that the students are going to learn,\\n\\ntable 1: the number of participants by grade and\\ngender\\ngroup\\np1\\np2\\n\\n6th grade\\ngirl\\nboy\\n16\\n28\\n53\\n51\\n\\n7th grade\\ngirl\\nboy\\n19\\n23\\n12\\n6\\n\\n8th grade\\ngirl\\nboy\\n18\\n13\\n21\\n20\\n\\nwere balanced between different parts of speech, including 20\\nadjectives, 20 nouns, and 20 verbs. based on previous works,\\nwe expected that students would have varying degrees of\\nfamiliarity with the words at pre-test, but that most words\\nwould be either completely novel (“unknown”) or somewhat\\nfamiliar (“partially known”) [8, 15]. this selection of\\nmaterials ensured that there would be variability in word\\nknowledge across students for each word and across words\\nfor each student.\\nin dscovar, students learned how to infer the meaning\\nof an unknown word in a sentence by using surrounding\\ncontextual information. having more information in a\\nsentence (i.e., a sentence with a high degree of contextual\\nconstraint) can decrease the uncertainty of inference. in\\nthis study, the degree of sentence constraint was determined\\nusing standard cloze testing methods: quantifying the\\ndiversity of responses from 30 human judges when the target\\nword is left as a fill-in-the-blank question.\\n\\n3.1.3\\n\\nstudy protocol\\n\\nthe word learning study comprised four parts: (1) a pretest, which was used to estimate baseline knowledge of\\nwords, (2) a training session, where learners were exposed to\\nwords in meaningful contexts, (3) an immediate post-test,\\nand (4) a delayed post-test, which occurred approximately\\none week after training.\\npre-test. the pre-test session was designed to measure\\nthe students’ prior knowledge of the target words. for\\neach target word, students were asked to answer two types\\nof questions: familiarity-rating questions and synonym\\nselection questions. in familiarity rating questions, students\\nprovided their self-rated familiarity levels (unknown, known,\\nand familiar) for presented target words. in synonymselection questions, students were asked to select a synonym\\nword for the given target word from five multiple choice\\noptions. the outcome from synonym-selection questions\\nprovided more objective measures for students’ prior domain\\nknowledge of target words.\\ntraining. approximately one week after the pre-test\\nsession, students participated in the training. during\\ntraining, students learned strategies to infer the meaning\\nof an unknown word in a sentence by using surrounding\\ncontextual information.\\na training session consisted of two parts: an instruction\\nvideo and practice questions. in the instruction video,\\nstudents saw an animated movie clip about how to identify\\nand use contextual information from the sentence to infer\\nthe meaning of an unknown word. in the practice question\\npart, students could exercise the skill that they learned from\\nthe video. dscovar provided sentences that included a\\ntarget word with different levels of surrounding contextual\\ninformation. the amount of contextual information for\\neach sentence was determined by external crowd workers\\n(details described in section 3.1.2). in the practice question\\npart, each target word was presented four times within82\\n\\n\\x0cdifferent sentences. students were asked to type a synonym\\nof the target word, which was presented in the sentence as\\nunderlined and bold. over two weeks, students participated\\nin two training sessions with a week’s gap between them.\\neach training session contained the instruction video and\\npractice questions for 30 target words. an immediate posttest session followed right after each training session.\\nfigure 1: an example of a training session question.\\nin this example, the target word is “education” with\\na feedback message for a high-accuracy response.\\n\\nfigure 2: ten semantic scales used for projecting\\ntarget words and responses [16].\\n• bad – good\\n\\n• complex – simple\\n\\n• passive – active\\n\\n• fast – slow\\n\\n• powerful – helpless\\n\\n• noisy – quiet\\n\\n• big – small\\n\\n• new – old\\n\\n• helpful – harmful\\n\\n• healthy – sick\\n\\nend. the word’s relationship with each semantic anchor can\\nbe automatically measured from its semantic similarity with\\nthese exemplar semantic elements.\\n\\n3.2.2\\n\\nstudents were randomly selected to experience different\\ninstruction video conditions (full instruction video vs.\\nrestricted instruction video). additionally, various difficulty\\nlevel conditions and feedback conditions (e.g., dscovar\\nprovides a feedback message to the student based on answer\\naccuracy vs. no feedback) were tested within the same\\nstudent. however, in this study, we focused on data\\nfrom students who experienced a full instruction video\\nand repeating difficulty conditions. repeating difficulty\\nconditions included questions with all high or medium\\ncontextual constraint levels. by doing so, we wanted to\\nminimize the impact from various experimental conditions\\nfor analyzing post-test outcomes. moreover, we filtered out\\nresponse sequences that did not include all four responses\\nfor the target word. as a result, we analyzed 818 response\\nsequences from 7,425 items in total.\\nimmediate and delayed post-test. the immediate\\npost-test occurred right after the students finished the\\ntraining; the delayed post-test was conducted one week later.\\ndata collected during the immediate and delayed posttests were used to estimate short-and long-term learning,\\nrespectively. test items were identical to those in the pretest\\nsession, except for item order, which varied across tests. for\\nanalysis of the delayed post-test data, we only used the data\\nfrom target words for which the student provided a correct\\nanswer in the earlier, immediate post-test session. as a\\nresult, 449 response sequences were analyzed for predicting\\nthe long-term retention.\\n\\n3.2\\n\\nsemantic score-based features\\n\\nwe now describe the semantic features tested in our\\nprediction models.\\n\\n3.2.1\\n\\nsemantic scales\\n\\nfor this study, we used semantic scales from osgood’s study\\n[16]. ten scales were selected by a cognitive psychologist as\\nbeing considered semantic attributes that can be detected\\nduring word learning (figure 2). each semantic scale\\nconsists of pairs of semantic attributes. for example, the\\nbad–good scale can show how the meaning of a word can\\nbe projected on a scale with bad and good located at either\\n\\nbasic semantic distance scores\\n\\nto extract meaningful semantic information, we have\\napplied the following measures that can be used to explain\\nvarious characteristics of student responses for different\\ntarget words. in this study, we used a pre-trained model\\nfor word2vec,1 built based on the google news corpus\\n(100 billion tokens with 3 million unique vocabularies,\\nusing a negative sampling algorithm), to measure semantic\\nsimilarity between words. the output of the pre-trained\\nword2vec model contained a numeric vector with 300\\nhundred dimensions.\\nfirst, we calculated the relationship between word pairs (i.e.,\\na single student response and the target word, or a pair of\\nresponses) in both the regular word2vec (w2v) score and\\nthe osgood semantic scale (osg) score. in the w2v score,\\nthe semantic relationship between words was represented\\nwith a cosine similarity between word vectors:\\ndw2v (w1 , w2 ) = 1 − |sim(v (w1 ), v (w2 ))|.\\n\\n(1)\\n\\nin this equation, the function v returned the vectorized\\nrepresentation of the word (w1 or w2 ) from the pre-trained\\nword2vec model. by calculating the cosine similarity\\nbetween two vectors (a cosine similarity function is noted\\nas sim), we could extract a single numeric similarity score\\nbetween two words. this score was converted into a\\ndistance-like score by taking the absolute value of the cosine\\nsimilarity score and subtracting from one.\\nfor the osg score, we extracted two different types of\\nscores: a non-normalized score and a normalized score. a\\nnon-normalized score showed how a word is similar to a\\nsingle anchor word (e.g., bad or good ) from the osgood scale.\\nnon\\nsosg\\n(w, ai,j ) = sim(v (w), v (ai,j ))\\n\\n(2)\\n\\nnon\\nnon\\nnon\\ndosg\\n(w1 , w2 ; ai,j ) = |sosg\\n(w1 , ai,j )| − |sosg\\n(w2 , ai,j )| (3)\\n\\nin equation 2, ai,j represents a single anchor word (j) in\\nthe i-th osgood scale. the similarity between the anchor\\nword and the evaluating word w was calculated with cosine\\nsimilarity of word2vec outcomes for both words. in a nonnormalized setting, the distance between two words given\\nby a particular anchor word was calculated by the difference\\nof absolute cosine similarity scores (equation 3).\\nthe second type of osg score is a normalized score. by\\nusing word2vec’s ability to do arithmetical calculation of\\n1\\napi and pre-trained model for word2vec was downloaded\\nfrom this url: https://github.com/3top/word2vec-api83\\n\\n\\x0cmultiple word vectors, the normalized osg score provided\\na relative location of the word from two anchor ends of the\\nosgood scale.\\nnrm\\nsosg\\n(w, ai ) = sim(v (w), v (ai,1 ) − v (ai,2 ))\\nnrm\\nnrm\\nnrm\\ndosg\\n(w1 , w2 ; ai ) = |sosg\\n(w1 , ai ) − sosg\\n(w2 , ai )|\\n\\n(4)\\n(5)\\n\\nin equation 4, the output represents the cosine similarity\\nscore between the word w and two anchor words (ai,1\\nand ai,2 ). for example, if the cosine similarity score of\\nnrm\\nsosg\\n(w, ai ) is close to -1, it means the word w is close to\\nthe first anchor word ai,1 . if the score is close to 1, it is vice\\nversa. in equation 5, the distance between two words was\\ncalculated as the absolute value of the difference between\\ntwo cosine similarity measures.\\n\\n3.2.3\\n\\nderiving predictive features\\n\\nbased on semantic distance equations explained in the\\nprevious section, this section explains examples of predictive\\nfeatures that we used to predict students’ short-term\\nlearning and long-term retention.\\ndistance between the target word and the\\nresponse. for regular word2vec score models and osgood\\nscale score models, distance measures between the target\\nword and the response (by using equations 1 and 5) were\\nused to estimate the accuracy of the response to a question.\\nthis feature represents the trial-by-trial answer accuracy of\\na student response. each response sequence for the target\\nword contained four distance scores.\\ndifference between responses. another feature that\\nwe used in both types of models was the difference between\\nresponses. this feature could capture how a student’s\\ncurrent answer is semantically different from the previous\\nresponse. from each response sequence, we could extract\\nthree derivative scores from four responses.\\nconvex hull area of responses.\\nalternative to\\nthe difference between responses feature, osgood scale\\nmodels were also tested with the area size of convex hull\\nthat can be generated by responses calculated with nonnormalized osgood scale scores (equation 3). for example,\\nfor each osgood scale, a non-normalized score provided\\ntwo-dimensional scores that can be used for geometric\\nrepresentation. by putting the target word in an origin\\nposition, a sequence of responses can create a polygon\\nthat can represent the semantic area that the student\\nexplored with responses. since some response sequences\\nwere unable to generate the polygon by including less than\\nthree unique responses, we added a small, random noise\\nthat uniformly distributed (between −10−4 and 10−4 ) to all\\nresponse points. additionally, a value of 10−20 was added to\\nall convex hull area output to create a visible lower-bound\\nvalue.\\nunlike the measure of difference between responses, this\\nfeature also considers angles that can be created between\\nresponses and the target word. this representation can\\nprovide more information than just using difference between\\nresponses.\\n\\n3.3\\n\\nmodeling\\n\\nto predict students’ short-term learning and long-term\\nretention, we used a mixed-effect logistic regression model\\n\\n(mlr). mlr is a general form of logistic regression model\\nthat includes random effect factors to capture variations\\nfrom repeated measures.\\n\\n3.3.1\\n\\noff-line variables\\n\\noff-line variables capture item- or subject-level variances\\nthat can be observed repeatedly from the data. in this study,\\nwe used multiple off-line variables as random effect factors.\\nfirst, results from familiarity-rating and synonym-selection\\nquestions from the pre-test session were used to include\\nitem- and subject-level variances. both variables include\\ninformation on the student’s prior domain knowledge level\\nfor target words. second, the question difficulty condition\\nwas considered as an item group level factor. in the analysis,\\nsentences for the target word that were presented to the\\nstudent contained the same difficulty level, either high or\\nmedium contextual constraint levels, over four trials. third,\\na different experiment group was used as a subject group\\nfactor. as described in section 3.1.1, this study contains\\ndata from students in different institutions in separate\\ngeographic locations. the inclusion of these participant\\ngroups in the model can be used to explain different\\nshort-term learning outcomes and long-term retention by\\ndemographic groups.\\n\\n3.3.2\\n\\nmodel building\\n\\nin this study, we compared the performance of mlr models\\nwith four different feature types. first, the baseline model\\nwas set to indicate the mlr model’s performance without\\nany fixed effect variables but only with random intercepts.\\nsecond, the response time model was built to be compared\\nwith semantic score-based models. many previous studies\\nhave used response time as an important predictor of student\\nengagement and learning [2, 12]. in this study, we used two\\ntypes of response time variables, the latency for initiating\\nthe response and finishing typing the response, as predictive\\nfeatures. both variables were measured in milliseconds over\\nfour trials and natural log transformed for the analysis.\\nthird, semantic features from regular word2vec scores were\\nused as predictors. this model was built to show how\\nsemantic scores from word2vec can be useful for predicting\\nstudents’ short- and long-term performance in dscovar.\\nlastly, osgood scale-based features were used as predictors.\\nthis model was compared with the regular word2vec score\\nmodel to examine the effectiveness of using osgood scales for\\nevaluating students’ performance in dscovar. for these\\nsemantic-score based models, we tested out different types\\nof predictive features that were described in section 3.2.3.\\nall models shared the same random intercept structure\\nthat treated each off-line variable as an individual random\\nintercept.\\nfor osgood scale models, we also derived reduced-scale\\nmodels. reduced-scale models were compared with the fullscale model, which uses all ten osgood scales. in this case,\\nusing fewer osgood scales can provide easier interpretation\\nof semantic analysis for intelligent tutoring system users.\\n\\n3.3.3\\n\\nmodel evaluation\\n\\nto compare performance between different models, this\\nstudy used various evaluation metrics, including auc (an\\narea under the curve score from a response operating\\ncharacteristic (roc) curve), f1 (a harmonic mean of\\nprecision and recall), and error rate (a ratio of the number of84\\n\\n\\x0cmisclassified items over total items). 95% confidence interval\\nof each evaluation metric was calculated from the outcome of\\na ten-fold cross-validation process repeated over ten times.\\nto select the semantic score-based features for models based\\non regular word2vec scores and osgood scale scores, we\\nused rankings from each evaluation metric. the model with\\nthe highest overall rank (i.e., sum the ranks from auc, f1 ,\\nand error rate, and select the model with the lowest ranksum value) was considered the best-performing model for\\nthe score type (i.e., models based on the regular word2vec\\nscore or osgood scale score). more details on this process\\nwill be illustrated in the next section.\\n\\n4. results\\n4.1 selecting models\\n\\nin this section, we selected the best-performing model based\\non the models’ overall ranks in each evaluation metric. all\\nmodel parameters were trained in each fold of repeated\\ncross-validation. we calculated 95% confidence intervals for\\ncomparison. to calculate the confidence interval of f1 and\\nerror rate measures, the maximum (f1 ) and minimum (error\\nrate) scores of each fold were extracted. these maximum\\nand minimum values were derived from applying multiple\\ncutoff points to the mixed-effect regression model.\\n\\n4.1.1\\n\\npredicting immediate learning\\n\\nfirst, we built models that predict the students’ immediate\\nlearning from the immediate post-test session.\\nfrom\\nmodels based on regular word2vec scores (w2v), the model\\nwith the distance between the target and responses and\\nthe difference between responses (dist+resp) provided the\\nhighest rank from various evaluation metrics (table 2).\\nfrom models based on osgood scales (osg), the model with\\nthe difference between responses (resp) provided the highest\\nrank.\\nthe selected w2v model provided significantly better\\nperformance than the baseline model. the selected osg\\nmodel also showed significantly better performance than the\\nbaseline model, except for the auc score. the selected\\nw2v model was significantly better than the model using\\nresponse time features in the auc score and error rates.\\nthe selected w2v model showed significantly better\\nperformance than the osg model only with the auc score.\\nfigure 3 shows that the w2v model has a slightly larger area\\nunder the roc curve than the osg model. in the precision\\nand recall curve, the selected w2v model provides more\\nbalanced trade-offs between precision and recall measures.\\nthe selected osg model outperforms the w2v model in\\nprecision only in a very low recall measure range.\\n\\n4.1.2\\n\\npredicting long-term retention\\n\\nwe also built prediction models to predict the students’\\nlong-term retention in the delayed post-test session. in\\nthis analysis, a student response was included only when\\nthe student provided correct answers to the immediate\\npost-test session questions.\\namong w2v score-based\\nmodels, the best-performing model contained the same\\nfeature types as the immediate post-test results (table 3).\\nby using the distance between the target and responses\\nand difference between responses (dist+resp), the model\\n\\nachieved significantly better performance than the baseline\\nmodel, except for the auc score.\\nfor osg models, the model with a convex hull area of\\nresponses (chull ) provided the highest overall rank from\\nevaluation metrics (table 3). the results were significantly\\nbetter than the baseline model, and marginally better than\\nthe w2v model. both selected w2v and osg models were\\nmarginally better than the response time model, except the\\nerror rate of the osg model was significantly better.\\nin figure 3, the selected w2v model slightly outperforms\\nthe osg model in mid-range true positive rates, while\\nthe osg model performed slightly better in a higher true\\npositive area. precision and recall curves show similar\\npatterns to those we observed from the immediate post-test\\nprediction models. the osg model only outperforms the\\nw2v model in a very low recall value area.\\n\\n4.1.3 comparing models\\ncompared to the selected w2v model in the immediate\\npost-test condition, the selected w2v model in the delayed\\npost-test retention condition showed a significantly lower\\nauc score, marginally higher f1 score, and marginally\\nhigher error rate. in terms of osg models, the selected osg\\nmodel for delayed post-test retention showed a significantly\\nbetter f1 score and error rates than the selected osg model\\nin the immediate post-test condition. based on these results,\\nwe can argue that osgood scale scores can be more useful for\\npredicting student retention in the delayed post-test session\\nthan predicting the outcome from the immediate post-test.\\nin terms of selected feature types, the best-performing\\nosg models used features based on the difference between\\nresponses (resp) or the convex hull area (chull ) that was\\ncreated from the relative location of the responses. on the\\nother hand, selected w2v models used both the distance\\nbetween the target word and responses and difference\\nbetween responses (dist+resp).\\nwhen we compared\\nboth w2v and osg models using the difference between\\nresponses feature, we found that performance is similar in\\nthe immediate post-test data. however, the osg model\\nwas significantly better in the delayed post-test data. these\\nresults show that osgood scale scores can be more useful for\\nrepresenting the relationship among response sequences.\\n\\n4.2 comparing the osgood scales\\n\\nto identify which osgood scales are more helpful than\\nothers for predicting students’ performance, we conducted\\na scale-wise importance analysis. the results from this\\nsection reveal which osgood scales are more important than\\nothers, and how the performance of prediction models with\\na reduced number of scales is comparable with the full-scale\\nmodel.\\n\\n4.2.1\\n\\nidentifying more important osgood scales\\n\\nin this section, based on the selected osgood score model\\nfrom section 4.1, we identified the level of contribution for\\nfeatures based on each osgood scale. for example, the\\nselected osg model for predicting the immediate post-test\\ndata uses the difference between responses in ten osgood\\nscales as features. in order to diagnose the importance level\\nof the first scale (bad–good ), we can retrain the model with\\nfeatures based on the nine other scales and compare the85\\n\\n\\x0ctable 2: ranks of predictive feature sets for regular word2vec models (w2v) and osgood score models\\n(osg) in the immediate post-test data. all models are significantly better than the baseline model. (bold:\\nthe selected model with highest overall rank.)\\nfeatures\\nbaseline\\nrt\\ndist\\nresp\\nchull\\ndist+resp\\ndist+chull\\n\\nauc\\n0.68 [0.67, 0.69] (5)\\n0.69 [0.68, 0.70] (4)\\n0.72 [0.71, 0.74] (1)\\n0.70 [0.69, 0.71] (3)\\nna\\n0.72 [0.71, 0.73] (2)\\nna\\n\\nw2v models\\nf1\\n0.74 [0.73, 0.74] (5)\\n0.75 [0.75, 0.76] (3)\\n0.76 [0.75, 0.76] (2)\\n0.75 [0.74, 0.76] (4)\\nna\\n0.76 [0.75, 0.77] (1)\\nna\\n\\nerr\\n0.33 [0.33, 0.34] (5)\\n0.31 [0.31, 0.32] (4)\\n0.29 [0.28, 0.30] (2)\\n0.31 [0.30, 0.32] (3)\\nna\\n0.29 [0.28, 0.30] (1)\\nna\\n\\nauc\\n0.68 [0.67, 0.69] (5)\\n0.69 [0.68, 0.70] (2)\\n0.67 [0.66, 0.68] (7)\\n0.69 [0.68, 0.70] (1)\\n0.69 [0.68, 0.70] (3)\\n0.68 [0.67, 0.69] (4)\\n0.67 [0.66, 0.68] (6)\\n\\nosg models\\nf1\\n0.74 [0.73, 0.74] (5)\\n0.75 [0.74, 0.76] (2)\\n0.73 [0.73, 0.74] (7)\\n0.75 [0.75, 0.76] (1)\\n0.74 [0.73, 0.75] (4)\\n0.74 [0.73, 0.75] (3)\\n0.74 [0.73, 0.74] (6)\\n\\nerr\\n0.33 [0.33, 0.34] (7)\\n0.31 [0.31, 0.32] (2)\\n0.33 [0.32, 0.34] (6)\\n0.31 [0.30, 0.32] (1)\\n0.32 [0.31, 0.33] (4)\\n0.31 [0.31, 0.32] (3)\\n0.33 [0.32, 0.34] (5)\\n\\ntable 3: ranks of predictive feature sets for w2v and osg models in the delayed post-test data. all models\\nare significantly better than the baseline model. (bold: the selected model with highest overall rank.)\\nfeatures\\nbaseline\\nrt\\ndist\\nresp\\nchull\\ndist+resp\\ndist+chull\\n\\nauc\\n0.65 [0.64, 0.67] (5)\\n0.67 [0.65, 0.68] (3)\\n0.66 [0.64, 0.68] (4)\\n0.69 [0.67, 0.71] (1)\\nna\\n0.68 [0.66, 0.70] (2)\\nna\\n\\nw2v models\\nf1\\n0.75 [0.74, 0.76] (5)\\n0.76 [0.76, 0.77] (4)\\n0.77 [0.76, 0.78] (3)\\n0.77 [0.76, 0.78] (2)\\nna\\n0.78 [0.77, 0.79] (1)\\nna\\n\\nerr\\n0.33 [0.32, 0.34] (5)\\n0.31 [0.30, 0.32] (3)\\n0.31 [0.30, 0.32] (4)\\n0.30 [0.29, 0.31] (2)\\nna\\n0.30 [0.29, 0.31] (1)\\nna\\n\\nperformance of the newly trained model with the existing\\nfull-scale model.\\nin table 4, we picked the top five scales that were\\nimportant in individual prediction tasks. we found that bigsmall, helpful-harmful, complex-simple, and fast-slow were\\ncommonly important osgood scales for predicting students’\\nperformance in immediate post-test and delayed post-test\\nsessions. scales like bad-good and passive-active were only\\nimportant scales in the immediate post-test prediction.\\nlikewise, new-old was an important scale only in the delayed\\npost-test prediction.\\ntable 4: scale-wise importance of each osgood\\nscale. scales were selected based on the sum of each\\nevaluation metric’s rank. (bold: osgood scales that\\nwere commonly important in both prediction tasks;\\n*: top five scales in each prediction task including\\ntied ranks)\\nscales\\nbad-good\\npassive-active\\npowerful-helpless\\nbig-small\\nhelpful-harmful\\ncomplex-simple\\nfast-slow\\nnoisy-quiet\\nnew-old\\nhealthy-sick\\n\\n4.2.2\\n\\nimm. post-test\\nauc f1 err all\\n1\\n1\\n1\\n1*\\n2\\n4\\n3\\n2*\\n7\\n9\\n6\\n7.5\\n3\\n3\\n4\\n3*\\n4\\n6\\n5\\n5.5*\\n8\\n5\\n2\\n5.5*\\n5\\n2\\n7\\n4*\\n6\\n8\\n8\\n7.5\\n9\\n7\\n9\\n9\\n10\\n10 10\\n10\\n\\ndel. post-test\\nauc f1 err all\\n4\\n10 4\\n6\\n8\\n6\\n6\\n7\\n10\\n8\\n10\\n10\\n1\\n3\\n2\\n2*\\n2\\n1\\n1\\n1*\\n3\\n5\\n7\\n4.5*\\n6\\n4\\n3\\n3*\\n7\\n9\\n9\\n9\\n5\\n2\\n8\\n4.5*\\n9\\n7\\n5\\n8\\n\\nperformance of reduced models\\n\\nbased on the scale-wise importance analysis results, we built\\nreduced-scale models that only contain features with more\\nimportant osgood scales. the prediction performance of\\nreduced-scale models was similar or marginally better than\\nfull-scale osg models. for example, the osg model for\\npredicting the immediate post-test outcome with the top\\ntwo scales (bad–good and passive–active) were marginally\\nbetter than the full-scale model (auc: 0.71 [0.70, 0.72], f1 :\\n0.76 [0.75, 0.77], error rate: 0.30 [0.29, 0.30]). similar results\\nwere observed for predicting retention in the delayed posttest (selected scales: helpful–harmful, big–small ) (auc: 0.71\\n[0.69, 0.72], f1 : 0.79 [0.78, 0.80], error rate: 0.28 [0.27,\\n\\nauc\\n0.65 [0.64, 0.67] (5)\\n0.67 [0.65, 0.68] (3)\\n0.66 [0.64, 0.68] (4)\\n0.63 [0.61, 0.65] (7)\\n0.69 [0.68, 0.71] (1)\\n0.64 [0.62, 0.66] (6)\\n0.69 [0.67, 0.71] (2)\\n\\nosg models\\nf1\\n0.75 [0.74, 0.76] (7)\\n0.76 [0.76, 0.77] (5)\\n0.78 [0.77, 0.79] (3)\\n0.76 [0.75, 0.77] (6)\\n0.78 [0.77, 0.79] (2)\\n0.77 [0.76, 0.78] (4)\\n0.78 [0.78, 0.79] (1)\\n\\nerr\\n0.33 [0.32, 0.34] (7)\\n0.31 [0.30, 0.32] (5)\\n0.30 [0.29, 0.31] (3)\\n0.32 [0.31, 0.33] (6)\\n0.28 [0.27, 0.29] (1)\\n0.31 [0.29, 0.32] (4)\\n0.29 [0.27, 0.30] (2)\\n\\n0.29]). although differences were small, the results indicate\\nthat using a small number of osgood scales can be similarly\\neffective to the full-scale model.\\n\\n5.\\n\\ndiscussion and conclusions\\n\\nin this paper, we introduced a novel semantic similarity\\nscoring method that uses predefined semantic scales to\\nrepresent the relationship between words. by combining\\nosgood’s semantic scales [16] and word2vec [13], we could\\nautomatically extract the semantic relationship between\\ntwo words in a more interpretable manner. to show this\\nmethod can effectively represent students’ knowledge in\\nvocabulary acquisition, we built prediction models that can\\nbe used to predict the student’s immediate learning and\\nlong-term retention. we found that our models performed\\nsignificantly better than the baseline and the responsetime-based models. in the future, we believe results from\\nusing an osgood scale-based student model could be used\\nto provide a more personalized learning experience, such\\nas generating questions that can improve an individual\\nstudent’s understanding for specific semantic attributes.\\nbased on our findings, we have identified the following\\npoints for further discussion. first, in section 4.1, we\\nfound that models using osgood scale scores perform\\nsimilarly with models using regular word2vec scores\\nfor predicting students’ long-term retention of acquired\\nvocabulary. however, we think our models can be further\\nimproved by incorporating additional features. for example,\\nnon-semantic score-based features like response time and\\northographic similarity among responses can be useful\\nfeatures for capturing different patterns of false predictions\\nof current models. moreover, some general measures to\\ncapture a student’s meta-cognitive or linguistic skills could\\nbe helpful to explain different retention results found even if\\nstudents provided the same response sequences. similarly, in\\nsection 4.1.3, we found that osgood scores can be a better\\nmetric to characterize the relationship between responses\\nin terms of predicting students’ retention. a composite\\nmodel that uses both regular word2vec score-based feature\\n(target-response distance) and osgood scale score-based\\nfeature (response-response distance) may also provide better86\\n\\n\\x0cfigure 3: roc curves and precision and recall curves for selected immediate post-test prediction models\\n(left) and delayed post-test prediction models (right). curves are smoothed out with a local polynomial\\nregression method based on repeated cross-validation results.\\n\\nprediction performance.\\nsecond, we found that models with a reduced number of\\nosgood scales performed marginally better than the fullscale model. however, differences were very small. since\\nthis study only used some of the semantic scales from\\nosgood’s study [16], further investigation would be required\\nto examine the validity of these scales, including other scales\\nnot used for this study, for capturing the semantic attributes\\nof student responses during vocabulary learning.\\nalso, there were some limitations in the current study\\nand areas for future work. first, expanding the scope\\nof analysis to the full set of experimental conditions\\nused in the study may reveal more complex interactions\\nbetween these conditions and students’ short- and longterm learning. second, this study used a fixed threshold\\nof 0.5 for investigating false prediction results. however, an\\noptimal threshold for each participant group or prediction\\nmodel could be selected, especially if there are different false\\npositive or negative patterns observed for different groups\\nof students. lastly, this study collected data from a single\\nvocabulary tutoring system that was used in a classroom\\nsetting. applying the proposed method to data that was\\ncollected from a non-classroom setting or other vocabulary\\nlearning system would be useful to show the generalization\\nof our suggested method.\\n\\n6.\\n\\nacknowledgments\\n\\nthe research reported here was supported by the institute of\\neducation sciences, u.s. department of education, through\\ngrant r305a140647 to the university of michigan. the\\nopinions expressed are those of the authors and do not\\nrepresent views of the institute or the u.s. department\\nof education. we thank dr. charles perfetti and his lab\\nteam at the university of pittsburgh, particularly adeetee\\nbhide and kim muth, and the helpful personnel at all of our\\npartner schools.\\n\\n7.\\n\\nreferences\\n\\n[1] s. adlof, g. frishkoff, j. dandy, and c. perfetti. effects of\\ninduced orthographic and semantic knowledge on\\nsubsequent learning: a test of the partial knowledge\\nhypothesis. reading and writing, 29(3):475–500, 2016.\\n[2] j. e. beck. engagement tracing: using response times to\\nmodel student disengagement. artificial intelligence in\\neducation: supporting learning through intelligent and\\nsocially informed technology, 125:88, 2005.\\n[3] k. collins-thompson and j. callan. automatic and human\\nscoring of word definition responses. in hlt-naacl,\\npages 476–483, 2007.\\n[4] m. coltheart. the mrc psycholinguistic database. the\\n\\n[5]\\n[6]\\n[7]\\n\\n[8]\\n\\n[9]\\n\\n[10]\\n[11]\\n\\n[12]\\n[13]\\n\\n[14]\\n[15]\\n[16]\\n[17]\\n\\n[18]\\n[19]\\n\\n[20]\\n\\nquarterly journal of experimental psychology,\\n33(4):497–505, 1981.\\ne. dale. vocabulary measurement: techniques and major\\nfindings. elementary english, 42(8):895–948, 1965.\\nf. t. durso and w. j. shore. partial knowledge of word\\nmeanings. journal of experimental psychology: general,\\n120(2):190, 1991.\\ng. a. frishkoff, k. collins-thompson, l. hodges, and\\ns. crossley. accuracy feedback improves word learning\\nfrom context: evidence from a meaning-generation task.\\nreading and writing, 29(4):609–632, 2016.\\ng. a. frishkoff, k. collins-thompson, s. nam, l. hodges,\\nand s. a. crossley. dynamic support of contextual\\nvocabulary acquisition for reading (dscovar): an\\nintelligent tutoring system for contextual word learning.\\nhandbook on educational technologies for literacy, 2016.\\ng. a. frishkoff, c. a. perfetti, and k. collins-thompson.\\npredicting robust vocabulary growth from measures of\\nincremental learning. scientific studies of reading,\\n15(1):71–91, 2011.\\nt. k. landauer. latent semantic analysis. wiley online\\nlibrary, 2006.\\ny. li, l. xu, f. tian, l. jiang, x. zhong, and e. chen.\\nword embedding revisited: a new representation learning\\nand explicit matrix factorization perspective. in\\nproceedings of the 24th international joint conference on\\nartificial intelligence, buenos aires, argentina, pages\\n3650–3656, 2015.\\ny. ma, l. agnihotri, m. h. education, r. baker, and\\ns. mojarad. effect of student ability and question difficulty\\non duration. in educational data mining, 2016.\\nt. mikolov, i. sutskever, k. chen, g. s. corrado, and\\nj. dean. distributed representations of words and phrases\\nand their compositionality. in advances in neural\\ninformation processing systems, pages 3111–3119, 2013.\\ng. a. miller. wordnet: a lexical database for english.\\ncommunications of the acm, 38(11):39–41, 1995.\\ns. nam. predicting off-task behaviors in an adaptive\\nvocabulary learning system. in educational data mining,\\n2016.\\nc. e. osgood, g. j. suci, and p. h. tannenbaum. the\\nmeasurement of meaning. university of illinois press, 1957.\\nk. ostrow, c. donnelly, s. adjei, and n. heffernan.\\nimproving student modeling through partial credit and\\nproblem difficulty. in proc. of the second acm conference\\non learning@scale, pages 11–20. acm, 2015.\\np. i. pavlik and j. r. anderson. practice and forgetting\\neffects on vocabulary memory: an activation-based model\\nof the spacing effect. cog. science, 29(4):559–586, 2005.\\ne. g. van inwegen, s. a. adjei, y. wang, and n. t.\\nheffernan. using partial credit and response history to\\nmodel user knowledge. international educational data\\nmining society, 2015.\\nl. m. yonek. the effects of rich vocabulary instruction\\non students’ expository writing. phd thesis, university of\\npittsburgh, 2008.',\n",
       " \"71\\n\\n\\x0cefficient feature embeddings for student classification\\nwith variational auto-encoders\\nseverin klingler\\n\\ndept. of computer science\\neth zurich, switzerland\\n\\nkseverin@inf.ethz.ch\\n\\nrafael wampfler\\n\\ndept. of computer science\\neth zurich, switzerland\\n\\nwrafael@inf.ethz.ch\\n\\nbarbara solenthaler\\n\\ndept. of computer science\\neth zurich, switzerland\\n\\nsobarbar@inf.ethz.ch\\n\\nabstract\\ngathering labeled data in educational data mining (edm)\\nis a time and cost intensive task. however, the amount\\nof available training data directly influences the quality of\\npredictive models. unlabeled data, on the other hand, is\\nreadily available in high volumes from intelligent tutoring\\nsystems and massive open online courses. in this paper, we\\npresent a semi-supervised classification pipeline that makes\\neffective use of this unlabeled data to significantly improve\\nmodel quality. we employ deep variational auto-encoders\\nto learn efficient feature embeddings that improve the performance for standard classifiers by up to 28% compared\\nto completely supervised training. further, we demonstrate\\non two independent data sets that our method outperforms\\nprevious methods for finding efficient feature embeddings\\nand generalizes better to imbalanced data sets compared\\nto expert features. our method is data independent and\\nclassifier-agnostic, and hence provides the ability to improve\\nperformance on a variety of classification tasks in edm.\\n\\nkeywords\\nsemi-supervised classification, variational auto-encoder, deep\\nneural networks, dimensionality reduction\\n\\n1.\\n\\nintroduction\\n\\nbuilding predictive models of student characteristics such\\nas knowledge level, learning disabilities, personality traits\\nor engagement is one of the big challenges in educational\\ndata mining (edm). such detailed student profiles allow\\nfor a better adaptation of the curriculum to the individual\\nneeds and is crucial for fostering optimal learning progress.\\nin order to build such predictive models, smaller-scale and\\ncontrolled user studies are typically conducted where detailed information about student characteristics are at hand\\n(labeled data). the quality of the predictive models, however, inherently depends on the number of study participants, which is typically on the lower side due to time and\\nbudget constraints. in contrast to such controlled user studies, digital learning environments such as intelligent tutoring\\nsystems (its), educational games, learning simulations, and\\nmassive open online courses (moocs) produce high volumes\\nof data. these data sets provide rich information about student interactions with the system, but come with no or only\\nlittle additional information about the user (unlabeled data).\\n\\ntanja käser\\n\\ngraduate school of education\\nstanford university, usa\\n\\ntkaeser@stanford.edu\\nmarkus gross\\n\\ndept. of computer science\\neth zurich, switzerland\\n\\ngrossm@inf.ethz.ch\\n\\nsemi-supervised learning bridges this gap by making use of\\npatterns in bigger unlabeled data sets to improve predictions\\non smaller labeled data sets. this is also the focus of this\\npaper. these techniques are well explored in a variety of\\ndomains and it has been shown that classifier performance\\ncan be improved for, e.g., image classification [15], natural language processing [28] or acoustic modeling [21]. in\\nthe education community, semi-supervised classification has\\nbeen used employing self-training, multi-view training and\\nproblem-specific algorithms. self-training has e.g. been applied for problem-solving performance [22]. in self-training,\\na classifier is first trained on labeled data and is then iteratively retrained using its most confident predictions on unlabeled data. self-training has the disadvantage that incorrect predictions decrease the quality of the classifier. multiview training uses different data views and has been explored\\nwith co-training [27] and tri-training [18] for predicting prerequisite rules and student performance, respectively. the\\nperformance of these methods, however, largely depends on\\nthe properties of the different data views, which are not yet\\nfully understood [34]. problem-specific semi-supervised algorithms have been used to organize learning resources in\\nthe web [19], with the disadvantage that they cannot be\\ndirectly applied for other classification tasks.\\nrecently, it has been shown (outside of the education context) that variational auto-encoders (vae) have the potential to outperform the commonly used semi-supervised classification techniques. vae is a neural network that includes\\nan encoder that transforms a given input into a typically\\nlower-dimensional representation, and a decoder that reconstructs the input based on the latent representation. hence,\\nvaes learn an efficient feature embedding (feature representation) using unlabeled data that can be used to improve the performance of any standard supervised learning\\nalgorithm [15]. this property greatly reduces the need for\\nproblem-specific algorithms. moreover, vaes feature the\\nadvantage that the trained deep generative models are able\\nto produce realistic samples that allow for accurate data\\nimputation and simulations [23], which makes them an appealing choice for edm. inspired by these advantages, and\\nthe demonstrated superior classifier performance in other\\ndomains as in computer vision [16, 23], this paper explores\\nvae for student classification in the educational context.72\\n\\n\\x0cwe present a complete semi-supervised classification pipeline\\nthat employs deep vaes to extract efficient feature embeddings from unlabeled student data. we have optimized the\\narchitecture of two different networks for educational data a simple variational auto-encoder and a convolutional variational auto-encoder. while our method is generic and hence\\nwidely applicable, we apply the pipeline to the problem of\\ndetecting students suffering from developmental dyscalculia\\n(dd), which is a learning disability in arithmetics. the large\\nand unlabeled data set at hand consists of student data of\\nmore than 7k students and we evaluate the performance of\\nour pipeline on two independent small and labeled data sets\\nwith 83 and 155 students. our evaluation first compares the\\nperformance of the two networks, where our results indicate\\nsuperiority of the convolutional vae. we then apply different classifiers to both labeled data sets, and demonstrate\\nnot only improvements in classification performance of up to\\n28% compared to other feature extraction algorithms, but\\nalso improved robustness to class imbalance when using our\\npipeline compared to other feature embeddings. the improved robustness of our vae is especially important for\\npredicting relatively rare student conditions - a challenge\\nthat is often met in edm applications.\\n\\n2.\\n\\nbackground\\n\\nin the semi-supervised classification setting we have access\\nto a large data set xb without labels and a much smaller\\nlabeled data set xs with labels ys . the idea behind semisupervised classification is to make use of patterns in the\\nunlabeled data set to improve the quality of the classifier\\nbeyond what would be possible with the small data set\\nxs alone. there are many different approaches to semisupervised classification including transductive svms, graphbased methods, self-training or representation learning [35].\\nin this work we focus on learning an efficient encoding z =\\ne(x) for x ∈ xb of the data domain using the unlabeled\\ndata xb only. this learnt data transformation e(·) - the\\nencoding - is then applied to the labeled data set xs . wellknown encoders include principle component analysis (pca)\\nor kernel pca (kpca). pca is a dimensionality reduction\\nmethod that finds the optimal linear transformation from\\nan n-dimensional to a k-dimensional space (given a meansquared error loss). kernel pca [24] extends pca allowing\\nnon-linear transformations into a k-dimensional space and\\nhas, among others, been successfully used for novelty detection in non-linear domains [11]. recently, variational autoencoders (vae) have outperformed other semi-supervised\\nclassification techniques on several data sets [15]. vae combine variational inference networks with generative models\\nparametrized by deep neural networks that exploit information in the data density to find efficient lower dimensional\\nrepresentations (feature embeddings) of the data.\\nauto-encoder. an auto-encoder or autoassociator [2] is a\\nneural network that encodes a given input into a (typically\\nlower dimensional) representation such that the original input can be reconstructed approximately. the auto-encoder\\nconsists of two parts. the encoder part of the network takes\\nthe n -dimensional input x ∈ rn and computes an encoding z = e(x) while the decoder d reconstructs the input\\nbased on the latent representation x̂ = d(z). if we train\\na network using the mean squared error loss and the network consists of a single linear hidden layer of size k, e.g.\\n\\ne(x) = w1 x + b1 and d(z) = w2 z + b2 for weights\\nw1 ∈ rk×n and w2 ∈ rn ×k and offsets b1 ∈ rk and\\nb2 ∈ rn , the autoencoder behaves similar to pca in that\\nthe network learns to project the input into the span of\\nthe k first principle components [2]. for more complex networks with non-linear layers multi-modal aspects of the data\\ncan be learnt. auto-encoders can be used in semi-supervised\\nclassification tasks because the encoder can compute a feature representation z of the original data x. these features\\ncan then be used to train a classifier. the learnt feature\\nembedding facilitates classification by clustering related observations in the computed latent space.\\nvariational auto-encoder. variational auto-encoders [15]\\nare generative models that combine bayesian inference with\\ndeep neural networks. they model the input data x as\\npθ (x|z) = f (x; z, θ)\\n\\np(z) = n (z|0, i)\\n\\n(1)\\n\\nwhere f is a likelihood function that performs a non-linear\\ntransformation with parameters θ of z by employing a deep\\nneural network. in this model the exact computation of\\nthe posterior pθ (z|x) is not computationally tractable. instead, the true posterior is approximated by a distribution\\nqφ (z|x) [16]. this inference network qφ (z|x) is parametrized\\nas a multivariate normal distribution as\\nqφ (z|x) = n (z|µφ (x), diag(σφ2 (x))),\\n\\n(2)\\n\\nσφ2 (x)\\n\\nwhere µφ (x) and\\ndenote vectors of means and variance\\nrespectively. both functions µφ (·) and σφ2 (·) are represented\\nas deep neural networks. hence, variational autoencoders\\nessentially replace the deterministic encoder e(x) and decoder d(z) by a probabilistic encoder qφ (z|x) and decoder\\npθ (x|z). direct maximization of the likelihood is computationally not tractable, therefore a lower bound on the likelihood has been derived [16]. the learning task then amounts\\nto maximizing this variational lower bound\\neqφ (z|x) [log pθ (x|z)] − kl [qφ (z|x)||p(z)] ,\\n\\n(3)\\n\\nwhere kl denotes the kullback-leibler divergence. the\\nlower bound consists of two intuitive terms. the first term\\nis the reconstruction quality while the second one regularizes the latent space towards the prior p(z). we perform\\noptimization of this lower bound by applying a stochastic\\noptimization method using gradient back-propagation [14].\\n\\n3.\\n\\nmethod\\n\\nin the following we introduce two networks. first, a simple\\nvariational auto-encoder consisting of fully connected layers to learn feature embeddings of student data. these encoders have shown to be powerful for semi-supervised classification [15], and are often applied due to their simplicity.\\nsecond, an advanced auto-encoder that combines the advantages of vae with the superiority of asymmetric encoders.\\nthis is motivated by the fact that asymmetric auto-encoders\\nhave shown superior performance and more meaningful feature representations compared to simple vae in other domains such as image synthesis [29].\\nstudent snapshots. there are many applications where\\nwe want to predict a label yn for each student n within an\\nits based on behavioral data xn . these labels typically\\nrelate to external variables or properties of a student, such73\\n\\n\\x0csimple student auto-encoder (s-sae)\\n𝑞𝜙 𝒛 𝒙)\\n\\nx\\n\\nsampling connection\\n\\nx\\n\\nfc\\n\\nfully connected layer\\n\\ncon\\ncon\\ncon\\n\\nfc\\n\\nnetwork connection\\n\\nfc\\n\\n𝜎\\n\\n𝑧\\n\\ncon\\ncon\\ncon\\n\\n𝜇\\n\\nfc\\n\\nfc\\n\\nx\\n\\ncnn student auto-encoder (cnn-sae)\\n𝑞𝜙 𝒛 𝒙)\\n\\n𝑝𝜃 𝒙 𝒛)\\n\\n𝜇\\n𝜎\\n\\n𝑝𝜃 𝒙 𝒛)\\nlstm\\n\\nfc\\n\\nlstm\\n\\nfc\\n\\n𝑧\\n\\nlstm recurrent lstm\\n\\nx\\n\\ncon\\n\\nconvolutional layer\\n\\nfigure 1: network layouts for our simple student auto-encoder (left) using only fully connected layers and our\\nimproved cnn student auto-encoder (right) using convolutions for the encoder and recurrent lstm layers\\nfor the decoder. in contrast to standard auto-encoders, the connections to the latent space z are sampled\\n(red dashed arrows) from a gaussian distribution.\\n\\nas age, learning disabilities, personality traits, learner types,\\nlearning outcome etc. similar to knowledge tracing (kt)\\nwe propose to model the data xn = {xn1 , . . . , xnt } as a\\nsequence of t observations. in contrast to kt we store f\\ndifferent feature values xnt ∈ rf for each element in the\\nsequence, where t denotes the tth opportunity within a task.\\nthis allows us to simultaneously store data from multiple\\ntasks in xnt , e.g. xn1 stores all features for student n that\\nwere observed during the first task opportunities. for every task in an its we can extract various different features\\nthat characterize how a student n was approaching the task.\\nthese features include performance, answer times, problem\\nsolving strategies, etc. we combine this information into a\\nstudent snapshot xn ∈ rt ×f , where t is the number of task\\nopportunities and f is the number of extracted features.\\nsimple student auto-encoder (s-sae). our simple variational autoencoder is following the general design outlined\\nin section 2 and is based on the student snapshot representation. for ease of notation we use x := vec(xn ), where\\nvec(·) is the matrix vectorization function to represent the\\nstudent snapshot of student n. the complete network layout is depicted in figure 1, left. the encoder and decoder\\nnetworks consist of l fully connected layers that are implemented as an affine transformation of the input followed by\\na non-linear activation function β(·) as xl = β(wl xl−1 +bl ),\\nwhere l is the layer index and wl and bl are a weight matrix\\nand offset vector of suitable dimensions. typical choices for\\nβ(·) include tanh, rectified linear units or sigmoid functions\\n[6]. to produce latent samples z we sample from the normal\\ndistribution (see equation (2)) using re-parametrization [16]\\nz = µφ (x) + σφ (x)\\x0f,\\n\\n(4)\\n\\nwhere \\x0f ∼ n (0, 1), to allow for back-propagation of gradients. for pθ (x|z) (see (1)) any suitable likelihood function can be used. we used a gaussian distribution for all\\npresented examples. note that the likelihood function is\\nparametrized by the entire (non-linear) decoder network.\\nthe training of variational auto-encoders can be challenging\\nas stochastic optimization was found to set qφ (z|x) = p(z)\\nin all but vanishingly rare cases [3], which corresponds to a\\nlocal maximum that does not use any information from x.\\nwe therefore add a warm-up phase that gradually gives the\\nregularization term in the target function more weight:\\neqφ (z|x) [log pθ (x|z)] − α kl [qφ (z|x)||p(z)] ,\\n\\n(5)\\n\\nwhere α ∈ [0, 1] is linearly increased with the number of\\nepochs. the warm-up phase has been successfully used\\nfor training deep variational auto-encoders [25]. furthermore, we initialize the weights of the dense layer computing\\nlog(σφ2 (x)) to 0 (yielding a variance of 1 at the beginning of\\nthe training). this was motivated by our observations that if\\nwe employ standard random weight initialization techniques\\n(glorot-norm, he-norm [9]) we can get relatively high initial\\nestimates for the variance σφ2 (x), which due to the sampling\\nleads to very unreliable samples z in the latent space. the\\nlarge variance in sampled points in the latent space leads to\\nbad convergence properties of the network.\\ncnn student auto-encoder (cnn-sae). following\\nthe recent findings in computer vision we present a second,\\nmore advanced network that typically outperforms simpler\\nvae. in [29], for example, these asymmetric auto-encoders\\nresulted in superior reconstruction of images as well as more\\nmeaningful feature embeddings. a specific kind of convolutional neural network was combined with an auto-encoder,\\nbeing able to directly capture low level pixel statistics and\\nhence to extract more high-level feature embeddings.\\ninspired by this previous work, we combine an asymmetric\\nauto-encoder (and a decoder that is able to capture low level\\nstatistics) with the advantages of variational auto-encoders.\\nfigure 1, right, shows our combined network. we employ\\nmultiple layers of one-dimensional convolutions to parametrize\\nthe encoder qφ (z|x) (again we assume a gaussian distribution, see (2)). the distribution is parametrized as follows:\\nµφ (x) = wµ h + bµ\\nlog(σφ2 (x)) = wσ h + bσ\\nh = convl (x) = β(wl ∗ convl−1 (x)),\\n\\nwhere ∗ is the convolution operator, wl , wµ , wσ are weights\\nof suitable dimensions, β(·) is a non-linear activation function and l denotes the layer depth. further, conv0 (x) = x.\\nwe keep the standard variational layer (see (4)) while changing the output layer to a recurrent layer using long term\\nshort term units (lstm). recurrent layers have successfully been used in auto-encoders before, e.g. in [5]. lstm\\nwere very successful for modeling temporal sequences because they can model long and short term dependencies between time steps. every lstm unit receives a copy of the\\nsampled points in latent-space, which allows the lstm network to combine context information (point in the latent74\\n\\n\\x0cfeature\\nselection\\n\\nnaive bayes\\nlogistic regression\\nsvm\\n\\nlabels\\n\\n𝑞𝜙 𝒛 𝒙)\\n\\nfeature\\nembedding\\n\\nlabeled\\ndata\\n\\n𝑝𝜃 𝒙 𝒛)\\n\\nencoder\\nunlabeled\\ndata\\n\\n𝑞𝜙 𝒛 𝒙)\\n\\nfeature\\nembedding\\n\\nunlabeled\\ndata\\n\\nsemi-supervised classification pipeline\\nencoder\\ndecoder\\n\\nuse trained encoder\\n\\nfigure 2: pipeline overview. we train the variational auto-encoder on a large unlabeled data set. the trained\\nencoder of the auto-encoder can be used to transform other data sets into an expressive feature embedding.\\nbased on this feature embedding we train different classifiers to predict the student labels.\\n\\nspace) with the sequence information (memory unit in the\\nlstm cell). using lstm cells the decoder pθ (x|z) assumes\\na gaussian distribution and is parametrized as follows:\\nµθt (z) = wµz · lstmt (z) + bµz\\n\\n4.1 experimental setup\\n\\n2\\nlog(σθt\\n(z)) = wσz · lstmt (z) + bσz ,\\n2\\nwhere µθt (z) and σθt\\n(z) are the tth components of µθ (z) and\\nσθ2 (z), respectively, lstmt (·) denotes the tth lstm cell and\\nw∗ and b∗ denote suitable weight and offset parameters.\\n\\nfeature selection. vae provide a natural way for performing feature selection. the inference network qφ (z|x)\\ninfers the mean and variance for every dimension zi . therefore, the most informative dimension zi has the highest kl\\ndivergence from the prior distribution p(zi ) = n (0, 1) while\\nuninformative dimensions will have a kl divergence close to\\n0 [10]. the kl divergence of zi to p(zi ) is given by\\nkl [qφ (zi |x)||p(zi )] = − log(σi ) +\\n\\n1\\nσi2 µ2i\\n− ,\\n2\\n2\\n\\nsets since their distribution of dd and non-dd children differs: the first study has approximately 50% dd, while the\\nsecond one includes 5% dd (typical prevalence of dd).\\n\\n(6)\\n\\nwhere µi and σi are the inferred parameter for the gaussian\\ndistribution qφ (zi |x). feature selection proceeds by keeping\\nthe k dimensions zi with the largest kl divergence.\\nsemi-supervised classification pipeline. the encoder\\nand the decoder of the variational auto-encoder can be used\\nindependently of each other. this independence allows us\\nto take the trained encoder and map new data to the learnt\\nfeature embedding. figure 2 provides an overview of the\\nentire pipeline for semi-supervised classification. in a first\\nunsupervised step we train a vae on unlabeled data. the\\nlearnt encoder qφ (z|x) is then used to transform labeled data\\nsets to the feature embedding. we finally apply our feature\\nselection step that considers the relative importance of the\\nlatent dimensions as previously described. we then train\\nstandard classifiers (logistic regression, naive bayes and\\nsupport vector machine) on the feature embeddings.\\n\\n4. results\\nwe evaluated our approach for the specific example of detecting developmental dyscalculia (dd), which is a learning\\ndisability affecting the acquisition of arithmetic skills [33].\\nbased on the learnt feature embedding on a large unlabeled\\ndata set the classifier performance was measured on two independent, small and labeled data sets from controlled user\\nstudies. we refer to them as balanced and imbalanced data\\n\\nall three data sets were collected from calcularis, which is\\nan intelligent tutoring system (its) targeted at elementary\\nschool children suffering from dd or exhibiting difficulties\\nin learning mathematics [13]. calcularis consists of different\\ngames for training number representations and calculation.\\nprevious work identified a set of games that are predictive\\nof dd within calcularis [17]. since timing features were\\nfound to be one of the most relevant indicators for detecting\\ndd [4] and to facilitate comparison to other feature embedding techniques we limited our analysis to log-normalized\\ntiming features, for which we can assume normal distribution [30]. therefore, we evaluated our pipeline on the subset of games from [17] for which meaningful timing features\\ncould be extracted and sufficient samples were available in all\\ndata sets (we used >7000 samples for training the vaes).\\nsince our pipeline currently does not handle missing data\\nonly students with complete data were included.\\ntiming features were extracted for the first 5 tasks in 5 different games. the selected games involve addition tasks\\n(adding a 2-digit number to a 1-digit number with tencrossing; adding two 2-digit numbers with ten-crossing), number conversion (spoken to written numbers in the ranges 010 and 0-100) and subtraction tasks (subtracting a 1-digit\\nnumber from a 2-digit number with ten-crossing). for every\\ntask we extracted the total answer time (time between the\\ntask prompt until the answer was entered) and the response\\ntime (time between the task prompt and the first input by\\nthe student). hence, each student is represented by a 50dimensional snapshot x (see section 3).\\nunlabeled data set. the unlabeled data set was extracted\\nusing live interaction logs from the its calcularis. in total,\\nwe collected data from 7229 children. note that we have\\nno additional information about the children such as dd or\\ngrade. we excluded all teacher accounts as well as log files\\nthat were < 20kb. since every new game in calcularis is\\nintroduced by a short video during the very first task, we\\nexcluded this particular task for all games.\\nbalanced data set. the first labeled data set is based\\non log files from 83 participants of a multi-center user study75\\n\\n\\x0cconducted in germany and switzerland, where approximately\\nhalf of the participants were diagnosed with dd (47 dd, 36\\ncontrol) [31]. during the study, children trained with calcularis at home for five times per week during six weeks and\\nsolved on average 1551 tasks. there were 28 participants\\nin 2nd grade (9 dd, 19 control), 40 children in 3rd grade\\n(23 dd, 17 control), 12 children in 4th grade (12 dd) and\\n3 children in 5th grade (3 dd). the diagnosis of dd was\\nbased on standardized neuropsychological tests [31].\\nimbalanced data set. the second labeled data set is from\\na user study conducted in the classroom of ten swiss elementary school classes. in total, 155 children participated, and\\na prevalence of dd of 5% could be detected (8 dd, 147 control). there were 97 children in 2nd grade (3 dd, 94 control)\\nand 58 children in 3rd grade (5 dd, 53 control). the dd diagnosis was computed based on standardized tests assessing\\nthe mathematical abilities of the children [32, 7]. during the\\nstudy, children solved 85 tasks directly in the classroom. on\\naverage, children needed 26 minutes to complete the tasks.\\nimplementation. the unlabeled data set was used to train\\nthe unsupervised vae for extracting compact feature embeddings of the data. based on the learnt data transformations we evaluated two standard classifiers: logistic regression (lr) and naive bayes (nb). we restricted our evaluation to simple classification models because we wanted to\\nassess the quality of the feature embedding and not the quality of the classifier. more advanced classifiers typically perform a (sometimes implicit) feature transformation as part\\nof their data fitting procedure. to represent at least one\\nmodel that performs such an embedding we included support vector machine (svm) in all our results. all classifier\\nparameters were chosen according to the default values in\\nscikit-learn. note that we have additionally performed randomized cross-validated hyper-parameter search for all classifiers, which, however, resulted in marginal improvements\\nonly. because of that, and to keep the model simple and especially easily reproducible, we use the default parameter set\\nin this work. for logistic regression we used l2 regularization with c = 1, for naive bayes we used gaussian distributions and for the svm rbf kernels and data point weights\\nhave been set inversely proportional to label frequencies. all\\nresults are cross-validated using 30 randomized training-test\\nsplits on the unlabeled data (test size 5%). the classification\\npart of the pipeline is additionally cross-validated using 300\\nlabel-stratified random training-test splits (test size 20%) to\\nensure highly reproducible classification results.\\nnetwork hyper-parameters were defined using the approach\\ndescribed in [1]. we increased the number of nodes per\\nlayer, the number of layers and the number of epochs until\\na good fit of the data was achieved. we then regularized\\nthe network using dropout [26] with increasing dropout rate\\nuntil the network was no longer overfitting the data. activation and weight initialization have been chosen according to common standards: we employ the most common\\nactivation function, namely rectified linear activation units\\n(relu) [20], for all activations. weight initialization was\\nperformed using the method by he et al. [9]. following this\\nprocedure, the following parameters were used for the ssae model: encoder and decoders used 3 layers of size 320.\\nthe cnn-sae model was parametrized as follows: 3 convo-\\n\\nlution layers with 64 convolution kernels and a filter length\\nof 3. we used a single layer of lstm cells with 80 nodes.\\nwe used a batch size of 500 samples and batch normalization and dropout (r = 0.25) at every layer. the warm-up\\nphase (see section 3) was set to 300 epochs. training was\\nstopped after 1000 (s-sae) and 500 (cnn-sae) epochs.\\nthe number of latent units was set to 8 in accordance to\\nprevious work on detecting students with dd that used 17\\nfeatures but found that about half of the features were sufficient to detect dd with high accuracy [17]. when feature\\nselection was applied we set the number of features to k = 4\\nand thus we kept exactly half of the latent space features.\\nall networks were implemented using the keras framework\\nwith tensorflowtm and optimized using adam stochastic\\noptimization with standard parameters according to [14].\\n\\n4.2\\n\\nperformance comparison\\n\\nour vae models are trained to extract efficient feature embeddings of the data. to assess the quality of these computed feature representations, we compare the classification\\nperformance of our method to previous techniques for finding efficient feature embeddings, as well as to feature sets\\noptimized specifically for the task of predicting dd.\\nnetwork comparison. in a first experiment we compared\\nthe feature embeddings generated by our simple s-sae and\\nour asymmetric cnn-sae with and without feature selection. figure 3 illustrates the average roc curves of our\\ncomplete semi-supervised classification pipeline. our feature embeddings based on asymmetric cnn-sae clearly\\noutperform the ones from the simple s-sae on both the\\nimbalanced and the balanced data set for naive bayes (nb)\\nand logistic regression (lr). for both models, feature selection improves the area under the roc curve (auc) for\\nthe imbalanced data set (cnn-sae: lr 4.2%, nb 6.3%;\\ns-sae: lr 6.8%, nb: 1.6%), but has no effect for the balanced data set. we believe that this is due to the ability of\\nthe classifiers to distinguish useful features from noisy ones\\ngiven enough samples. since the performance of the classifiers with feature selection (fs) is better or equal to no\\nfeature selection in each experiment, we used the cnn-sae\\nfs model for all further evaluations.\\nclassification performance. in figure 4 we compare the\\nclassifier performance for different feature embeddings. we\\ncompare our method based on vae to two well-known methods for finding optimal feature embeddings, namely principle\\ncomponent analysis (pca, green) and kernel pca (kpca,\\nred) [24]. for comparison and as a baseline for the performance of the different methods, we include direct classification results (gray), for which no feature embedding was\\ncomputed. we used k = 8 (dimensionality of feature embedding) for all methods. the features extracted by our\\npipeline compare favorably to pca and kernel pca showing improvements in terms of auc of 28% for logistic regression and 23% for naive bayes on the imbalanced data\\nset and an improvement of 3.75% for logistic regression\\nand 7.5% for naive bayes on the balanced data set. by\\nusing simple classifiers, we demonstrated that our encoder\\nlearns an effective feature embedding. more sophisticated\\nclassifiers (such as svm with non-linear kernels) typically\\nproceed by first embedding the input into a specific feature\\nspace that is different from the original space.76\\n\\n\\x0clogistic regression\\n\\n1.0\\ncnn-sae\\ncnn-sae\\n0.8 fs\\ns-sae\\ns-sae\\n0.6 fs\\n\\n0.8\\n\\ntrue positive rate\\n\\ntrue positive rate\\n\\n1.0\\n\\n0.6\\n0.4\\n0.2\\n0.0\\n0.0\\n\\n0.2\\n\\n0.4\\n\\n0.6\\n\\n0.8\\n\\ncnn-sae\\ncnn-sae fs\\ns-sae\\ns-sae fs\\n\\n0.4\\n0.2\\n0.0\\n0.0\\n\\n1.0\\n\\nnaive bayes\\n\\nfalse positive rate\\n\\n0.2\\n\\n0.4\\n\\n0.6\\n\\n0.8\\n\\n1.0\\n\\nfalse positive rate\\n\\n(a) imbalanced data set\\nlogistic regression\\n\\n1.0\\ncnn-sae\\ncnn-sae\\n0.8 fs\\ns-sae\\ns-sae\\n0.6 fs\\n\\n0.8\\n\\ntrue positive rate\\n\\ntrue positive rate\\n\\n1.0\\n\\n0.6\\n0.4\\n0.2\\n0.0\\n0.0\\n\\n0.2\\n\\n0.4\\n\\n0.6\\n\\n0.8\\n\\nfalse positive rate\\n\\n1.0\\n\\nnaive bayes\\ncnn-sae\\ncnn-sae fs\\ns-sae\\ns-sae fs\\n\\n0.4\\n0.2\\n0.0\\n0.0\\n\\n0.2\\n\\n0.4\\n\\n0.6\\n\\n0.8\\n\\n1.0\\n\\nfalse positive rate\\n\\n(b) balanced data set\\nfigure 3: roc curves for the two proposed models with and without feature selection (fs). our\\nasymmetric cnn-sae outperforms the simple ssae consistently with (blue) and without (purple)\\nfeature selection. feature selection improves performance only on the imbalanced data set.\\n\\nfor the imbalanced data set the overall performance for\\nsvm is significantly lower for all embeddings. this is in line\\nwith previous work [12] showing that for imbalanced data\\nsets, the decision boundaries of svms are heavily skewed\\ntowards the minority class resulting in a preference for the\\nmajority class and thus a high miss-classification rate for the\\nminority class. indeed, we found that svm predicted only\\nmajority labels on the imbalanced data set. for the balanced data set our feature embedding shows improvements\\nof 2.5% over alternative embeddings when using svm.\\nfurther, table 1 shows the performance of all feature embeddings using three additional common classification metrics:\\nroot mean squared error (rmse), classification accuracy\\n(acc.) and area under the precision recall curve (aupr).\\nwe statistically compared the classification metrics of our\\nfeature embedding to the best alternative feature embedding using an independent t-test and bonferroni correction\\nfor multiple tests (α = 0.05). our feature embedding significantly outperformed alternative embeddings for all classifiers on both the balanced and imbalanced data sets on most\\nmetrics. the main exception was the performance of svm\\non the imbalanced data set, which exhibited large variance\\nfor all feature embeddings and the worst overall classification performance (compared to the other classifiers).\\nwhen comparing classification performance on the imbalanced and the balanced data sets we observed that our\\npipeline using vaes showed significant performance improvements compared to other methods for finding feature embeddings. while the unlabeled and the balanced data sets stem\\nfrom an adaptive version of calcularis the imbalanced data\\nwas collected using a fixed task sequence. as our method\\nshows larger improvements on the imbalanced data, we be-\\n\\nlieve cnn-sae learned an embedding that is robust beyond\\nadaptive its. the relative improvements of our feature embeddings is smallest for svm on the balanced data set. we\\nbelieve that this is due to ability of the svm to learn complex decision boundaries given sufficient data. however, the\\nability for complex decision boundaries renders svms more\\nvulnerable to class imbalance, yielding performance at random level on the imbalanced data set.\\ncomparison to specialized models. recently, a specialized naive bayes classifier (s-nb) for the detection of\\ndevelopmental dyscalculia (dd) was introduced presenting\\na set of features optimized for the detection of dd [17].\\nthe development of s-nb including the set of features was\\nbased on the balanced data set used in this work. in comparison to s-nb, our approach relies on timing data only\\nand the extracted features are independent of the classification task. we compared the performance of s-nb to our\\ncnn-sae model on both data sets. for the balanced data\\nset we found an auc of 0.94 for the specialized model (snb) compared to an auc of 0.86 for naive bayes using our\\nfeature embedding. on the imbalanced data set we found\\nan auc of 0.67 for s-nb compared to an auc of 0.77 using logistic regression with our feature embedding. these\\nfindings demonstrate that while our feature embedding performs slightly worse on the balanced data set (for which the\\ns-nb was developed), we significantly outperform s-nb by\\n15% on the imbalanced data set, which suggests that our\\nvae model automatically extracts feature embeddings that\\nare more robust than expert features.\\nrobustness on sample size. ideally, a classifier’s performance should gracefully decrease as fewer data is provided.\\na good feature embedding allows a classifier to generalize\\nwell based on few labeled examples because similar samples\\nare clustered together in the feature embedding. we therefore investigated the robustness of the different feature representations with respect to the training set size. for this we\\nused the balanced data set where we varied the training set\\nsize between 7 (10% of the data) and 62 (90% of the data)\\nby random label-stratified sub-sampling. figure 5 compares\\nthe auc of the different feature embeddings over different\\nsizes of the training set. in case of naive bayes and logistic regression our embedding provides superior performance\\nfor all training set sizes. for large enough data sets svm\\nusing the raw feature data (direct, grey) is performing as\\nwell as using our embedding (cnn-sae, blue). however,\\nfor smaller data sets starting at 30 samples the performance\\nof svm based on the raw features declines more rapidly\\ncompared to the svm based on our feature embedding.\\n\\n5.\\n\\nconclusion\\n\\nwe adapted the recently developed variational auto-encoders\\nto educational data for the task of semi-supervised classification of student characteristics. we presented a complete pipeline for semi-supervised classification that can be\\nused with any standard classifier. we demonstrated that extracted structures from large scale unlabeled data sets can\\nsignificantly improve classification performance for different\\nlabeled data sets. our findings show that the improvements\\nare especially pronounced for small or imbalanced data sets.\\nimbalanced data sets typically arise in edm when detecting\\nrelatively rare conditions such as learning disabilities. im-77\\n\\n\\x0c\\x00,\\x00p\\x00e\\x00d\\x00o\\x00d\\x00q\\x00f\\x00h\\x00g\\x00\\x03\\x00g\\x00d\\x00w\\x00d\\x00\\x03\\x00v\\x00h\\x00w\\n\\n\\x001\\x00d\\x00l\\x00y\\x00h\\x00\\x03\\x00%\\x00d\\x00\\\\\\x00h\\x00v\\n\\n\\x00\\x13\\x00\\x11\\x00\\x1c\\x00\\x13\\n\\x00'\\x00l\\x00u\\x00h\\x00f\\x00w\\n\\x00\\x13\\x00\\x11\\x00\\x1b\\x00\\x18\\x003\\x00&\\x00$\\n\\x00\\x13\\x00\\x11\\x00\\x1b\\x00\\x13\\x00.\\x00h\\x00u\\x00q\\x00h\\x00o\\x00\\x03\\x003\\x00&\\x00$\\n\\x00\\x13\\x00\\x11\\x00\\x1a\\x00\\x18\\x00&\\x001\\x001\\x00\\x10\\x006\\x00$\\x00(\\n\\x00\\x13\\x00\\x11\\x00\\x1a\\x00\\x13\\n\\x00\\x13\\x00\\x11\\x00\\x19\\x00\\x18\\n\\x00\\x13\\x00\\x11\\x00\\x19\\x00\\x13\\n\\x00\\x13\\x00\\x11\\x00\\x18\\x00\\x18\\n\\x00\\x13\\x00\\x11\\x00\\x18\\x00\\x13\\n\\n\\x00%\\x00d\\x00o\\x00d\\x00q\\x00f\\x00h\\x00g\\x00\\x03\\x00g\\x00d\\x00w\\x00d\\x00\\x03\\x00v\\x00h\\x00w\\n\\n\\x00$\\x008\\x00&\\n\\n\\x00$\\x008\\x00&\\n\\n\\x00$\\x008\\x00&\\n\\n\\x00/\\x00r\\x00j\\x00l\\x00v\\x00w\\x00l\\x00f\\x00\\x03\\x005\\x00h\\x00j\\x00u\\x00h\\x00v\\x00v\\x00l\\x00r\\x00q\\n\\n\\x00\\x13\\x00\\x11\\x00\\x1c\\x00\\x13\\n\\x00\\x13\\x00\\x11\\x00\\x1b\\x00\\x18\\n\\x00\\x13\\x00\\x11\\x00\\x1b\\x00\\x13\\n\\x00\\x13\\x00\\x11\\x00\\x1a\\x00\\x18\\n\\x00\\x13\\x00\\x11\\x00\\x1a\\x00\\x13\\n\\x00\\x13\\x00\\x11\\x00\\x19\\x00\\x18\\n\\x00\\x13\\x00\\x11\\x00\\x19\\x00\\x13\\n\\x00\\x13\\x00\\x11\\x00\\x18\\x00\\x18\\n\\x00\\x13\\x00\\x11\\x00\\x18\\x00\\x13\\n\\n\\x00,\\x00p\\x00e\\x00d\\x00o\\x00d\\x00q\\x00f\\x00h\\x00g\\x00\\x03\\x00g\\x00d\\x00w\\x00d\\x00\\x03\\x00v\\x00h\\x00w\\n\\n\\x00v\\x00r\\x00x\\x00u\\x00f\\x00h\\n\\n\\x006\\x009\\x000\\n\\n\\x00\\x13\\x00\\x11\\x00\\x1c\\x00\\x13\\n\\x00'\\x00l\\x00u\\x00h\\x00f\\x00w\\n\\x00\\x13\\x00\\x11\\x00\\x1b\\x00\\x18\\x003\\x00&\\x00$\\n\\x00\\x13\\x00\\x11\\x00\\x1b\\x00\\x13\\x00.\\x00h\\x00u\\x00q\\x00h\\x00o\\x00\\x03\\x003\\x00&\\x00$\\n\\x00\\x13\\x00\\x11\\x00\\x1a\\x00\\x18\\x00&\\x001\\x001\\x00\\x10\\x006\\x00$\\x00(\\n\\x00\\x13\\x00\\x11\\x00\\x1a\\x00\\x13\\n\\x00\\x13\\x00\\x11\\x00\\x19\\x00\\x18\\n\\x00\\x13\\x00\\x11\\x00\\x19\\x00\\x13\\n\\x00\\x13\\x00\\x11\\x00\\x18\\x00\\x18\\n\\x00\\x13\\x00\\x11\\x00\\x18\\x00\\x13\\n\\n\\x00%\\x00d\\x00o\\x00d\\x00q\\x00f\\x00h\\x00g\\x00\\x03\\x00g\\x00d\\x00w\\x00d\\x00\\x03\\x00v\\x00h\\x00w\\n\\n\\x00'\\x00l\\x00u\\x00h\\x00f\\x00w\\n\\x003\\x00&\\x00$\\n\\x00.\\x00h\\x00u\\x00q\\x00h\\x00o\\x00\\x03\\x003\\x00&\\x00$\\n\\x00&\\x001\\x001\\x00\\x10\\x006\\x00$\\x00(\\n\\n\\x00,\\x00p\\x00e\\x00d\\x00o\\x00d\\x00q\\x00f\\x00h\\x00g\\x00\\x03\\x00g\\x00d\\x00w\\x00d\\x00\\x03\\x00v\\x00h\\x00w\\n\\n\\x00v\\x00r\\x00x\\x00u\\x00f\\x00h\\n\\n\\x00%\\x00d\\x00o\\x00d\\x00q\\x00f\\x00h\\x00g\\x00\\x03\\x00g\\x00d\\x00w\\x00d\\x00\\x03\\x00v\\x00h\\x00w\\n\\n\\x00v\\x00r\\x00x\\x00u\\x00f\\x00h\\n\\nlogistic regression\\n\\n0\\n\\n10\\n\\n20\\n\\n30\\n\\n40\\n\\n50\\n\\n60\\n\\n70\\n\\n0.90\\ndirect\\n0.85pca\\n0.80kernel pca\\ncnn-sae\\n0.75\\n0.70\\n0.65\\n0.60\\n0.55\\n0.50\\n0\\n10\\n\\nnumber of training samples\\n\\nnaive bayes\\n\\nauc\\n\\n0.90\\n0.85\\n0.80\\n0.75\\n0.70\\n0.65\\n0.60\\n0.55\\n0.50\\n\\nauc\\n\\nauc\\n\\nfigure 4: classification performance for different feature embeddings. our variational auto-encoder (blue)\\noutperforms other embeddings by up to 28% (imbalanced data set) and by up to 7.5% (balanced data set).\\n\\n20\\n\\n30\\n\\n40\\n\\n50\\n\\n60\\n\\n70\\n\\n0.90\\ndirect\\n0.85pca\\n0.80kernel pca\\ncnn-sae\\n0.75\\n0.70\\n0.65\\n0.60\\n0.55\\n0.50\\n0\\n10\\n\\nnumber of training samples\\n\\nsvm\\ndirect\\npca\\nkernel pca\\ncnn-sae\\n\\n20\\n\\n30\\n\\n40\\n\\n50\\n\\n60\\n\\n70\\n\\nnumber of training samples\\n\\nfigure 5: comparison of classifier performance on the balanced data for different training set sizes (moving\\naverage fitted to data points). the features automatically extracted by our variational auto-encoder (blue)\\nmaintain a performance advantage even if the training size shrinks to 7 samples (10% of the original size).\\n\\ntable 1: comparison of our method to alternative embeddings. our approach using a variational auto-encoder\\n(cnn-sae) significantly outperforms other approaches for most cases. the best score for each metric and\\nclassifier is shown in bold. *= statistically significant difference (t-test with bonferroni correction, α = 0.05).\\ndirect\\nauc rmse\\n\\naupr\\n\\nacc.\\n\\npca\\nauc rmse\\n\\naupr\\n\\nacc.\\n\\nkernel pca\\nauc rmse\\n\\naupr\\n\\nacc.\\n\\ncnn-sae\\nauc\\nrmse\\n\\naupr\\n\\nacc.\\n\\nimbalanced data set\\nlogistic regression\\nnaive bayes\\nsvm\\n\\n0.53\\n0.51\\n0.55\\n\\n0.27\\n0.29\\n0.25\\n\\n0.18\\n0.23\\n0.22*\\n\\n0.91\\n0.91\\n0.94\\n\\n0.54\\n0.50\\n0.40\\n\\n0.25\\n0.29\\n0.25\\n\\n0.17\\n0.10\\n0.08\\n\\n0.93\\n0.90\\n0.94\\n\\n0.61\\n0.57\\n0.42\\n\\n0.25\\n0.28\\n0.25\\n\\n0.16\\n0.20\\n0.09\\n\\n0.93\\n0.91\\n0.93\\n\\n0.78*\\n0.70*\\n0.59\\n\\n0.24*\\n0.25*\\n0.25\\n\\n0.28*\\n0.24\\n0.16\\n\\n0.94*\\n0.93*\\n0.94\\n\\nbalanced data set\\nlogistic regression\\nnaive bayes\\nsvm\\n\\n0.80\\n0.80\\n0.81\\n\\n0.44\\n0.49\\n0.42\\n\\n0.82\\n0.80\\n0.84*\\n\\n0.73\\n0.73\\n0.75\\n\\n0.80\\n0.77\\n0.79\\n\\n0.42\\n0.46\\n0.43\\n\\n0.84\\n0.77\\n0.81\\n\\n0.73\\n0.71\\n0.73\\n\\n0.80\\n0.76\\n0.80\\n\\n0.42\\n0.46\\n0.43\\n\\n0.83\\n0.76\\n0.83\\n\\n0.75\\n0.70\\n0.73\\n\\n0.83*\\n0.86*\\n0.83\\n\\n0.40*\\n0.38*\\n0.40*\\n\\n0.84\\n0.86*\\n0.81\\n\\n0.77\\n0.80*\\n0.79*\\n\\nproved classification results with simple classifiers such as\\nlogistic regression might indicate that vaes learn feature\\nembeddings that are interpretable by human experts. in\\nthe future we want to explore the learnt representations and\\ncompare it to traditional categorizations of students (skills,\\nperformance, etc.). additionally, we want to extend our\\nresults to include additional feature types and data reliability indicators to handle missing data. although we trained\\nour networks on comparatively small sample sizes, the presented method scales (due to mini-batch learning) to much\\nlarger data sets (>100k users ) allowing the training of more\\ncomplex vae. moreover, the generative model pθ (x|z) that\\nis part of any vae can be used to produce realistic data\\nsamples [29]. up-sampling of the minority class provides a\\npotential way to improve the decision boundaries for classi-\\n\\nfiers. in contrast to common up-sampling methods such as\\nadasyn [8], vae-based sampling does not require nearest\\nneighbor computations which makes them better applicable\\nto small data sets. preliminary results for random subsets\\nof the balanced data set showed improvements in auc by\\nup-sampling based on vae of 2-3% compared to adasyn.\\nwhile we applied our method to the specific case of detecting\\ndevelopmental dyscalculia, the presented pipeline is generic\\nand thus can be applied to any educational data set and\\nused for the detection of any student characteristic.\\nacknowledgments. this work was supported by eth\\nresearch grant eth-23 13-2.\\n\\n6.\\n\\nreferences78\\n\\n\\x0c[1] y. bengio. practical recommendations for gradientbased training of deep architectures. in neural\\nnetworks: tricks of the trade, pages 437–478. 2012.\\n[2] y. bengio et al. learning deep architectures for ai.\\nfoundations and trends in machine learning, 2009.\\n[3] s. r. bowman, l. vilnis, o. vinyals, a. dai,\\nr. jozefowicz, and s. bengio. generating sentences\\nfrom a continuous space. in proc. conll, pages\\n10–21, 2016.\\n[4] b. butterworth. dyscalculia screener. nelson\\npublishing company ltd., 2003.\\n[5] o. fabius and j. r. van amersfoort. variational\\nrecurrent auto-encoders. in proc. iclr, 2015.\\n[6] i. goodfellow, y. bengio, and a. courville. deep\\nlearning. mit press, 2016.\\n[7] j. haffner, k. baro, p. parzer, and f. resch.\\nheidelberger rechentest: erfassung mathematischer\\nbasiskomptenzen im grundschulalter, 2005.\\n[8] h. he, y. bai, e. a. garcia, and s. li. adasyn:\\nadaptive synthetic sampling approach for imbalanced\\nlearning. in proc. ijcnn, pages 1322–1328, 2008.\\n[9] k. he, x. zhang, s. ren, and j. sun. delving deep\\ninto rectifiers: surpassing human-level performance on\\nimagenet classification. in proc. iccv, pages\\n1026–1034, 2015.\\n[10] i. higgins, l. matthey, x. glorot, a. pal, b. uria,\\nc. blundell, s. mohamed, and a. lerchner. early\\nvisual concept learning with unsupervised deep\\nlearning. arxiv preprint arxiv:1606.05579, 2016.\\n[11] h. hoffmann. kernel pca for novelty detection.\\npattern recognition, pages 863–874, 2007.\\n[12] t. imam, k. ting, and j. kamruzzaman. z-svm: an\\nsvm for improved classification of imbalanced data. ai\\n2006: advances in artificial intelligence, pages\\n264–273, 2006.\\n[13] t. käser, g.-m. baschera, j. kohn, k. kucian,\\nv. richtmann, u. grond, m. gross, and m. von aster.\\ndesign and evaluation of the computer-based training\\nprogram calcularis for enhancing numerical cognition.\\nfrontiers in developmental psychology, 2013.\\n[14] d. kingma and j. ba. adam: a method for stochastic\\noptimization. proc. iclr, 2015.\\n[15] d. p. kingma, s. mohamed, d. j. rezende, and\\nm. welling. semi-supervised learning with deep\\ngenerative models. in proc. nips, pages 3581–3589,\\n2014.\\n[16] d. p. kingma and m. welling. auto-encoding\\nvariational bayes. proc. iclr, 2014.\\n[17] s. klingler, t. käser, a. busetto, b. solenthaler,\\nj. kohn, m. von aster, and m. gross. stealth\\nassessment in its - a study for developmental\\ndyscalculia. in proc. its, pages 79–89, 2016.\\n[18] g. kostopoulos, s. b. kotsiantis, and p. b. pintelas.\\npredicting student performance in distance higher\\neducation using semi-supervised techniques. in proc.\\nmedi, pages 259–270, 2015.\\n[19] i. labutov and h. lipson. web as a textbook:\\ncurating targeted learning paths through the\\nheterogeneous learning resources on the web. in\\nproc. edm, pages 110–118, 2016.\\n[20] y. lecun, y. bengio, and g. hinton. deep learning.\\n\\nnature, pages 436–444, 2015.\\n[21] h. liao, e. mcdermott, and a. senior. large scale\\ndeep neural network acoustic modeling with\\nsemi-supervised training data for youtube video\\ntranscription. in proc. asru, pages 368–373, 2013.\\n[22] w. min, b. w. mott, j. p. rowe, and j. c. lester.\\nleveraging semi-supervised learning to predict student\\nproblem-solving performance in narrative-centered\\nlearning environments. in proc. its, pages 664–665,\\n2014.\\n[23] d. j. rezende, s. mohamed, and d. wierstra.\\nstochastic backpropagation and approximate\\ninference in deep generative models. in proc. icml,\\npages 1278–1286, 2014.\\n[24] b. schölkopf, a. smola, and k.-r. müller. kernel\\nprincipal component analysis. in proc. icann, pages\\n583–588, 1997.\\n[25] c. k. sønderby, t. raiko, l. maaløe, s. k. sønderby,\\nand o. winther. ladder variational autoencoders. in\\nproc. nips, pages 3738–3746, 2016.\\n[26] n. srivastava, g. e. hinton, a. krizhevsky,\\ni. sutskever, and r. salakhutdinov. dropout: a simple\\nway to prevent neural networks from overfitting.\\njmlr, pages 1929–1958, 2014.\\n[27] v. tam, e. y. lam, s. fung, w. fok, and a. h. yuen.\\nenhancing educational data mining techniques on\\nonline educational resources with a semi-supervised\\nlearning approach. in proc. tale, pages 203–206,\\n2015.\\n[28] j. turian, l. ratinov, and y. bengio. word\\nrepresentations: a simple and general method for\\nsemi-supervised learning. in proc. acl, pages\\n384–394, 2010.\\n[29] a. van den oord, n. kalchbrenner, l. espeholt,\\no. vinyals, a. graves, et al. conditional image\\ngeneration with pixelcnn decoders. in proc. nips,\\npages 4790–4798, 2016.\\n[30] w. j. van der linden. a lognormal model for response\\ntimes on test items. journal of educational and\\nbehavioral statistics, 31(2):181–204, 2006.\\n[31] m. von aster, l. rauscher, k. kucian, t. käser,\\nu. mccaskey, and j. kohn. calcularis - evaluation of\\na computer-based learning program for enhancing\\nnumerical cognition for children with developmental\\ndyscalculia, 2015. 62nd annual meeting of the\\namerican academy of child and adolescent\\npsychiatry.\\n[32] m. von aster, m. w. zulauf, and r. horn.\\nneuropsychologische testbatterie für\\nzahlenverarbeitung und rechnen bei kindern:\\nzareki-r. pearson, 2006.\\n[33] m. g. von aster and r. s. shalev. number\\ndevelopment and developmental dyscalculia.\\ndevelopmental medicine & child neurology, pages\\n868–873, 2007.\\n[34] c. xu, d. tao, and c. xu. a survey on multi-view\\nlearning. neural comput. appl., pages 2031–2038,\\n2013.\\n[35] x. zhu. semi-supervised learning literature survey.\\ntechnical report, university of wisconsin-madison,\\n2006.\"]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\f",
      "zone out no more: mitigating mind wandering during\n",
      "computerized reading\n",
      "sidney k. d’mello, caitlin mills, robert bixler, & nigel bosch\n",
      "university of notre dame\n",
      "118 haggar hall\n",
      "notre dame, in 46556, usa\n",
      "sdmello@nd.edu\n",
      "\n",
      "abstract\n",
      "mind wandering, defined as shifts in attention from task-related\n",
      "processing to task-unrelated thoughts, is a ubiquitous\n",
      "phenomenon that has a negative influence on performance and\n",
      "productivity in many contexts, including learning. we propose\n",
      "that next-generation learning technologies should have some\n",
      "mechanism to detect and respond to mind wandering in real-time.\n",
      "towards this end, we developed a technology that automatically\n",
      "detects mind wandering from eye-gaze during learning from\n",
      "instructional texts. when mind wandering is detected, the\n",
      "technology intervenes by posing just-in-time questions and\n",
      "encouraging re-reading as needed. after multiple rounds of\n",
      "iterative refinement, we summatively compared the technology to\n",
      "a yoked-control in an experiment with 104 par\n",
      "\f",
      "zone out no more: mitigating mind wandering during\n",
      "computerized reading\n",
      "sidney k. d’mello, caitlin mills, robert bixler, & nigel bosch\n",
      "university of notre dame\n",
      "118 haggar hall\n",
      "notre dame, in 46556, usa\n",
      "sdmello@nd.edu\n",
      "\n",
      "abstract\n",
      "mind wandering, defined as shifts in attention from task-related\n",
      "processing to task-unrelated thoughts, is a ubiquitous\n",
      "phenomenon that has a negative influence on performance and\n",
      "productivity in many contexts, including learning. we propose\n",
      "that next-generation learning technologies should have some\n",
      "mechanism to detect and respond to mind wandering in real-time.\n",
      "towards this end, we developed a technology that automatically\n",
      "detects mind wandering from eye-gaze during learning from\n",
      "instructional texts. when mind wandering is detected, the\n",
      "technology intervenes by posing just-in-time questions and\n",
      "encouraging re-reading as needed. after multiple rounds of\n",
      "iterative refinement, we summatively compared the technology to\n",
      "a yoked-control in an experiment with 104 par\n",
      "\f",
      "zone out no more: mitigating mind wandering during\n",
      "computerized reading\n",
      "sidney k. d’mello, caitlin mills, robert bixler, & nigel bosch\n",
      "university of notre dame\n",
      "118 haggar hall\n",
      "notre dame, in 46556, usa\n",
      "sdmello@nd.edu\n",
      "\n",
      "abstract\n",
      "mind wandering, defined as shifts in attention from task-related\n",
      "processing to task-unrelated thoughts, is a ubiquitous\n",
      "phenomenon that has a negative influence on performance and\n",
      "productivity in many contexts, including learning. we propose\n",
      "that next-generation learning technologies should have some\n",
      "mechanism to detect and respond to mind wandering in real-time.\n",
      "towards this end, we developed a technology that automatically\n",
      "detects mind wandering from eye-gaze during learning from\n",
      "instructional texts. when mind wandering is detected, the\n",
      "technology intervenes by posing just-in-time questions and\n",
      "encouraging re-reading as needed. after multiple rounds of\n",
      "iterative refinement, we summatively compared the technology to\n",
      "a yoked-control in an experiment with 104 par\n",
      "\f",
      "zone out no more: mitigating mind wandering during\n",
      "computerized reading\n",
      "sidney k. d’mello, caitlin mills, robert bixler, & nigel bosch\n",
      "university of notre dame\n",
      "118 haggar hall\n",
      "notre dame, in 46556, usa\n",
      "sdmello@nd.edu\n",
      "\n",
      "abstract\n",
      "mind wandering, defined as shifts in attention from task-related\n",
      "processing to task-unrelated thoughts, is a ubiquitous\n",
      "phenomenon that has a negative influence on performance and\n",
      "productivity in many contexts, including learning. we propose\n",
      "that next-generation learning technologies should have some\n",
      "mechanism to detect and respond to mind wandering in real-time.\n",
      "towards this end, we developed a technology that automatically\n",
      "detects mind wandering from eye-gaze during learning from\n",
      "instructional texts. when mind wandering is detected, the\n",
      "technology intervenes by posing just-in-time questions and\n",
      "encouraging re-reading as needed. after multiple rounds of\n",
      "iterative refinement, we summatively compared the technology to\n",
      "a yoked-control in an experiment with 104 par\n",
      "\f",
      "zone out no more: mitigating mind wandering during\n",
      "computerized reading\n",
      "sidney k. d’mello, caitlin mills, robert bixler, & nigel bosch\n",
      "university of notre dame\n",
      "118 haggar hall\n",
      "notre dame, in 46556, usa\n",
      "sdmello@nd.edu\n",
      "\n",
      "abstract\n",
      "mind wandering, defined as shifts in attention from task-related\n",
      "processing to task-unrelated thoughts, is a ubiquitous\n",
      "phenomenon that has a negative influence on performance and\n",
      "productivity in many contexts, including learning. we propose\n",
      "that next-generation learning technologies should have some\n",
      "mechanism to detect and respond to mind wandering in real-time.\n",
      "towards this end, we developed a technology that automatically\n",
      "detects mind wandering from eye-gaze during learning from\n",
      "instructional texts. when mind wandering is detected, the\n",
      "technology intervenes by posing just-in-time questions and\n",
      "encouraging re-reading as needed. after multiple rounds of\n",
      "iterative refinement, we summatively compared the technology to\n",
      "a yoked-control in an experiment with 104 par\n",
      "\f",
      "zone out no more: mitigating mind wandering during\n",
      "computerized reading\n",
      "sidney k. d’mello, caitlin mills, robert bixler, & nigel bosch\n",
      "university of notre dame\n",
      "118 haggar hall\n",
      "notre dame, in 46556, usa\n",
      "sdmello@nd.edu\n",
      "\n",
      "abstract\n",
      "mind wandering, defined as shifts in attention from task-related\n",
      "processing to task-unrelated thoughts, is a ubiquitous\n",
      "phenomenon that has a negative influence on performance and\n",
      "productivity in many contexts, including learning. we propose\n",
      "that next-generation learning technologies should have some\n",
      "mechanism to detect and respond to mind wandering in real-time.\n",
      "towards this end, we developed a technology that automatically\n",
      "detects mind wandering from eye-gaze during learning from\n",
      "instructional texts. when mind wandering is detected, the\n",
      "technology intervenes by posing just-in-time questions and\n",
      "encouraging re-reading as needed. after multiple rounds of\n",
      "iterative refinement, we summatively compared the technology to\n",
      "a yoked-control in an experiment with 104 par\n",
      "\f",
      "zone out no more: mitigating mind wandering during\n",
      "computerized reading\n",
      "sidney k. d’mello, caitlin mills, robert bixler, & nigel bosch\n",
      "university of notre dame\n",
      "118 haggar hall\n",
      "notre dame, in 46556, usa\n",
      "sdmello@nd.edu\n",
      "\n",
      "abstract\n",
      "mind wandering, defined as shifts in attention from task-related\n",
      "processing to task-unrelated thoughts, is a ubiquitous\n",
      "phenomenon that has a negative influence on performance and\n",
      "productivity in many contexts, including learning. we propose\n",
      "that next-generation learning technologies should have some\n",
      "mechanism to detect and respond to mind wandering in real-time.\n",
      "towards this end, we developed a technology that automatically\n",
      "detects mind wandering from eye-gaze during learning from\n",
      "instructional texts. when mind wandering is detected, the\n",
      "technology intervenes by posing just-in-time questions and\n",
      "encouraging re-reading as needed. after multiple rounds of\n",
      "iterative refinement, we summatively compared the technology to\n",
      "a yoked-control in an experiment with 104 par\n",
      "\f",
      "zone out no more: mitigating mind wandering during\n",
      "computerized reading\n",
      "sidney k. d’mello, caitlin mills, robert bixler, & nigel bosch\n",
      "university of notre dame\n",
      "118 haggar hall\n",
      "notre dame, in 46556, usa\n",
      "sdmello@nd.edu\n",
      "\n",
      "abstract\n",
      "mind wandering, defined as shifts in attention from task-related\n",
      "processing to task-unrelated thoughts, is a ubiquitous\n",
      "phenomenon that has a negative influence on performance and\n",
      "productivity in many contexts, including learning. we propose\n",
      "that next-generation learning technologies should have some\n",
      "mechanism to detect and respond to mind wandering in real-time.\n",
      "towards this end, we developed a technology that automatically\n",
      "detects mind wandering from eye-gaze during learning from\n",
      "instructional texts. when mind wandering is detected, the\n",
      "technology intervenes by posing just-in-time questions and\n",
      "encouraging re-reading as needed. after multiple rounds of\n",
      "iterative refinement, we summatively compared the technology to\n",
      "a yoked-control in an experiment with 104 par\n",
      "\f",
      "zone out no more: mitigating mind wandering during\n",
      "computerized reading\n",
      "sidney k. d’mello, caitlin mills, robert bixler, & nigel bosch\n",
      "university of notre dame\n",
      "118 haggar hall\n",
      "notre dame, in 46556, usa\n",
      "sdmello@nd.edu\n",
      "\n",
      "abstract\n",
      "mind wandering, defined as shifts in attention from task-related\n",
      "processing to task-unrelated thoughts, is a ubiquitous\n",
      "phenomenon that has a negative influence on performance and\n",
      "productivity in many contexts, including learning. we propose\n",
      "that next-generation learning technologies should have some\n",
      "mechanism to detect and respond to mind wandering in real-time.\n",
      "towards this end, we developed a technology that automatically\n",
      "detects mind wandering from eye-gaze during learning from\n",
      "instructional texts. when mind wandering is detected, the\n",
      "technology intervenes by posing just-in-time questions and\n",
      "encouraging re-reading as needed. after multiple rounds of\n",
      "iterative refinement, we summatively compared the technology to\n",
      "a yoked-control in an experiment with 104 par\n",
      "\f",
      "zone out no more: mitigating mind wandering during\n",
      "computerized reading\n",
      "sidney k. d’mello, caitlin mills, robert bixler, & nigel bosch\n",
      "university of notre dame\n",
      "118 haggar hall\n",
      "notre dame, in 46556, usa\n",
      "sdmello@nd.edu\n",
      "\n",
      "abstract\n",
      "mind wandering, defined as shifts in attention from task-related\n",
      "processing to task-unrelated thoughts, is a ubiquitous\n",
      "phenomenon that has a negative influence on performance and\n",
      "productivity in many contexts, including learning. we propose\n",
      "that next-generation learning technologies should have some\n",
      "mechanism to detect and respond to mind wandering in real-time.\n",
      "towards this end, we developed a technology that automatically\n",
      "detects mind wandering from eye-gaze during learning from\n",
      "instructional texts. when mind wandering is detected, the\n",
      "technology intervenes by posing just-in-time questions and\n",
      "encouraging re-reading as needed. after multiple rounds of\n",
      "iterative refinement, we summatively compared the technology to\n",
      "a yoked-control in an experiment with 104 par\n",
      "\f",
      "zone out no more: mitigating mind wandering during\n",
      "computerized reading\n",
      "sidney k. d’mello, caitlin mills, robert bixler, & nigel bosch\n",
      "university of notre dame\n",
      "118 haggar hall\n",
      "notre dame, in 46556, usa\n",
      "sdmello@nd.edu\n",
      "\n",
      "abstract\n",
      "mind wandering, defined as shifts in attention from task-related\n",
      "processing to task-unrelated thoughts, is a ubiquitous\n",
      "phenomenon that has a negative influence on performance and\n",
      "productivity in many contexts, including learning. we propose\n",
      "that next-generation learning technologies should have some\n",
      "mechanism to detect and respond to mind wandering in real-time.\n",
      "towards this end, we developed a technology that automatically\n",
      "detects mind wandering from eye-gaze during learning from\n",
      "instructional texts. when mind wandering is detected, the\n",
      "technology intervenes by posing just-in-time questions and\n",
      "encouraging re-reading as needed. after multiple rounds of\n",
      "iterative refinement, we summatively compared the technology to\n",
      "a yoked-control in an experiment with 104 par\n",
      "\f",
      "zone out no more: mitigating mind wandering during\n",
      "computerized reading\n",
      "sidney k. d’mello, caitlin mills, robert bixler, & nigel bosch\n",
      "university of notre dame\n",
      "118 haggar hall\n",
      "notre dame, in 46556, usa\n",
      "sdmello@nd.edu\n",
      "\n",
      "abstract\n",
      "mind wandering, defined as shifts in attention from task-related\n",
      "processing to task-unrelated thoughts, is a ubiquitous\n",
      "phenomenon that has a negative influence on performance and\n",
      "productivity in many contexts, including learning. we propose\n",
      "that next-generation learning technologies should have some\n",
      "mechanism to detect and respond to mind wandering in real-time.\n",
      "towards this end, we developed a technology that automatically\n",
      "detects mind wandering from eye-gaze during learning from\n",
      "instructional texts. when mind wandering is detected, the\n",
      "technology intervenes by posing just-in-time questions and\n",
      "encouraging re-reading as needed. after multiple rounds of\n",
      "iterative refinement, we summatively compared the technology to\n",
      "a yoked-control in an experiment with 104 par\n",
      "\f",
      "zone out no more: mitigating mind wandering during computerized reading sidney k. d’mello, caitlin mills, robert bixler, & nigel bosch university of notre dame 118 haggar hall notre dame, in 46556, usa sdmello@nd.edu  abstract mind wandering, defined as shifts in attention from task-related processing to task-unrelated thoughts, is a ubiquitous phenomenon that has a negative influence on performance and productivity in many contexts, including learning. we propose that next-generation learning technologies should have some mechanism to detect and respond to mind wandering in real-time. towards this end, we developed a technology that automatically detects mind wandering from eye-gaze during learning from instructional texts. when mind wandering is detected, the technology intervenes by posing just-in-time questions and encouraging re-reading as needed. after multiple rounds of iterative refinement, we summatively compared the technology to a yoked-control in an experiment with 104 par\n",
      "\f",
      "zone out no more: mitigating mind wandering during computerized reading sidney k. d’mello, caitlin mills, robert bixler, & nigel bosch university of notre dame 118 haggar hall notre dame, in 46556, usa sdmello@nd.edu  abstract mind wandering, defined as shifts in attention from task-related processing to task-unrelated thoughts, is a ubiquitous phenomenon that has a negative influence on performance and productivity in many contexts, including learning. we propose that next-generation learning technologies should have some mechanism to detect and respond to mind wandering in real-time. towards this end, we developed a technology that automatically detects mind wandering from eye-gaze during learning from instructional texts. when mind wandering is detected, the technology intervenes by posing just-in-time questions and encouraging re-reading as needed. after multiple rounds of iterative refinement, we summatively compared the technology to a yoked-control in an experiment with 104 par\n",
      "\f",
      "zone out no more: mitigating mind wandering during computerized reading sidney k. d’mello, caitlin mills, robert bixler, & nigel bosch university of notre dame 118 haggar hall notre dame, in 46556, usa sdmello@nd.edu  abstract mind wandering, defined as shifts in attention from task-related processing to task-unrelated thoughts, is a ubiquitous phenomenon that has a negative influence on performance and productivity in many contexts, including learning. we propose that next-generation learning technologies should have some mechanism to detect and respond to mind wandering in real-time. towards this end, we developed a technology that automatically detects mind wandering from eye-gaze during learning from instructional texts. when mind wandering is detected, the technology intervenes by posing just-in-time questions and encouraging re-reading as needed. after multiple rounds of iterative refinement, we summatively compared the technology to a yoked-control in an experiment with 104 par\n",
      "\f",
      "zone out no more: mitigating mind wandering during computerized reading sidney k. d’mello, caitlin mills, robert bixler, & nigel bosch university of notre dame 118 haggar hall notre dame, in 46556, usa sdmello@nd.edu  abstract mind wandering, defined as shifts in attention from task-related processing to task-unrelated thoughts, is a ubiquitous phenomenon that has a negative influence on performance and productivity in many contexts, including learning. we propose that next-generation learning technologies should have some mechanism to detect and respond to mind wandering in real-time. towards this end, we developed a technology that automatically detects mind wandering from eye-gaze during learning from instructional texts. when mind wandering is detected, the technology intervenes by posing just-in-time questions and encouraging re-reading as needed. after multiple rounds of iterative refinement, we summatively compared the technology to a yoked-control in an experiment with 104 par\n",
      "\f",
      "zone out no more: mitigating mind wandering during computerized reading sidney k. d’mello, caitlin mills, robert bixler, & nigel bosch university of notre dame 118 haggar hall notre dame, in 46556, usa sdmello@nd.edu  abstract mind wandering, defined as shifts in attention from task-related processing to task-unrelated thoughts, is a ubiquitous phenomenon that has a negative influence on performance and productivity in many contexts, including learning. we propose that next-generation learning technologies should have some mechanism to detect and respond to mind wandering in real-time. towards this end, we developed a technology that automatically detects mind wandering from eye-gaze during learning from instructional texts. when mind wandering is detected, the technology intervenes by posing just-in-time questions and encouraging re-reading as needed. after multiple rounds of iterative refinement, we summatively compared the technology to a yoked-control in an experiment with 104 par\n"
     ]
    }
   ],
   "source": [
    "# 5) replace carriage returns (i.e., \"\\n\") with a white space\n",
    "# check that the result looks okay by printing the \n",
    "# first 1000 characters of the 1st doc:\n",
    "for i,doc in enumerate(documents):\n",
    "    documents[i] = doc.replace('\\n', ' ')\n",
    "    print(documents[12][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\f",
      "zone out no more  mitigating mind wandering during computerized reading sidney k  d mello  caitlin mills  robert bixler    nigel bosch university of notre dame 118 haggar hall notre dame  in 46556  usa sdmello nd edu  abstract mind wandering  defined as shifts in attention from task related processing to task unrelated thoughts  is a ubiquitous phenomenon that has a negative influence on performance and productivity in many contexts  including learning  we propose that next generation learning technologies should have some mechanism to detect and respond to mind wandering in real time  towards this end  we developed a technology that automatically detects mind wandering from eye gaze during learning from instructional texts  when mind wandering is detected  the technology intervenes by posing just in time questions and encouraging re reading as needed  after multiple rounds of iterative refinement  we summatively compared the technology to a yoked control in an experiment with 104 par\n"
     ]
    }
   ],
   "source": [
    "# 6) replace the punctation below by a white space\n",
    "# check that the result looks okay \n",
    "# (e.g., by print the first 1000 characters of the 1st doc)\n",
    "import re\n",
    "punctuation = ['.', '...', '!', '#', '\"', '%', '$', \"'\", '&', ')', \n",
    "               '(', '+', '*', '-', ',', '/', '.', ';', ':', '=', \n",
    "               '<', '?', '>', '@', '\",', '\".', '[', ']', '\\\\', ',',\n",
    "               '_', '^', '`', '{', '}', '|', '~', '−', '”', '“', '’']\n",
    "\n",
    "for i in range(len(documents)):\n",
    "    for p in punctuation:\n",
    "        documents[i] = documents[i].replace(p, \" \")\n",
    "print(documents[12][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) remove numbers by either a white space or the word \"number\"\n",
    "# again, print the first 1000 characters of the first document\n",
    "# to check that you're doing the right thing\n",
    "for i,doc in enumerate(documents):\n",
    "    for x in range(0, 10):\n",
    "        doc = doc.replace(str(x),'')\n",
    "    documents[i] = doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zone mitigating mind wandering computerized reading sidney k d mello caitlin mills robert bixler nigel bosch university notre dame haggar hall notre dame usa sdmello nd edu abstract mind wandering defined shifts attention task related processing task unrelated thoughts ubiquitous phenomenon negative influence performance productivity many contexts including learning propose next generation learning technologies mechanism detect respond mind wandering real time towards end developed technology automatically detects mind wandering eye gaze learning instructional texts mind wandering detected technology intervenes posing time questions encouraging re reading needed multiple rounds iterative refinement summatively compared technology yoked control experiment participants key dependent variable performance post reading comprehension assessment results suggest technology successful correcting comprehension deficits attributed mind wandering d sigma specific conditions thereby highlighting po\n"
     ]
    }
   ],
   "source": [
    "# 8) Remove the stop words below from our documents\n",
    "# print the first 1000 characters of the first document\n",
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', \n",
    "              'ourselves', 'you', 'your', 'yours', 'yourself', \n",
    "              'yourselves', 'he', 'him', 'his', 'himself', 'she', \n",
    "              'her', 'hers', 'herself', 'it', 'its', 'itself', \n",
    "              'they', 'them', 'their', 'theirs', 'themselves', \n",
    "              'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "              'these', 'those', 'am', 'is', 'are', 'was', 'were', \n",
    "              'be', 'been', 'being', 'have', 'has', 'had', 'having', \n",
    "              'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', \n",
    "              'but', 'if', 'or', 'because', 'as', 'until', 'while', \n",
    "              'of', 'at', 'by', 'for', 'with', 'about', 'against', \n",
    "              'between', 'into', 'through', 'during', 'before', \n",
    "              'after', 'above', 'below', 'to', 'from', 'up', 'down', \n",
    "              'in', 'out', 'on', 'off', 'over', 'under', 'again', \n",
    "              'further', 'then', 'once', 'here', 'there', 'when', \n",
    "              'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
    "              'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
    "              'nor', 'not', 'only', 'own', 'same', 'so', 'than', \n",
    "              'too', 'very', 's', 't', 'can', 'will', \n",
    "              'just', 'don', 'should', 'now']\n",
    "\n",
    "for i,doc in enumerate(documents):\n",
    "    doc = [w for w in doc.split() if w not in stop_words]\n",
    "    doc = \" \".join(doc)\n",
    "    documents[i] = doc \n",
    "print(documents[12][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zone out more mitigating mind wandering during computerized reading sidney mello caitlin mills robert bixler nigel bosch university notre dame haggar hall notre dame usa sdmello edu abstract mind wandering defined shifts attention from task related processing task unrelated thoughts ubiquitous phenomenon that has negative influence performance and productivity many contexts including learning propose that next generation learning technologies should have some mechanism detect and respond mind wandering real time towards this end developed technology that automatically detects mind wandering from eye gaze during learning from instructional texts when mind wandering detected the technology intervenes posing just time questions and encouraging reading needed after multiple rounds iterative refinement summatively compared the technology yoked control experiment with participants the key dependent variable was performance post reading comprehension assessment our results suggest that the te\n"
     ]
    }
   ],
   "source": [
    "# 9) remove words with one and two characters (e.g., 'd', 'er', etc.)\n",
    "# print the first 1000 characters of the first document\n",
    "for i,doc in enumerate(documents):\n",
    "    #documents is a list of strings\n",
    "    new_d = []\n",
    "    for word in doc.split():\n",
    "        if len(word) > 2: \n",
    "            new_d.append(word)\n",
    "    doc = \" \".join(new_d)\n",
    "    documents[i] = doc\n",
    "print(documents[12][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) package all of your work above into a function that cleans a given document\n",
    "\n",
    "def clean_list_of_documents(documents):\n",
    "    \n",
    "    cleaned_docs = []\n",
    "    for i,doc in enumerate(documents):\n",
    "        new_d = []\n",
    "        for word in doc.split():\n",
    "            if len(word) > 2: \n",
    "                new_d.append(word)\n",
    "        doc = \" \".join(new_d)\n",
    "        documents[i] = doc\n",
    "    for i,doc in enumerate(documents):\n",
    "        for x in range(0, 10):\n",
    "            doc = doc.replace(str(x),'')\n",
    "            documents[i] = doc\n",
    "    for i,doc in enumerate(documents):\n",
    "        doc = [w for w in doc.split() if w not in stop_words]\n",
    "        doc = \" \".join(doc)\n",
    "        documents[i] = doc \n",
    "    for doc in documents: \n",
    "        start = doc.rfind('\\nabstract\\n')+len('abstract')\n",
    "        end = doc.rfind('reference')\n",
    "        doc = abstract.append(doc[start:end])\n",
    "    return cleaned_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assessing computer literacy adults low literacy skills andrew olney institute intelligent systems university memphis memphis, aolney@memphis.edu daphne greenberg department educational psychology, special education, communication disorders georgia state university atlanta, dgreenberg@gsu.edu abstract adaptive learning technologies hold great promise improving reading skills adults low literacy, adults low literacy skills typically low computer literacy skills. order determine whether adults low literacy skills would able use intelligent tutoring system reading comprehension, adapted task computer literacy assessment delivered adults reading skills rd th grade levels. paper presents four analyses data. first, report pass/fail data natively exported assessment particular computer-based tasks. second, undertook goms analysis computer-based task, predict task completion time skilled user, found negatively correlated proportion correct item, r() −., .. third, used goms task decomposition de\n"
     ]
    }
   ],
   "source": [
    "# 11a) reimport your raw data using the code in 2)\n",
    "documents = []\n",
    "for paper in papers:\n",
    "    file=open(paper,'r')\n",
    "    documents.append(file.read())\n",
    "documents[12]\n",
    "        \n",
    "# 11b) clean your files using the function above\n",
    "clean_list_of_documents(documents)\n",
    "\n",
    "# 11c) print the first 1000 characters of the first document\n",
    "print(documents[11][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Build your list of vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list of words (i.e., the vocabulary) is going to become the columns of your matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) Describe why we need to figure out the vocabulary used in our corpus (refer back to Sherin's paper, and explain in your own words): \n",
    "We can use the vocabulary to compute vectors for a passage, meaning we can see which words are used most in conjunction with others and hold the most meaning for large bodies of work and documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grade prediction temporal course-wise influence zhiyun ren computer science george mason university university drive, fairfax, zren@gmu.edu xia ning computer information science indiana university purdue university indianapolis university blvd, indianapolis, huzefa rangwala computer science george mason university university drive, fairfax, rangwala@cs.gmu.edu xning@cs.iupui.edu abstract critical need develop new educational technology applications analyze data collected universities ensure students graduate timely fashion years); well prepared jobs respective fields study. paper, present novel approach analyzing historical educational records large, public university perform next-term grade prediction; i.e., estimate grades student get course he/she enroll next term. accurate next-term grade prediction holds promise better student degree planning, personalized advising automated interventions ensure students stay track chosen degree program graduate time. present factorization-based approach called matrix factorization temporal course-wise influence incorporates course-wise influence effects temporal effects grade prediction. model, students courses represented latent “knowledge” space. grade student course modeled similarity latent representation “knowledge” space. course-wise influence considered additional factor grade prediction. experimental results show proposed method outperforms several baseline approaches infer meaningful patterns pairs courses within academic programs. keywords next-term grade prediction, course-wise influence, temporal effect, latent factor introduction data analytics forefront innovation several today’s popular educational technologies (edtech) []. currently, one grand challenges facing higher education problem student retention graduation []. critical need develop new edtech applications analyze data collected universities ensure students graduate timely fashion years), well prepared jobs respective fields study. end, several universities deploy suite software tools. example, degree planners assist students deciding majors fields study, choosing sequence courses within chosen major providing advice achieving career learning objectives. early warning systems [] inform advisors/students progress, additionally provide cues intervention students risk failing one courses dropping program study. work, focus problem next-term grade prediction goal predict grade student expected obtain course he/she may enroll next term (future). past years, several algorithms developed analyze educational data, including matrix factorization (mf) algorithms inspired recommender system research. methods decompose student-course (or student-task) grade matrix two low-rank matrices, prediction grade student untaken course calculated product corresponding vectors two decomposed matrices [, ]. traditional algorithms shown strong ability deal sparse datasets [] extensions incorporated temporal dynamic information []. setting, consider student’s knowledge continuously enriched taking sequence courses; important incorporate dynamic influence sequential courses within models. therefore, present novel approach referred matrix factorization temporal course-wise influence (mftci) model predict next term student grades. mftci considers student’s grade certain course determined two components: (i) student’s competence respect course’s topics, content requirement, etc., (ii) student’s previous performance courses. performed comprehensive set experiments various datasets. experimental results show proposed method outperforms several state-of-the-art methods. main contributions work paper follows: model incorporate temporal course-wise influence addition matrix factorization grade http://www.blackboard.com/mobilelearning/planner.aspx prediction. experimental results demonstrate significant improvement course-wise influence. model successfully captures meaningful coursewise influences correlate course content. learned influences pairs courses help understanding pre-requisite structures within programs tuning academic program chains. related work past years, several methods developed model student behavior academic performance [, ], gain improvement learning outcomes []. methods influenced recommender system (rs) research [], including collaborative filtering (cf) [] matrix factorization [], attracted increasing attention educational mining applications relate student grade prediction [] in-class assessment prediction []. sweeney et. al. [, ] performed extensive study several recommender system approaches including svd, svd-knn factorization machine (fm) predict next-term grade performance. inspired contentbased recommendation [] approaches, polyzou et. al. [] addressed future course grade prediction problem three approaches: course-specific regression, student-specific regression course-specific matrix factorization. moreover, neighborhood-based approaches [, predict grades based student similarities, i.e., first identify similar students use grades estimate grades students similar profiles. order capture changing user dynamics time rs, various dynamic models developed. many models based matrix factorization state space models. sun et. al. [, ] model user preference change using state space model latent user factors, estimate user factors time using noncausal kalman filters. similarly, chua et.al. [] apply linear dynamical systems (lds) non-negative matrix factorization (nmf) model user dynamics. et. al. [] encapsulate temporal relationships within non-negative matrix formulation. zhang et. al. [] learn explicit transition matrix latent factors user, estimate user item latent factors transition matrices within bayesian framework. popular methods dynamic modeling include time-weighting similarity decaying [], tensor factorization [] point processes []. method proposed paper tackle challenges next-term grade prediction relates evolvement student knowledge taking sequence courses. key contribution involves incorporate temporal course-wise relationships within approach. additionally, proposed approach learns pairwise relationships courses help understanding pre-requisite structures within programs tuning academic program chains. preliminaries . problem statement notations formally, student-course grades represented series matrices {g ..., terms. row represents student, column represents course, value denoted gs,c represents grade student got course term (gs,c (, ], gs,c indicates student take course term add small value failing grade distinguish score situation.). student-course grades tth term represented ti= size number students number courses. given database (student, course, grade) term (i.e., next-term grade prediction problem predict grades student courses might enroll next term simplify notations, specifically stated paper, use gs,c denote gs,c testing set (student, course, grade) triples tth term, represented matrix rows grade matrices representing student simply represented g(s, specific courses student grade row given g(s, :). paper, vectors (e.g., uts represented bold lower-case letters matrices (e.g., represented upper-case letters. column vectors represented transpose supscriptt otherwise default row vectors. predicted/approximated value denoted head. methods . temporal course-wise influence consider student grade certain course denoted gs,c determined two factors. first factor student competence respect course c’s topics, content requirement. modeled latent factor model, competence captured using size-k latent factor c’s topics contents captured using size-k latent factor latent space competence modeled “similarity” via dot product (i.e., uts second factor previous performance student courses. hypothesize course positive influence course student achieved high grade tends high grade hypothesis, model second factor product performance student previous “related” course pairwise course relationships learned formulation. note consider pairwise course influence time independent, i.e., influence one course another change time. however, impact previous performance/grades modeled using decay function time. taking two factors, estimated grade given follows: g̃s,c uts e−α e−α |c ∈gt (s,:) a(c c)gs,c |gt (s, :)| ∆(t −) c ∈gt (s,:) () a(c c)gs,c |gt (s, :)| ∆(t −) a(c influence (s, :)/gt (s, subset courses courses taken first/second previous terms, |gt (s, :)|/|gt (s, :)| number taken courses. e−α /e−α denote time-decay factors. equation consider previous two terms. previous terms included even stronger time-decay factors. given grade estimation equation formulate grade prediction problem term following optimization problem, min u,v,a (gs,c g̃s,c (ku kf kf s,c kak∗ λkak` e−α |gt (s,:)| s,ci e−α (s,:)| s,ci (if taken term (if taken term )] min kz kf ρ(tr(ut ))) apply admm [] technique equation reformulating optimization problem follows, (gs,c g̃s,c (ku kf kf s,c kz λkz k` (ka kf kf +ρ(tr(ut ))) () closed-form solution problem () (x) soft-thresholding function shrinks singular values threshold is, (x) diag((σ α)+ .. optimization algorithm mftci s.t., s,cj step update problem becomes latent non-negative student factors course factors, respectively; kak∗ nuclear norm induce low rank; kak` norm introduce sparsity addition, non-negativity constraint enforce positive influence across courses. u,v,a,u ,u ,z ,z a(ci a(ci [ρ(a(ci (ci ρ(a(ci (ci ρu (ci ρu (ci (gs,cj g̃s,cj () projection [, +∞), learning rate. s.t., min using gradient descent, elements updated follows. () singular value decomposition (x)+ max(x, ). problem becomes min λkz k` kf ρ(tr(ut )(a () () closed-form solution +ρ(tr(ut ))) () (x) soft-thresholding function shrinks values threshold is, a≥ (x) )+ () two auxiliary variables, two dual variables. variables solved via alternating approach follows. ()+ defined equation step update fixing variables step update updated based standard admm updates: solving problem becomes classical matrix factorization problem: min u,v (fs,c uts kus k kvc k s,c () fs,c gs,c ∆(t ∆(t (see ). matrix factorization problem solved using alternating minimization. step update fixing variables solving problem becomes min s.t., (gs,c g̃s,c (ka kf kf s,c +ρ(tr(ut ))) ρ(tr(ut ))) a≥ () addition, conduct computational complexity analysis mftci put appendix. experiments . dataset description evaluated method student grade records obtained george mason university (gmu) fall spring . period included data , transfer students , first-time freshmen (non-transfer i.e., students begin study gmu) across majors enrolled , courses. specifically, extracted data six large diverse majors non-transfer transfer students. majors include: (i) applied information technology (ait), (ii) table dataset descriptions major ait biol ceie cpe psyc total non-transfer students #(s,c) , , , , , , , , , , transfer students #(s,c) , , , , , , , , , , #s, #s-c number students, courses student-course pairs educational records across majors fall spring , respectively. fall fall fall spring fall fall spring spring fall training set: test set: figure different experimental protocols biology (biol), (iii) civil, environmental infrastructure engineering (ceie), (iv) computer engineering (cpe) (v) computer science (cs) (vi) psychology (psyc). table provides information datasets. . experimental protocol assess performance next-term grade prediction models, trained models data term make predictions term evaluate method three test terms, i.e., spring , fall spring . example, evaluating predictions term fall , data fall spring considered training data data fall testing data. datasets. figure shows three different train-test splits. define tick denote difference two consecutive letter grades (e.g., c-). assess performance grade prediction method, convert predicted grades closest letter grades compute percentage predicted grades error (or -ticks), within -tick within -ticks denoted pct pct pct respectively. problem course selection degree planning, courses predicted within ticks considered sufficiently correct. name metrics percentage tick accuracy (pta). . baseline methods compare performance proposed method following baseline approaches. .. matrix factorization matrix factorization known successful predicting ratings accurately recommender systems []. approach applied directly next-term grade prediction problem considering student-course grade matrix user-item rating matrix recommender systems. based assumption course student represented low-dimensional space, corresponding knowledge space, two low-rank matrices containing latent factors learned represent courses students []. specifically, grade student achieve course predicted follows: g̃s,c uts () global bias term, student course bias terms (in case, student course c), respectively, rk×n rk×m latent factors student course respectively. .. matrix factorization without bias (mf considered student course latent factors predict next-term grades. therefore, grade student achieve course calculated follows: g̃s,c uts . evaluation metrics use root mean squared error (rmse) mean absolute error (mae) metrics evaluation, defined follows: s,c∈gt (gs,c g̃s,c |gt s,c∈gt |gs,c g̃s,c |gt gs,c g̃s,c ground truth predicted grade student course testing set (student, course, grade) triples tth term. normally, next-term grade prediction problem, mae intuitive rmse since mae straightforward method calculates deviation errors directly rmse implications penalizing large errors more. dataset, student’s grade letter grade (i.e. a-, f). done previously polyzou et. al. [] .. () non-negative matrix factorization (nmf) [] add non-negative constraints matrix matrix equation . non-negativity constraints allows approaches better interpretability accuracy non-negative data []. results discussion . overall performance table presents comparison pct pct pct non-transfer students three terms considered test: spring , fall spring . observe mftci model outperforms baselines across different test sets. average, mftci outperforms mf, mf nmf methods .%, .% .% terms pct .%, .% .% terms pct .%, .% .% terms pct respectively. observe similar results transfer students well (not included brevity). table comparison performance pta (%) methods mf nmf mftci spring fall spring pct (↑) pct (↑) pct (↑) pct pct pct pct pct pct . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . “↑” indicates higher better. ii) reported values pct pct pct percentages. iii) best performing methods highlighted bold. . table presents performance baselines mftci model three different terms non-transfer transfer students using rmse mae evaluation metrics. mftci model consistently outperforms baselines across different datasets terms mae. addition, results shows mf nmf mftci tend better performance spring term fall term. similar trend observed fall term spring term. suggests mftci likely better performance information training set. . analysis individual majors divide non-transfer students based majors test baselines mftci model major, separately. table shows comparison pct pct pct different majors. results show mftci best performance almost majors. among results, mftci highest accuracy predicting grades psyc biol students student-course pairs training set. . effects previous terms mftci order see influence number previous terms considered mftci, run model ∆(t equation method represented mftcip figure shows comparison results mae six subsets data reported table “ntr” stands non-transfer students “tr” stands transfer students. results show mftci consistently outperforms mftcip datasets. suggests considering two previous terms necessary achieving good prediciton results. moreover, since consider student’s knowledge modeled using exponential decaying function time, include influence third previous term model influence grade prediction negligible comparison previous two terms. . visualization course influence interpret captured course influence matrix (see ), extract top values corresponding course names (and topics) analysis. figure show captured pairwise course influences ait majors, respectively. node corresponds one course represented shortened course’s name. notice figures influences reflect content dependency courses. example, major, “object oriented programming” course significant influence performance “low-level pro- mftcip mftci mae . . . . . figure mftci ntr spring ntr fall ntr spring spring fall spring comparison performance mftcip gramming” course (the former one also latter one’s prerequisite course); “linear algebra” “discrete mathematics” influence other; “formal methods models” course influence “analysis algorithms” course. case ait major, “introductory it” course “introductory computing” course influence “it problem programming” course; “multimedia web design” course influence “applied programming” course “it global economy” course. gmu sample schedule eight-term courses major order guide undergraduate students finish study step step based level, content difficulty courses among identified relationships shown figures found ait courses influences guide map, respectively. rest identified influences among general electives required courses (e.g., “public speaking” course), specific electives pertaining major (e.g., “research methods” course). shows model learns meaningful course-wise influences successfully uses improve model. figure shows identified course influences biol, ceie, cpe psyc majors. identified course-wise influences seem capture similarity course content. conclusion future work presented matrix factorization temporal coursewise influence (mftci) model integrates factorization models influence courses taken preceding terms predict student grades next term. evaluate model student educational records fall spring collected george ma http://catalog.gmu.edu table comparison performance rmse mae. methods mf nmf mftci spring rmse mae . . . . . . . . non-transfer students fall spring rmse mae rmse . . . . . . . . . . . . mae . . . . spring rmse . . . . mae . . . . transfer students fall rmse mae . . . . . . . . research methods western history . object oriented programming computer ethics . . . data structures . digital electronics . introductory programming . formal methods models analytic geometry calculus . . reading writng . public speaking . . . . low-level programming . spring rmse mae . . . . . . . . . linear algebra . advanced composition . . discrete mathematics . analysis algorithms figure identified course influences major table comparison performance different majors methods mf pct nmf mftci mf pct nmf mftci mf pct nmf mftci ait . . . . . . . . . . . . biol ceie cpe psyc . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . son university. dataset study contains non-transfer transfer students six different majors. experimental evaluation shows mftci consistently outperforms different state-of-the-art methods. moreover, analyze effects previous terms mftci, make conclusion necessary consider two previous terms. addition, visualize patterns learned pairs courses. results strongly demonstrate learned course influences correlate course content within academic programs. future, explore incorporation additional constraints pairwise course influence matrix, prerequisite information, compulsory elective provision course. explore using course influence information build degree planner future students. acknowledgments funding provided nsf grant, . appendix computational complexity analysis computational complexity mftci determined four steps alternating approach described above. update equation using gradient descent method via alternating minimization, computational complexity o(niteruv ns,c n)) o(niteruv (k×ns,c (typically ns,c max(m, n)), ns,c total number student-course dyads, number students, number courses, latent dimensions niteruv number iterations. update equation using gradient descent method, computational complexity upper-bounded ns,c )), ncc number course pairs o(nitera (ncc ns,c taken least one student, average number students course, upper bounds average number students co-take two courses, nitera number iteractions. essentially, update need update a(ci co-taken students. a(ci never taken together, remain update equation singular value decomposition involved thus computational complexity upper bounded o(m update equation computational complexity o(m update calculus applications composition . . introductory computing . computer hardware public speaking . . . multimedia web design . . western history . discrete mathematics . applied . . global economy . . information security . problem programming introductory statistics . database fundamentals . applied programming . introductory problem object oriented techniques . advanced composition figure identified course influences ait major introductory engineering general chemistry cell structure function biostatistics . . . . . . . organic chemistry general chemistry computer graphics . . . chemistry engineers . . organic chemistry lab organic chemistry lab physics lab physics . . . . physics microeconomic calculus . . . calculus . physics biology microorganisms (a) identified course influences biol major (b) identified course influences ceie major statistics psychology composition social psychology . . introductory engineering . calculus iii . . university physics . physics lab linear algebra calculus . . calculus . . university physics . introductory programming . cognitive psychology . . research psychology . abnormal psychology . composition . . . . physics lab (c) identified course influences cpe major physiological psychology (d) identified course influences psyc major figure identified course influences different majors equation , computational complexity o(m thus, computational complexity mtfci ns,c o(niter(niteruv ns,c nitera (ncc ns,c o(niter(niteruv (k×ns,c )+nitera (ncc )+m )), niter number iterations four steps. although complexity dominated due svd since (i.e., number courses) typically large, run time dominated ns,c (i.e., number student-course dyads). references [] charu aggarwal. recommender systems: textbook. springer publishing company, incorporated, st edition, . [] rsjd baker al. data mining education. international encyclopedia education, :–, . [] stephen boyd, neal parikh, eric chu, borja peleato, jonathan eckstein. distributed optimization statistical learning via alternating direction method multipliers. foundations trends machine learning, ():–, . [] hana bydžovská. collaborative filtering methods suitable student performance prediction? portuguese conference artificial intelligence, pages –. springer, . [] freddy chong tat chua, richard oentaryo, ee-peng lim. modeling temporal adoptions using dynamic matrix factorization. ieee th international conference data mining, pages –. ieee, . [] tristan denley. course recommendation system method, january . patent app. /,. [] ding xue li. time weight collaborative filtering. proceedings th acm international conference information knowledge management, cikm ’, pages –, new york, ny, usa, . acm. [] asmaa elbadrawy, scott studham, george karypis. personalized multi-regression models predicting students performance course activities. umn cs, pages –, . [] he. examining studentsâăź online interaction live video streaming environment using data mining text mining. computers human behavior, ():–, . [] ngoc-diep ho. nonnegative matrix factorization algorithms applications. phd thesis, école polytechnique, . [] chein-shung hwang yi-ching su. unified clustering locality preserving matrix factorization student performance prediction. iaeng int. comput. sci, ():–, . [] bin ju, yuntao qian, minchao ye, rong ni, chenxi zhu. using dynamic multi-task non-negative matrix factorization detect evolution user preferences collaborative filtering. plos one, ():e, . [] yehuda koren, robert bell, chris volinsky. matrix factorization techniques recommender systems. computer, ():–, august . [] yehuda koren, robert bell, chris volinsky, al. matrix factorization techniques recommender systems. computer, ():–, . [] daniel lee sebastian seung. algorithms non-negative matrix factorization. advances neural information processing systems, pages –, . [] dixin luo, hongteng xu, zhen, xia ning, hongyuan zha, xiaokang yang, wenjun zhang. multi-task multi-dimensional hawkes processes modeling event sequences. proceedings th international conference artificial intelligence, ijcai’, pages –. aaai press, . [] rabab naqvi. data mining educational settings. pakistan journal engineering, technology science, (), . [] xia ning, christian desrosiers, george karypis. comprehensive survey neighborhood-based recommendation methods. francesco ricci, lior rokach, bracha shapira, editors, recommender systems handbook, pages –. springer, . [] michelle parker. advising retention graduation. . [] michael pazzani daniel billsus. adaptive web. chapter content-based recommendation systems, pages –. springer-verlag, berlin, heidelberg, . [] alejandro peña-ayala. educational data mining: survey data mining-based analysis recent works. expert systems applications, ():–, . [] štefan pero tomáš horváth. comparison collaborative-filtering techniques small-scale student performance prediction task. innovations advances computing, informatics, systems sciences, networking engineering, pages –. springer, . [] agoritsa polyzou george karypis. grade prediction models specific students courses. international journal data science analytics, pages –, . [] agoritsa polyzou george karypis. grade prediction models specific students courses. international journal data science analytics, pages –, . [] sanjog ray anuj sharma. collaborative filtering based approach recommending elective courses. international conference information intelligence, systems, technology management, pages –. springer, . [] francesco ricci, lior rokach, bracha shapira, paul kantor. recommender systems handbook., . [] jill simons. national study student early alert models four-year institutions higher education. eric, . [] john sun, dhruv parthasarathy, kush varshney. collaborative kalman filtering dynamic matrix factorization. ieee transactions signal processing, ():–, . [] john sun, kush varshney, karthik subbian. dynamic matrix factorization: state space approach. ieee international conference acoustics, speech signal processing (icassp), pages –. ieee, . [] mack sweeney, jaime lester, huzefa rangwala. next-term student grade prediction. big data (big data), ieee international conference on, pages –. ieee, . [] mack sweeney, huzefa rangwala, jaime lester, aditya johri. next-term student performance prediction: recommender systems approach. arxiv preprint arxiv:., . [] nguyen thai-nghe, lucas drumond, artus krohn-grimberghe, lars schmidt-thieme. recommender system predicting student performance. procedia computer science, ():–, . [] liang xiong, chen, tzu-kuo huang, jeff schneider, jaime carbonell. temporal collaborative filtering bayesian probabilistic tensor factorization, pages –. . [] chenyi zhang, wang, hongkun yu, jianling sun, ee-peng lim. latent factor transition dynamic collaborative filtering. sdm, pages –. siam, .'"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12485\n"
     ]
    }
   ],
   "source": [
    "# 13) create a function that takes in a list of documents\n",
    "# and returns a set of unique words. Make sure that you\n",
    "# sort the list alphabetically before returning it. \n",
    "def get_vocabulary(documents):\n",
    "    voc = []\n",
    "    for i,doc in enumerate(documents):\n",
    "        for word in doc.split(): \n",
    "            voc.append(word)\n",
    "    voc = list(set(voc))\n",
    "    return voc\n",
    "\n",
    "# Then print the length of your vocabulary (it should be \n",
    "# around 5500 words)\n",
    "#print(get_vocabulary(documents))\n",
    "print(len(get_vocabulary(documents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14) what was the size of Sherin's vocabulary? \n",
    "too large but 12485"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - transform your documents into 100-words chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15) create a function that takes in a list of documents\n",
    "# and returns a list of 100-words chunk \n",
    "# (with a 25 words overlap between them)\n",
    "# Optional: add two arguments, one for the number of words\n",
    "# in each chunk, and one for the overlap size\n",
    "# Advice: combining all the documents into one giant string\n",
    "# and splitting it into separate words will make your life easier!\n",
    "chunks = []\n",
    "with open('Papers/papers*.txt', 'w') as outfile:\n",
    "    for doc in documents:\n",
    "        with open('Papers/papers*.txt') as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)\n",
    "def chunks(documents):\n",
    "    for doc in documents: \n",
    "        n = 100\n",
    "        chunks = [[i * n:(i + 1) * n] for i in range((len(documents) + n - 1) // n )]\n",
    "        print(chunk) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-329-55ba41a018a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# a length of 100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Optional: use assert to do this check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m      \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"length is not equal to 100, length is \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chunks' is not defined"
     ]
    }
   ],
   "source": [
    "# 16) create a for loop to double check that each chunk has \n",
    "# a length of 100\n",
    "# Optional: use assert to do this check\n",
    "for chunk in chunks:\n",
    "     assert (len(chunk) == 100),\"length is not equal to 100, length is \" + str(len(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'function' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-339-3334886bee62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 17) print the first chunk, and compare it to the original text.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# does that match what Sherin describes in his paper?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'function' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# 17) print the first chunk, and compare it to the original text.\n",
    "# does that match what Sherin describes in his paper?\n",
    "print(chunk[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18) how many chunks did Sherin have? What does a chunk become \n",
    "# in the next step of our topic modeling algorithm? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19) what are some other preprocessing steps we could do \n",
    "# to improve the quality of the text data? Mention at least 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20) in your own words, describe the next steps of the \n",
    "# data modeling algorithms (listed below):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Vector and Matrix operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Weight word frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Matrix normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 - Deviation Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10 - Visualizing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Step - Putting it all together: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in python code, our goal is to recreate the steps above as functions\n",
    "# so that we can just one line to run topic modeling on a list of \n",
    "# documents: \n",
    "def ExtractTopicsVSM(documents, numTopics):\n",
    "    ''' this functions takes in a list of documents (strings), \n",
    "        runs topic modeling (as implemented by Sherin, 2013)\n",
    "        and returns the clustering results, the matrix used \n",
    "        for clustering a visualization '''\n",
    "    \n",
    "    # step 2: clean up the documents\n",
    "    documents = clean_list_of_documents(documents)\n",
    "    \n",
    "    # step 3: let's build the vocabulary of these docs\n",
    "    vocabulary = get_vocabulary(documents)\n",
    "    \n",
    "    # step 4: we build our list of 100-words overlapping fragments\n",
    "    documents = flatten_and_overlap(documents)\n",
    "    \n",
    "    # step 5: we convert the chunks into a matrix\n",
    "    matrix = docs_by_words_matrix(documents, vocabulary)\n",
    "    \n",
    "    # step 6: we weight the frequency of words (count = 1 + log(count))\n",
    "    matrix = one_plus_log_mat(matrix, documents, vocabulary)\n",
    "    \n",
    "    # step 7: we normalize the matrix\n",
    "    matrix = normalize(matrix)\n",
    "    \n",
    "    # step 8: we compute deviation vectors\n",
    "    matrix = transform_deviation_vectors(matrix, documents)\n",
    "    \n",
    "    # step 9: we apply a clustering algorithm to find topics\n",
    "    results_clustering = cluster_matrix(matrix)\n",
    "    \n",
    "    # step 10: we create a visualization of the topics\n",
    "    visualization = visualize_clusters(results_clustering, vocabulary)\n",
    "    \n",
    "    # finally, we return the clustering results, the matrix, and a visualization\n",
    "    return results_clustering, matrix, visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
